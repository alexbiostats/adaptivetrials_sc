{"title":"Interim Monitoring for Futility/Efficacy/Safety","markdown":{"yaml":{"title":"Interim Monitoring for Futility/Efficacy/Safety","toc":true,"toc_float":true,"toc-location":"left","format":{"html":{"code-fold":"show","code-overflow":"wrap","code-tools":true}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n\nIn this module we introduce the \"original\" adaptive trial element of interim monitoring to stop a trial early for efficacy, futility, and/or safety. In these designs we may terminate a trial early because it is highly unlikely we would be able to detect a significant effect if we continued (i.e., *futility*), because we already have sufficient evidence of a significant treatment effect (i.e., *efficacy*), or because there are concerns relating to adverse events (i.e., *safety*).\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/4dfadc6943624b96a77e617e7b4e60ee\" title=\"Interim Monitoring for Futility/Efficacy/Safety\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/3_interim_monitoring.pptx).\n\n\n# Code Examples in R\n\nGiven the lengthy history of developing methods for interim monitoring, there are a host of software options available today. Within R, some of the packages to consider include:\n\n* [`rpact`](https://www.rpact.org/): a package for confirmatory adaptive clinical trial design, simulation, and analysis; a [Shiny app](https://cran.r-project.org/web/packages/Sequential/index.html) is also available\n* [`gsDesign`](https://keaven.github.io/gsDesign/): includes both an R package and an [online interface](https://rinpharma.shinyapps.io/gsdesign/) to estimate group sequential boundaries\n* [`GroupSeq`](https://cran.r-project.org/web/packages/GroupSeq/vignettes/GroupSeq.html): a package with a graphical user interface accessed via R which helps to avoid having to learn too much coding or syntax to access boundaries\n* [`RCTdesign`](http://www.rctdesign.org/Welcome.html): a package initially developed for S-Plus, but with a new version for use in R; need to request access via signed license agreement which can be a little burdensome\n* [`Sequential`](https://cran.r-project.org/web/packages/Sequential/index.html): a package focused on exact sequential analysis for Poisson and binomial data where group sizes do not have to be specified in advance\n\nWe will focus on `rpact` for our examples below:\n\n```{r}\nlibrary(rpact)\n```\n\n\n## `rpact` and the `getDesignGroupSequential()` Function\n\nThe powerhouse function `getDesignGroupSequential()` allows us to specify numerous elements of various group sequential designs.\n\n```{r, eval=F}\n# Syntax: using getDesignGroupSequential function to calculate interim monitoring boundaries\ndesign <- getDesignGroupSequential(\n    sided = <specify if alternative hypothesis is one- or two-sided>,  # default is 1 for one-sided, can also specified 2 for two-sided hypothesis test\n    alpha = <desired type I error rate>, # default is 0.025\n    beta = <desired type II error rate>, # default is 0.20 (i.e., power=1-beta, so default is 80% power)\n    kMax = <maximum number of stages>, # default is 3, number of interim analyses is kMax-1\n    informationRates = <fixed information rates prior to start of trial when interim monitoring will occur>, # default is (1:kMax)/kMax, can manually specify and ignore kMax argument\n    typeOfDesign = <type of boundaries for efficacy monitoring>, # many choices (see documentation) including O'Brien-Fleming (\"OF\"), Pocock (\"P\"), and alpha-spending versions (\"asOF\" and \"asP\")\n    typeBetaSpending = <type of boundaries for futility monitoring>, # many choices (see documentation) including O'Brien-Fleming (\"bsOF\") and Pocock (\"bsP\")\n    futilityBounds = <manually defined futility boundaries on the test statistic Z-scale> # can define futility with desired rules or use spending in next argument\n)\n```\n\n\n## `rpact` and Monitoring for Only Efficacy\n\nIn some settings, we may only wish to monitor our clinical trial to allow stopping for efficacy. In this setting, we would only stop early if we observed an overwhelming effect for our outcome that is being monitored.\n\n\n### Two-Sided Efficacy Boundaries\n\nLet's start by exploring a few different boundaries and compare them graphically. We'll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed:\n\n```{r}\neo_of <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=2, alpha=0.05) # O'Brien-Fleming\neo_hp <- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=2, alpha=0.05) # Haybittle-Peto\neo_asof <- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp <- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending Pocock-like boundary\n```\n\nThe package includes the ability to extract these objects and make a plot:\n\n```{r}\n# create plot of stopping boundaries\ndesignSet <- getDesignSet(designs = c(eo_of, eo_hp, eo_asof, eo_asp), variedParameters = \"typeOfDesign\")\n\nplot(designSet,\n     type=1, # can plot boundaries with type=1, but also plot other characteristics (see ?plot.TrialDesignSet)\n     legendPosition = 2) # functionality doesn't seem to work to move legend placement\n```\n\nHowever, the legend placement argument does not seem to be working, making it a little challenging to view. Instead, we can create our own figure by extracting the relevant information. First let's look at the R output for our O'Brien-Fleming object and compare with the extracted critical values:\n\n```{r}\nprint(eo_of) # review output; without print() in Rmd it creates nicely formatted results\n\neo_of_crit <- eo_of$criticalValues # extracts the critical values, which here are symmetric\neo_of_crit # check values are correct\n```\n\nNow that we see we've extracted the correct information, we can extract for each boundary and create a plot:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects\neo_hp_crit <- eo_hp$criticalValues\neo_asof_crit <- eo_asof$criticalValues\neo_asp_crit <- eo_asp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-eo_of_crit, type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2','blue'), lwd=2, legend=c('OF','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* O'Brien-Fleming (OF) and the alpha-spending O'Brien-Fleming-like boundary (asOF) are very similar, but we do see that the alpha-spending is slightly more conservative at earlier interim looks.\n* Both Haybittle-Peto (HP) and the alpha-spending Pocock-like boundary (asP) have the same critical value at the first three looks. However, HP is more conservative, because at the final look its value of `r eo_hp_crit[4]` is very similar to the traditional threshold of 1.96. In contrast, asP's boundary at the end of the study is now `r eo_asp_crit[4]`.\n* Boundaries that are more conservative early on tend to be most similar to designs without interim monitoring for futility for the final analysis.\n\n\n### One-Sided Efficacy Boundaries\n\nWe can also easily make the same figure we had before by modifying some arguments in `getDesignGroupSequential()`:\n\n* `sided` becomes 1\n* `alpha` becomes 0.025\n\nYou may be wondering, why would we change our $\\alpha$ from 0.05 to 0.025? In practice, we could choose a more liberal one-sided $\\alpha=0.05$, however this would change our critical value from `r qnorm(0.975)` at $\\alpha=0.025$ to `r qnorm(0.95)` at $\\alpha=0.05$. This could increase our risk of a type I error (i.e., falsely finding an effect when none exists). Depending on the context, we may wish to use the more conservative one-sided $\\alpha=0.025$.\n\nNow, let's examine the modified code and observe the boundaries. We'll add a comparison with $\\alpha=0.05$ for our O'Brien-Fleming boundary for comparison:\n\n```{r}\neo_of1 <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.025) # O'Brien-Fleming\neo_hp1 <- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=1, alpha=0.025) # Haybittle-Peto\neo_asof1 <- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp1 <- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending Pocock-like boundary\n\n# add OBF with 0.05 alpha\neo_of1_alpha05 <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.05) # O'Brien-Fleming with 0.05 boundary\n\n# extract critical values\neo_of1_crit <- eo_of1$criticalValues\neo_hp1_crit <- eo_hp1$criticalValues\neo_asof1_crit <- eo_asof1$criticalValues\neo_asp1_crit <- eo_asp1$criticalValues\neo_of1_alpha05_crit <- eo_of1_alpha05$criticalValues\n\n# compare OBF with 0.025 and 0.05\nrbind( 'Two-Sided OBF with alpha=0.05'=eo_of_crit, 'One-Sided OBF with alpha=0.025'=eo_of1_crit, 'One-Sided OBF with alpha=0.05'=eo_of1_alpha05_crit)\n```\n\nFirst, we see the top two rows are the same for our two-sided O'Brien-Fleming boundary with $\\alpha=0.05$ and our one-sided O'Brien-Fleming boundary with $\\alpha=0.025$. This is because our two-sided boundary assumed that our $\\alpha=0.05$ was distributed symmetrically with 0.025 on both sides.\n\nFrom the comparison of our one-sided O'Brien Fleming boundaries, the lower row with $\\alpha=0.05$ has lower critical value thresholds, making it more likely to declare significance. \n\nFor visual comparison of boundaries:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_alpha05_crit, type='o', lwd=2, pch=16, col='gray65')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof1_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp1_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp1_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','gray65','green4','orangered2','blue'), lwd=2, legend=c('OF','OF 0.05','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* We can visually see that the O'Brien-Fleming with $\\alpha=0.025$ (OF) has higher critical value thresholds compared to the O'Brien-Fleming boundary with $\\alpha=0.05$ (OF 0.05).\n* All boundaries here are identical to our previous two-sided figure since we used $\\alpha=0.025$, but there is no lower boundary.\n* For one-sided hypothesis tests, the directionality of our test is *very* important. We must be careful to ensure we are implementing the proper test:\n    * If we are testing $H_1\\colon \\mu_{trt} > \\mu_{con}$ and our estimate is $\\delta = \\mu_{trt} - \\mu_{con}$, then we would want a positive critical value to stop the study early for efficacy in our one-sided test. For example, if $Z=5$ we would stop at any interim look for all presented methods.\n    * If $Z=-5$ for our one-sided test with $H_1\\colon \\mu_{trt} > \\mu_{con}$, we would continue the study because we do not have evidence of efficacy. *(However, if we had incorporated interim monitoring for futility we might have stopped for futility.)*\n\n\n## `rpact` and Monitoring for Only Futility\n\nOur previous section examined efficacy-only interim monitoring. We can also design studies where we are only monitoring for futility, indicating we would only stop early if it is highly unlikely that we would be able to declare efficacy at the conclusion of the trial if we reached full enrollment.\n\n### Two-Sided Futility Boundaries\n\nLet's start by exploring a few different boundaries and compare them graphically. We'll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed. Using `getDesignGroupSequential()`, we need to add an argument that manually defines $\\alpha$-spending via the `userAlphaSpending` argument:\n\n```{r}\n# Two-sided O'Brien-Fleming futility boundaries\nfo_of <- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.05), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock futility boundaries\nfo_p <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.05, userAlphaSpending = c(0,0,0,0.05), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n```\n\nLet's dissect some of the output and how it may look differently from our efficacy-only boundary:\n\n```{r}\nprint(fo_of)\n```\n\n\nFrom `print(fo_of)` in the lower `Output:` section:\n\n* We now have a `futilityBoundsNonBinding` line that only includes 3 values, one of which is `NA`:\n    * These three values represent our first three stages, which correspond to our interim analyses since the fourth stage means the trial is complete.\n    * The first interim look with this design is `NA`, indicating that for the chosen boundary type and location of the interim look there is no setting where the design would recommend terminating for futility. In other words, we will always enroll at least 50% of the participants in our study using these O'Brien-Fleming boundaries for futility monitoring, assuming the DSMB or external factors do not terminate the trial before completion.\n    * Note, the because there is a stage without a valid futility boundary, the default behavior in `rpact` is to reallocate our $\\beta$-spending to later stages. If you wish to leave the first stage $\\beta$ \"unspent\", you can add the argument `betaAdjustment=F` to the function.\n* The `Critical values` line includes 4 values, three of which are $\\infty$:    \n    * The \"fourth\" stage critical value is represented by the `Critical values` row, where we see the final threshold is our $Z$-score based on a two-sided test with $\\alpha=0.05$ (i.e., `qnorm(0.975)`=1.96).\n    * The three `Inf` values indicate it is impossible to stop for efficacy since we will never observe a test statistic of $Z=\\infty$.\n    * In practice, we could also set the critical values to be very large (e.g., $Z=5$) since this would correspond to a very large treatment effect.\n\nLet's create our plots in a similar way to our efficacy examples to see these two-sided boundaries.\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of_fbnd <- fo_of$futilityBounds\nfo_p_fbnd <- fo_p$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of_crit <- fo_of$criticalValues\nfo_p_crit <- fo_p$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* Unlike with efficacy boundaries, Pocock (P) futility boundaries are not constant over all stages.\n* As noted before, the O'Brien-Fleming (OF) boundaries do not have an interim stopping rule at the first stage.\n\n\n\n### One-Sided Futility Boundaries\n\nWe can also modify our previous futility boundaries for a one-sided hypothesis test. Here we will again change $\\alpha=0.05$ to $\\alpha=0.025$ for consistency with our previous example:\n\n```{r}\n# One-sided O'Brien-Fleming futility boundaries\nfo_of1 <- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.025, # defines alpha level for one-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.025), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# One-sided Pocock futility boundaries\nfo_p1 <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n```\n\nLet's create our plots in a similar way to our efficacy examples to see these two-sided boundaries. \n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of1_fbnd <- fo_of1$futilityBounds\nfo_p1_fbnd <- fo_p1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of1_crit <- fo_of1$criticalValues\nfo_p1_crit <- fo_p1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of1_fbnd, fo_of1_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p1_fbnd, fo_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* With a one-sided stopping boundary, the O'Brien-Fleming boundaries now have a critical value at the first stage! Since it is negative we can identify that we would have to observe a large effect in the \"wrong\" direction from our hypothesis test to stop after only 25% of the study was enrolled.\n* The O'Brien-Fleming boundary is less aggressive than the Pocock boundary. This suggests that a Pocock design may reduce our power without making other modifications (e.g., increased $N_{max}$ for the study relative to a fixed sample design).\n\nWe can also compare the stopping boundaries to see how they change between our one- and two-sided designs:\n\n```{r}\nrbind( 'OBF Two-Sided'=c(fo_of_fbnd, fo_of_crit[4]), 'OBF One-Sided'=c(fo_of1_fbnd, fo_of1_crit[4]),\n       'Pocock Two-Sided'=c(fo_p_fbnd, fo_p_crit[4]), 'Pocock One-Sided'=c(fo_p1_fbnd, fo_p1_crit[4]))\n```\n\nWe see in the table that while boundaries are similar between one- and two-sided cases, they do have some differences, especially at earlier stages.\n\n\n## `rpact` and Monitoring for Both Efficacy and Futility\n\nNow that we've covered both efficacy-only and futility-only interim monitoring, we may be interested in seeing how we can combine them together within one design. Here we'll incorporate our code from above to leverage stopping boundaries for both efficacy and futility.\n\n\n### Two-Sided Boundaries for Efficacy and Futility\n\nIt is possible to choose different boundaries styles for futility and efficacy. Here we will consider three designs: alpha/beta-spending O'Brien-Fleming-like boundaries for both, Pocock for both, or alpha-spending with O'Brien-Fleming-like boundaries for efficacy and Pocock for futility.\n\n```{r}\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of <- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.05, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n\n# \nef_ofp <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.05,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n```\n\nLet's jump right into our visualized boundary shapes:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of_fbnd <- ef_of$futilityBounds\nef_p_fbnd <- ef_p$futilityBounds\nef_ofp_fbnd <- ef_ofp$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of_crit <- ef_of$criticalValues\nef_p_crit <- ef_p$criticalValues\nef_ofp_crit <- ef_ofp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nThis figure is a lot busier than our previous figure, but we have some takeaways here still:\n\n* There are now 3 outcomes at each interim stage: stop for efficacy, stop for futility, or continue to the next stage.\n* By the end of the trial, we see the boundaries have all converged.\n* When mixing the O'Brien-Fleming for efficacy and Pocock for futility we see that the efficacy boundaries are the same as the O'Brien-Fleming for both efficacy and futilty design, but the futility boundaries have shifted since they need to arrive at the same efficacy threshold.\n\nFor ease of viewing, we can also plot each design separately in a panel figure:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# Plot boundaries\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,1.1)) # 2x2 panel figure\n\n# OBF\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"O'Brien-Fleming-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"Pocock-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# OBF+Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"OBF for Eff, P for Fut\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n```\n\n\n### One-Sided Boundaries for Efficacy and Futility\n\nLet's now examine how our boundaries change with a one-sided example when monitoring for both futility and efficacy. As before, we will change $\\alpha$ to 0.025:\n\n```{r}\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of1 <- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.025, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p1 <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.025, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n\n# \nef_ofp1 <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.025,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n```\n\nLet's jump right into our visualized boundary shapes:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of1_fbnd <- ef_of1$futilityBounds\nef_p1_fbnd <- ef_p1$futilityBounds\nef_ofp1_fbnd <- ef_ofp1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of1_crit <- ef_of1$criticalValues\nef_p1_crit <- ef_p1$criticalValues\nef_ofp1_crit <- ef_ofp1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_fbnd, ef_of1_crit[4]), type='o', lwd=2, pch=16) # futility \nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_crit), type='o', lwd=2, pch=16) # efficacy \n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_fbnd, ef_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_fbnd, ef_ofp1_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nA similar story emerges from our previous examples:\n\n* Pocock has the biggest penalty for efficacy monitoring as evidenced by its larger critical value at 100% information.\n* O'Brien-Fleming-like boundaries for both efficacy and futility have the same efficacy boundaries when using OBF for efficacy but Pocock for futility, however the futility bondaries change compared to the Pocock-like boundaries.\n* With a one-sided test, the direction of our hypothesis matters. This figure suggests that everything on the bottom portion of the graph would result in futility stopping.\n\n\n# Simulation Study\n\nNow that we've seen examples of how to calculate boundaries and visualize them, let's explore a brief simulation study. We will evaluate the following interim monitoring settings:\n\n* Two-sided futility only with O'Brien-Fleming-like beta-spending boundaries\n* Two-sided efficacy only with O'Brien-Fleming-like alpha-spending boundaries\n* Two-sided efficacy and futility with O'Brien-Fleming-like alpha/beta-spending boundaries\n* Two-sided efficacy and futility with Pocock-like alpha/beta-spending boundaries\n* Fixed sample design\n\nFor these scenarios, we will assume 5 total looks, $\\alpha=0.05$, and $\\beta=0.8$. We will evaluate three scenarios:\n\n* Null scenario with no difference between arms with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0,1)$\n* Alternative scenario with clinically meaningful difference between groups with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0.4,1)$\n* Alternative scenario with half the clinically meaningful difference between groups with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0.2,1)$\n\nOur sample sizes will be determined based on the $n$ needed for the alternative scenario:\n\n```{r}\npower.t.test(delta=0.4, sd=1, sig.level=0.05, power=0.8)\n```\n\nRecall, we always round up to the next integer to ensure that we have the desired power level of 80%. In other words, 99.08057 becomes 100 per group.\n\nOur next code chunk provides a simple step-by-step simulation. The chunk is hidden by default, but you can unhide it to see the behind-the-scenes structure or to modify the setting for anything you are interested in! The general set-up will also work for other types of outcomes (e.g., binary, ordinal, etc.), but will need different data generating mechanisms (i.e., the `rxxxx` functions).\n\nFirst, let's estimate the boundaries we will need for each simulation based on our five scenarios. Here we will assume everything is estimated on a $Z$-scale. For simplicity, we will also leverage the fact that all our boundaries are symmetric and we will evaluate the $|Z|$ (i.e., the absolute value of $Z$):\n\n```{r}\n#| echo: true\n#| code-fold: true\n## Two-sided futility only with O'Brien-Fleming-like beta-spending boundaries\nfo_of <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.05, userAlphaSpending = c(0,0,0,0,0.05), \n                                 typeBetaSpending = \"bsOF\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nfo_of_bounds <- cbind( futility = c(fo_of$futilityBounds, fo_of$criticalValues[5]),\n                       efficacy = fo_of$criticalValues)\n\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nfo_of_bounds[ is.na(fo_of_bounds )] <- 0\n\n\n## Two-sided efficacy only with O'Brien-Fleming-like alpha-spending boundaries\neo_of <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 5, sided=2, alpha=0.05) # O'Brien-Fleming\n\n# create object that stores all boundaries with each row representing stage 1-5\n# note, no futility monitoring in this design so we will place all values as 0 at stages 1-4\neo_of_bounds <- cbind( futility = c(rep(0,4), fo_of$criticalValues[5]),\n                       efficacy = eo_of$criticalValues)\n\n\n## Two-sided efficacy and futility with O'Brien-Fleming-like alpha/beta-spending boundaries\nef_of <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.05, \n                                 typeBetaSpending = \"bsOF\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nef_of_bounds <- cbind( futility = c(ef_of$futilityBounds, ef_of$criticalValues[5]),\n                       efficacy = ef_of$criticalValues)\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nef_of_bounds[ is.na(ef_of_bounds )] <- 0\n\n\n## Two-sided efficacy and futility with Pocock-like alpha/beta-spending boundaries\nef_p <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.05, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nef_p_bounds <- cbind( futility = c(ef_p$futilityBounds, ef_p$criticalValues[5]),\n                       efficacy = ef_p$criticalValues)\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nef_p_bounds[ is.na(ef_p_bounds )] <- 0\n\n\n## Fixed sample design\nfs_bounds <- cbind( futility = c(rep(0,4), qnorm(0.975) ),\n                    efficacy = c(rep(Inf,4), qnorm(0.975)) )\n```\n\nIn addition to estimating the boundaries, we created an object we can use for reference in the simulation study:\n\n```{r}\n# print example of bounds for two-sided OBF for futility and efficacy\nef_of_bounds\n```\n\nSince we are leveraging that the boundaries are symmetric, we replace the stage 1 futility `NA` boundary with a 0, since $Pr(|Z|<0)=0$. Likewise, for the fixed sample design we see:\n\n```{r}\n# print fixed sample boundaries\nfs_bounds\n```\n\nFor efficacy, we know $Pr(|Z|>\\infty)=0$, representing a case where we cannot stop for futility or efficacy until the trial has concluded.\n\nFor each scenario we will simulate data for 1000 hypothetical trials, apply our stopping rules, and summarize the:\n\n* Rejection rate for each scenario (e.g., type I error rate for null scenarios, power for alternative scenarios)\n* Expected sample size and its standard deviation\n* Average stopping stage and its standard deviation (directly related to ESS, but may be nice to present in different ways)\n* Proportion of simulated trials stopping early and for what reason\n\n## Null Scenario Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own.\n\n```{r null-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 5145 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nRemembering that this is the null scenario where we simulated *no difference* between study arms, we can take away numerous nuggets of information:\n\n* Having the fixed sample design is important to describe what the expected type I error rate (i.e., rejection rate) is for our simulated data. While $\\alpha=0.05$, we see that in this simulation of 1000 trials the observed type I error rate was 4.2%. In other words, $1000 \\times 0.042 = 42$ trials rejected the null hypothesis that $\\mu_1 = \\mu_2$, even though we simulated data where the means were equal.\n* In terms of type I error control, relative to the fixed sample design:\n    * The OBF designs with futility have lower type I error rates\n    * The OBF efficacy only and Pocock E+F designs have increased type I error rates\n* In terms of the expected sample size:\n    * Interim monitoring helped to reduce the average sample size needed until study termination\n    * Pocock has the lowest ESS and average stopping point because it is more aggressive with early stopping\n    * The OBF efficacy only design rarely stopped, which is good since it would only stop for efficacy\n    * Designs with futility stopping terminated at a point prior to trial conclusion ~80% of the time! This represents a much more efficient design relative to a fixed sample design when there is no effect.\n* Since we simulated the null scenario here, we shouldn't stop for efficacy. However, we see that:\n    * The OBF Efficacy Only design stopped 2.1% of the time for efficacy\n    * The OBF Efficacy+Futility design stopped 2.0% of the time for efficacy\n    * The Pocock Efficacy+Futility design stopped 5.4% of the time, helping to see the more aggressive stopping boundaries\n\nOf course, all of the above only considers the null scenario. For a complete view of the trade-offs, we should look at alternative scenarios as well.\n\n\n## Alternative Scenario I Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., `mean1` or `mean2`), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n```{r alt1-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0.4 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 7354 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nRemembering that this is the alternative scenario where we simulated a difference of $\\delta=0.4$ between study arms which was used to estimate the sample size needed for 80% power, we can take away numerous nuggets of information:\n\n* Having the fixed sample design is important to describe what the expected power (i.e., rejection rate) is for our simulated data. While $\\beta=0.2$, we see that in this simulation of 1000 trials the observed power was 79.8%, which is quite close to the target of 80%.\n* In terms of power, relative to the fixed sample design:\n    * All designs with futility monitoring for these boundaries have reduced power (~5% less for OBF, but 13.1% less for Pocock!)\n    * The OBF efficacy only design has slightly higher power at 79.9% (i.e., 1 more out of 1000 simulated trials was found to be significant compared to the fixed sample design)\n* In terms of the expected sample size:\n    * All designs show a lower ESS than the fixed sample design\n    * The OBF efficacy design has slightly higher power *and an ESS that is about 41 participants lower*. We see based on the average stopping segment that it is stopping, on average, after 80% of participants have been observed.\n    * The Pocock design has the lowest ESS, but this is because it stops for futility aggressively 28.1% of the time, even though we simulated data with a difference.\n\nWe need to balance these results with performance under the null scenario, taking into account if we are willing to tolerate reductions in power for the trade-off of more futility stopping in the null, or vice versa.\n\n### Note on Futility Stopping Under the Alternative\n\nAn interesting quirk worth noting is that futility stopping may still be beneficial in some settings. For example, our target is 80% power, meaning that 20% of the time our study will not detect the effect even though it exists. For the 20% of trials that were not significant, it still may be helpful to stop the trial early to conserve resources.\n\nFor example, let's observe the confusion matrix of the OBF design stopping for efficacy and futility with the fixed sample design:\n\n```{r}\ntable( OBF = sim_res_ef_of[2,], FS = sim_res_fs[2,])\n```\n\nFrom this table, we see that of the 202 simulated trials that didn't detect the effect, 92.1% (186/202) stopped for futility early. The trade-off, of course, is that we see of the 798 trials the did detect an effect, 7.5% (60/798) stopped for futility that would have gone on to reject the null hypothesis. The final piece of information to note is that 7.9% (16/202) of trials where the fixed sample did not reject $H_0$, the OBF approach stopped early for efficacy and detected an effect. \n\nIt can be hard to juggle all these conflicting pieces of information, but it does highlight that we are often interested in trying to minimize the trade-off that harms out overall trial performance. In this case, the OBF design stopping for both efficacy and futility does reduce power to 75.4% from 79.8% in the fixed design, but it does so with a reduced type I error rate in the null scenario with a large reduction in ESS.\n\nHowever, it is worth noting in practice we don't know if a trial stopping for futility is truly null, or we happened to observe something null-ish by chance. To address this, we may wish to evaluate other adaptive elements like sample size re-estimation to increase the probability of conducting a successful trial.\n\n\n## Alternative Scenario II Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the half of the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., `mean1` or `mean2`), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n```{r alt2-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0.2 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 7811 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nIn this scenario, we simulated a design where there is an effect, but it is only half that of what we used for the power calculation (i.e., $\\delta=0.2$ in this simulation versus $\\delta=0.4$ used in our power calculation). If this effect size is no longer clinically relevant, we would hope to stop for futility without having to carry out the entire study. If this effect is relevant, other adaptive methods like sample size re-estimation are needed to increase our power.\n\nFor this scenario, we see that designs with futility monitoring stop between 61.9-72.2% of the time. The power of the fixed sample design is low at 29.6%, with all designs including any interim monitoring showing lower power. This is partially due to futility monitoring if implemented, but also because the final testing threshold is adjusted for multiple testing making it harder to reject. Likely, many of the cases that are discordant are because the test statistic fell between `qnorm(0.975)`=1.96 and the adjusted threshold for each method.\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n\n* [Guidance on interim analysis methods in clinical trials](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/guidance-on-interim-analysis-methods-in-clinical-trials/5051FDCF5284970B3DB01FE609AAA4C2?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper focusing on interim analyses in clinical trials with included case studies","srcMarkdownNoYaml":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n# Overview\n\nIn this module we introduce the \"original\" adaptive trial element of interim monitoring to stop a trial early for efficacy, futility, and/or safety. In these designs we may terminate a trial early because it is highly unlikely we would be able to detect a significant effect if we continued (i.e., *futility*), because we already have sufficient evidence of a significant treatment effect (i.e., *efficacy*), or because there are concerns relating to adverse events (i.e., *safety*).\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/4dfadc6943624b96a77e617e7b4e60ee\" title=\"Interim Monitoring for Futility/Efficacy/Safety\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/3_interim_monitoring.pptx).\n\n\n# Code Examples in R\n\nGiven the lengthy history of developing methods for interim monitoring, there are a host of software options available today. Within R, some of the packages to consider include:\n\n* [`rpact`](https://www.rpact.org/): a package for confirmatory adaptive clinical trial design, simulation, and analysis; a [Shiny app](https://cran.r-project.org/web/packages/Sequential/index.html) is also available\n* [`gsDesign`](https://keaven.github.io/gsDesign/): includes both an R package and an [online interface](https://rinpharma.shinyapps.io/gsdesign/) to estimate group sequential boundaries\n* [`GroupSeq`](https://cran.r-project.org/web/packages/GroupSeq/vignettes/GroupSeq.html): a package with a graphical user interface accessed via R which helps to avoid having to learn too much coding or syntax to access boundaries\n* [`RCTdesign`](http://www.rctdesign.org/Welcome.html): a package initially developed for S-Plus, but with a new version for use in R; need to request access via signed license agreement which can be a little burdensome\n* [`Sequential`](https://cran.r-project.org/web/packages/Sequential/index.html): a package focused on exact sequential analysis for Poisson and binomial data where group sizes do not have to be specified in advance\n\nWe will focus on `rpact` for our examples below:\n\n```{r}\nlibrary(rpact)\n```\n\n\n## `rpact` and the `getDesignGroupSequential()` Function\n\nThe powerhouse function `getDesignGroupSequential()` allows us to specify numerous elements of various group sequential designs.\n\n```{r, eval=F}\n# Syntax: using getDesignGroupSequential function to calculate interim monitoring boundaries\ndesign <- getDesignGroupSequential(\n    sided = <specify if alternative hypothesis is one- or two-sided>,  # default is 1 for one-sided, can also specified 2 for two-sided hypothesis test\n    alpha = <desired type I error rate>, # default is 0.025\n    beta = <desired type II error rate>, # default is 0.20 (i.e., power=1-beta, so default is 80% power)\n    kMax = <maximum number of stages>, # default is 3, number of interim analyses is kMax-1\n    informationRates = <fixed information rates prior to start of trial when interim monitoring will occur>, # default is (1:kMax)/kMax, can manually specify and ignore kMax argument\n    typeOfDesign = <type of boundaries for efficacy monitoring>, # many choices (see documentation) including O'Brien-Fleming (\"OF\"), Pocock (\"P\"), and alpha-spending versions (\"asOF\" and \"asP\")\n    typeBetaSpending = <type of boundaries for futility monitoring>, # many choices (see documentation) including O'Brien-Fleming (\"bsOF\") and Pocock (\"bsP\")\n    futilityBounds = <manually defined futility boundaries on the test statistic Z-scale> # can define futility with desired rules or use spending in next argument\n)\n```\n\n\n## `rpact` and Monitoring for Only Efficacy\n\nIn some settings, we may only wish to monitor our clinical trial to allow stopping for efficacy. In this setting, we would only stop early if we observed an overwhelming effect for our outcome that is being monitored.\n\n\n### Two-Sided Efficacy Boundaries\n\nLet's start by exploring a few different boundaries and compare them graphically. We'll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed:\n\n```{r}\neo_of <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=2, alpha=0.05) # O'Brien-Fleming\neo_hp <- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=2, alpha=0.05) # Haybittle-Peto\neo_asof <- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp <- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending Pocock-like boundary\n```\n\nThe package includes the ability to extract these objects and make a plot:\n\n```{r}\n# create plot of stopping boundaries\ndesignSet <- getDesignSet(designs = c(eo_of, eo_hp, eo_asof, eo_asp), variedParameters = \"typeOfDesign\")\n\nplot(designSet,\n     type=1, # can plot boundaries with type=1, but also plot other characteristics (see ?plot.TrialDesignSet)\n     legendPosition = 2) # functionality doesn't seem to work to move legend placement\n```\n\nHowever, the legend placement argument does not seem to be working, making it a little challenging to view. Instead, we can create our own figure by extracting the relevant information. First let's look at the R output for our O'Brien-Fleming object and compare with the extracted critical values:\n\n```{r}\nprint(eo_of) # review output; without print() in Rmd it creates nicely formatted results\n\neo_of_crit <- eo_of$criticalValues # extracts the critical values, which here are symmetric\neo_of_crit # check values are correct\n```\n\nNow that we see we've extracted the correct information, we can extract for each boundary and create a plot:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects\neo_hp_crit <- eo_hp$criticalValues\neo_asof_crit <- eo_asof$criticalValues\neo_asp_crit <- eo_asp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-eo_of_crit, type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2','blue'), lwd=2, legend=c('OF','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* O'Brien-Fleming (OF) and the alpha-spending O'Brien-Fleming-like boundary (asOF) are very similar, but we do see that the alpha-spending is slightly more conservative at earlier interim looks.\n* Both Haybittle-Peto (HP) and the alpha-spending Pocock-like boundary (asP) have the same critical value at the first three looks. However, HP is more conservative, because at the final look its value of `r eo_hp_crit[4]` is very similar to the traditional threshold of 1.96. In contrast, asP's boundary at the end of the study is now `r eo_asp_crit[4]`.\n* Boundaries that are more conservative early on tend to be most similar to designs without interim monitoring for futility for the final analysis.\n\n\n### One-Sided Efficacy Boundaries\n\nWe can also easily make the same figure we had before by modifying some arguments in `getDesignGroupSequential()`:\n\n* `sided` becomes 1\n* `alpha` becomes 0.025\n\nYou may be wondering, why would we change our $\\alpha$ from 0.05 to 0.025? In practice, we could choose a more liberal one-sided $\\alpha=0.05$, however this would change our critical value from `r qnorm(0.975)` at $\\alpha=0.025$ to `r qnorm(0.95)` at $\\alpha=0.05$. This could increase our risk of a type I error (i.e., falsely finding an effect when none exists). Depending on the context, we may wish to use the more conservative one-sided $\\alpha=0.025$.\n\nNow, let's examine the modified code and observe the boundaries. We'll add a comparison with $\\alpha=0.05$ for our O'Brien-Fleming boundary for comparison:\n\n```{r}\neo_of1 <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.025) # O'Brien-Fleming\neo_hp1 <- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=1, alpha=0.025) # Haybittle-Peto\neo_asof1 <- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp1 <- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending Pocock-like boundary\n\n# add OBF with 0.05 alpha\neo_of1_alpha05 <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.05) # O'Brien-Fleming with 0.05 boundary\n\n# extract critical values\neo_of1_crit <- eo_of1$criticalValues\neo_hp1_crit <- eo_hp1$criticalValues\neo_asof1_crit <- eo_asof1$criticalValues\neo_asp1_crit <- eo_asp1$criticalValues\neo_of1_alpha05_crit <- eo_of1_alpha05$criticalValues\n\n# compare OBF with 0.025 and 0.05\nrbind( 'Two-Sided OBF with alpha=0.05'=eo_of_crit, 'One-Sided OBF with alpha=0.025'=eo_of1_crit, 'One-Sided OBF with alpha=0.05'=eo_of1_alpha05_crit)\n```\n\nFirst, we see the top two rows are the same for our two-sided O'Brien-Fleming boundary with $\\alpha=0.05$ and our one-sided O'Brien-Fleming boundary with $\\alpha=0.025$. This is because our two-sided boundary assumed that our $\\alpha=0.05$ was distributed symmetrically with 0.025 on both sides.\n\nFrom the comparison of our one-sided O'Brien Fleming boundaries, the lower row with $\\alpha=0.05$ has lower critical value thresholds, making it more likely to declare significance. \n\nFor visual comparison of boundaries:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_alpha05_crit, type='o', lwd=2, pch=16, col='gray65')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof1_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp1_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp1_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','gray65','green4','orangered2','blue'), lwd=2, legend=c('OF','OF 0.05','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* We can visually see that the O'Brien-Fleming with $\\alpha=0.025$ (OF) has higher critical value thresholds compared to the O'Brien-Fleming boundary with $\\alpha=0.05$ (OF 0.05).\n* All boundaries here are identical to our previous two-sided figure since we used $\\alpha=0.025$, but there is no lower boundary.\n* For one-sided hypothesis tests, the directionality of our test is *very* important. We must be careful to ensure we are implementing the proper test:\n    * If we are testing $H_1\\colon \\mu_{trt} > \\mu_{con}$ and our estimate is $\\delta = \\mu_{trt} - \\mu_{con}$, then we would want a positive critical value to stop the study early for efficacy in our one-sided test. For example, if $Z=5$ we would stop at any interim look for all presented methods.\n    * If $Z=-5$ for our one-sided test with $H_1\\colon \\mu_{trt} > \\mu_{con}$, we would continue the study because we do not have evidence of efficacy. *(However, if we had incorporated interim monitoring for futility we might have stopped for futility.)*\n\n\n## `rpact` and Monitoring for Only Futility\n\nOur previous section examined efficacy-only interim monitoring. We can also design studies where we are only monitoring for futility, indicating we would only stop early if it is highly unlikely that we would be able to declare efficacy at the conclusion of the trial if we reached full enrollment.\n\n### Two-Sided Futility Boundaries\n\nLet's start by exploring a few different boundaries and compare them graphically. We'll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed. Using `getDesignGroupSequential()`, we need to add an argument that manually defines $\\alpha$-spending via the `userAlphaSpending` argument:\n\n```{r}\n# Two-sided O'Brien-Fleming futility boundaries\nfo_of <- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.05), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock futility boundaries\nfo_p <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.05, userAlphaSpending = c(0,0,0,0.05), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n```\n\nLet's dissect some of the output and how it may look differently from our efficacy-only boundary:\n\n```{r}\nprint(fo_of)\n```\n\n\nFrom `print(fo_of)` in the lower `Output:` section:\n\n* We now have a `futilityBoundsNonBinding` line that only includes 3 values, one of which is `NA`:\n    * These three values represent our first three stages, which correspond to our interim analyses since the fourth stage means the trial is complete.\n    * The first interim look with this design is `NA`, indicating that for the chosen boundary type and location of the interim look there is no setting where the design would recommend terminating for futility. In other words, we will always enroll at least 50% of the participants in our study using these O'Brien-Fleming boundaries for futility monitoring, assuming the DSMB or external factors do not terminate the trial before completion.\n    * Note, the because there is a stage without a valid futility boundary, the default behavior in `rpact` is to reallocate our $\\beta$-spending to later stages. If you wish to leave the first stage $\\beta$ \"unspent\", you can add the argument `betaAdjustment=F` to the function.\n* The `Critical values` line includes 4 values, three of which are $\\infty$:    \n    * The \"fourth\" stage critical value is represented by the `Critical values` row, where we see the final threshold is our $Z$-score based on a two-sided test with $\\alpha=0.05$ (i.e., `qnorm(0.975)`=1.96).\n    * The three `Inf` values indicate it is impossible to stop for efficacy since we will never observe a test statistic of $Z=\\infty$.\n    * In practice, we could also set the critical values to be very large (e.g., $Z=5$) since this would correspond to a very large treatment effect.\n\nLet's create our plots in a similar way to our efficacy examples to see these two-sided boundaries.\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of_fbnd <- fo_of$futilityBounds\nfo_p_fbnd <- fo_p$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of_crit <- fo_of$criticalValues\nfo_p_crit <- fo_p$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* Unlike with efficacy boundaries, Pocock (P) futility boundaries are not constant over all stages.\n* As noted before, the O'Brien-Fleming (OF) boundaries do not have an interim stopping rule at the first stage.\n\n\n\n### One-Sided Futility Boundaries\n\nWe can also modify our previous futility boundaries for a one-sided hypothesis test. Here we will again change $\\alpha=0.05$ to $\\alpha=0.025$ for consistency with our previous example:\n\n```{r}\n# One-sided O'Brien-Fleming futility boundaries\nfo_of1 <- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.025, # defines alpha level for one-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.025), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# One-sided Pocock futility boundaries\nfo_p1 <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n```\n\nLet's create our plots in a similar way to our efficacy examples to see these two-sided boundaries. \n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of1_fbnd <- fo_of1$futilityBounds\nfo_p1_fbnd <- fo_p1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of1_crit <- fo_of1$criticalValues\nfo_p1_crit <- fo_p1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of1_fbnd, fo_of1_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p1_fbnd, fo_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\n* With a one-sided stopping boundary, the O'Brien-Fleming boundaries now have a critical value at the first stage! Since it is negative we can identify that we would have to observe a large effect in the \"wrong\" direction from our hypothesis test to stop after only 25% of the study was enrolled.\n* The O'Brien-Fleming boundary is less aggressive than the Pocock boundary. This suggests that a Pocock design may reduce our power without making other modifications (e.g., increased $N_{max}$ for the study relative to a fixed sample design).\n\nWe can also compare the stopping boundaries to see how they change between our one- and two-sided designs:\n\n```{r}\nrbind( 'OBF Two-Sided'=c(fo_of_fbnd, fo_of_crit[4]), 'OBF One-Sided'=c(fo_of1_fbnd, fo_of1_crit[4]),\n       'Pocock Two-Sided'=c(fo_p_fbnd, fo_p_crit[4]), 'Pocock One-Sided'=c(fo_p1_fbnd, fo_p1_crit[4]))\n```\n\nWe see in the table that while boundaries are similar between one- and two-sided cases, they do have some differences, especially at earlier stages.\n\n\n## `rpact` and Monitoring for Both Efficacy and Futility\n\nNow that we've covered both efficacy-only and futility-only interim monitoring, we may be interested in seeing how we can combine them together within one design. Here we'll incorporate our code from above to leverage stopping boundaries for both efficacy and futility.\n\n\n### Two-Sided Boundaries for Efficacy and Futility\n\nIt is possible to choose different boundaries styles for futility and efficacy. Here we will consider three designs: alpha/beta-spending O'Brien-Fleming-like boundaries for both, Pocock for both, or alpha-spending with O'Brien-Fleming-like boundaries for efficacy and Pocock for futility.\n\n```{r}\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of <- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.05, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n\n# \nef_ofp <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.05,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n```\n\nLet's jump right into our visualized boundary shapes:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of_fbnd <- ef_of$futilityBounds\nef_p_fbnd <- ef_p$futilityBounds\nef_ofp_fbnd <- ef_ofp$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of_crit <- ef_of$criticalValues\nef_p_crit <- ef_p$criticalValues\nef_ofp_crit <- ef_ofp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nThis figure is a lot busier than our previous figure, but we have some takeaways here still:\n\n* There are now 3 outcomes at each interim stage: stop for efficacy, stop for futility, or continue to the next stage.\n* By the end of the trial, we see the boundaries have all converged.\n* When mixing the O'Brien-Fleming for efficacy and Pocock for futility we see that the efficacy boundaries are the same as the O'Brien-Fleming for both efficacy and futilty design, but the futility boundaries have shifted since they need to arrive at the same efficacy threshold.\n\nFor ease of viewing, we can also plot each design separately in a panel figure:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# Plot boundaries\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,1.1)) # 2x2 panel figure\n\n# OBF\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"O'Brien-Fleming-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"Pocock-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# OBF+Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"OBF for Eff, P for Fut\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n```\n\n\n### One-Sided Boundaries for Efficacy and Futility\n\nLet's now examine how our boundaries change with a one-sided example when monitoring for both futility and efficacy. As before, we will change $\\alpha$ to 0.025:\n\n```{r}\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of1 <- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.025, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p1 <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.025, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n\n# \nef_ofp1 <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.025,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n```\n\nLet's jump right into our visualized boundary shapes:\n\n```{r}\n#| echo: true\n#| code-fold: true\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of1_fbnd <- ef_of1$futilityBounds\nef_p1_fbnd <- ef_p1$futilityBounds\nef_ofp1_fbnd <- ef_ofp1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of1_crit <- ef_of1$criticalValues\nef_p1_crit <- ef_p1$criticalValues\nef_ofp1_crit <- ef_ofp1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_fbnd, ef_of1_crit[4]), type='o', lwd=2, pch=16) # futility \nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_crit), type='o', lwd=2, pch=16) # efficacy \n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_fbnd, ef_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_fbnd, ef_ofp1_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n```\n\nA similar story emerges from our previous examples:\n\n* Pocock has the biggest penalty for efficacy monitoring as evidenced by its larger critical value at 100% information.\n* O'Brien-Fleming-like boundaries for both efficacy and futility have the same efficacy boundaries when using OBF for efficacy but Pocock for futility, however the futility bondaries change compared to the Pocock-like boundaries.\n* With a one-sided test, the direction of our hypothesis matters. This figure suggests that everything on the bottom portion of the graph would result in futility stopping.\n\n\n# Simulation Study\n\nNow that we've seen examples of how to calculate boundaries and visualize them, let's explore a brief simulation study. We will evaluate the following interim monitoring settings:\n\n* Two-sided futility only with O'Brien-Fleming-like beta-spending boundaries\n* Two-sided efficacy only with O'Brien-Fleming-like alpha-spending boundaries\n* Two-sided efficacy and futility with O'Brien-Fleming-like alpha/beta-spending boundaries\n* Two-sided efficacy and futility with Pocock-like alpha/beta-spending boundaries\n* Fixed sample design\n\nFor these scenarios, we will assume 5 total looks, $\\alpha=0.05$, and $\\beta=0.8$. We will evaluate three scenarios:\n\n* Null scenario with no difference between arms with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0,1)$\n* Alternative scenario with clinically meaningful difference between groups with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0.4,1)$\n* Alternative scenario with half the clinically meaningful difference between groups with $A_1 \\sim N(0,1)$ and $A_2 \\sim N(0.2,1)$\n\nOur sample sizes will be determined based on the $n$ needed for the alternative scenario:\n\n```{r}\npower.t.test(delta=0.4, sd=1, sig.level=0.05, power=0.8)\n```\n\nRecall, we always round up to the next integer to ensure that we have the desired power level of 80%. In other words, 99.08057 becomes 100 per group.\n\nOur next code chunk provides a simple step-by-step simulation. The chunk is hidden by default, but you can unhide it to see the behind-the-scenes structure or to modify the setting for anything you are interested in! The general set-up will also work for other types of outcomes (e.g., binary, ordinal, etc.), but will need different data generating mechanisms (i.e., the `rxxxx` functions).\n\nFirst, let's estimate the boundaries we will need for each simulation based on our five scenarios. Here we will assume everything is estimated on a $Z$-scale. For simplicity, we will also leverage the fact that all our boundaries are symmetric and we will evaluate the $|Z|$ (i.e., the absolute value of $Z$):\n\n```{r}\n#| echo: true\n#| code-fold: true\n## Two-sided futility only with O'Brien-Fleming-like beta-spending boundaries\nfo_of <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.05, userAlphaSpending = c(0,0,0,0,0.05), \n                                 typeBetaSpending = \"bsOF\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nfo_of_bounds <- cbind( futility = c(fo_of$futilityBounds, fo_of$criticalValues[5]),\n                       efficacy = fo_of$criticalValues)\n\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nfo_of_bounds[ is.na(fo_of_bounds )] <- 0\n\n\n## Two-sided efficacy only with O'Brien-Fleming-like alpha-spending boundaries\neo_of <- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 5, sided=2, alpha=0.05) # O'Brien-Fleming\n\n# create object that stores all boundaries with each row representing stage 1-5\n# note, no futility monitoring in this design so we will place all values as 0 at stages 1-4\neo_of_bounds <- cbind( futility = c(rep(0,4), fo_of$criticalValues[5]),\n                       efficacy = eo_of$criticalValues)\n\n\n## Two-sided efficacy and futility with O'Brien-Fleming-like alpha/beta-spending boundaries\nef_of <- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.05, \n                                 typeBetaSpending = \"bsOF\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nef_of_bounds <- cbind( futility = c(ef_of$futilityBounds, ef_of$criticalValues[5]),\n                       efficacy = ef_of$criticalValues)\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nef_of_bounds[ is.na(ef_of_bounds )] <- 0\n\n\n## Two-sided efficacy and futility with Pocock-like alpha/beta-spending boundaries\nef_p <- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.05, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=2, beta=0.2)\n\n# create object that stores all boundaries with each row representing stage 1-5\nef_p_bounds <- cbind( futility = c(ef_p$futilityBounds, ef_p$criticalValues[5]),\n                       efficacy = ef_p$criticalValues)\n# since we using |Z|, we can replace our NA futility bound with 0 since it reflects an impossible stopping point at the first stage\nef_p_bounds[ is.na(ef_p_bounds )] <- 0\n\n\n## Fixed sample design\nfs_bounds <- cbind( futility = c(rep(0,4), qnorm(0.975) ),\n                    efficacy = c(rep(Inf,4), qnorm(0.975)) )\n```\n\nIn addition to estimating the boundaries, we created an object we can use for reference in the simulation study:\n\n```{r}\n# print example of bounds for two-sided OBF for futility and efficacy\nef_of_bounds\n```\n\nSince we are leveraging that the boundaries are symmetric, we replace the stage 1 futility `NA` boundary with a 0, since $Pr(|Z|<0)=0$. Likewise, for the fixed sample design we see:\n\n```{r}\n# print fixed sample boundaries\nfs_bounds\n```\n\nFor efficacy, we know $Pr(|Z|>\\infty)=0$, representing a case where we cannot stop for futility or efficacy until the trial has concluded.\n\nFor each scenario we will simulate data for 1000 hypothetical trials, apply our stopping rules, and summarize the:\n\n* Rejection rate for each scenario (e.g., type I error rate for null scenarios, power for alternative scenarios)\n* Expected sample size and its standard deviation\n* Average stopping stage and its standard deviation (directly related to ESS, but may be nice to present in different ways)\n* Proportion of simulated trials stopping early and for what reason\n\n## Null Scenario Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own.\n\n```{r null-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 5145 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nRemembering that this is the null scenario where we simulated *no difference* between study arms, we can take away numerous nuggets of information:\n\n* Having the fixed sample design is important to describe what the expected type I error rate (i.e., rejection rate) is for our simulated data. While $\\alpha=0.05$, we see that in this simulation of 1000 trials the observed type I error rate was 4.2%. In other words, $1000 \\times 0.042 = 42$ trials rejected the null hypothesis that $\\mu_1 = \\mu_2$, even though we simulated data where the means were equal.\n* In terms of type I error control, relative to the fixed sample design:\n    * The OBF designs with futility have lower type I error rates\n    * The OBF efficacy only and Pocock E+F designs have increased type I error rates\n* In terms of the expected sample size:\n    * Interim monitoring helped to reduce the average sample size needed until study termination\n    * Pocock has the lowest ESS and average stopping point because it is more aggressive with early stopping\n    * The OBF efficacy only design rarely stopped, which is good since it would only stop for efficacy\n    * Designs with futility stopping terminated at a point prior to trial conclusion ~80% of the time! This represents a much more efficient design relative to a fixed sample design when there is no effect.\n* Since we simulated the null scenario here, we shouldn't stop for efficacy. However, we see that:\n    * The OBF Efficacy Only design stopped 2.1% of the time for efficacy\n    * The OBF Efficacy+Futility design stopped 2.0% of the time for efficacy\n    * The Pocock Efficacy+Futility design stopped 5.4% of the time, helping to see the more aggressive stopping boundaries\n\nOf course, all of the above only considers the null scenario. For a complete view of the trade-offs, we should look at alternative scenarios as well.\n\n\n## Alternative Scenario I Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., `mean1` or `mean2`), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n```{r alt1-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0.4 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 7354 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nRemembering that this is the alternative scenario where we simulated a difference of $\\delta=0.4$ between study arms which was used to estimate the sample size needed for 80% power, we can take away numerous nuggets of information:\n\n* Having the fixed sample design is important to describe what the expected power (i.e., rejection rate) is for our simulated data. While $\\beta=0.2$, we see that in this simulation of 1000 trials the observed power was 79.8%, which is quite close to the target of 80%.\n* In terms of power, relative to the fixed sample design:\n    * All designs with futility monitoring for these boundaries have reduced power (~5% less for OBF, but 13.1% less for Pocock!)\n    * The OBF efficacy only design has slightly higher power at 79.9% (i.e., 1 more out of 1000 simulated trials was found to be significant compared to the fixed sample design)\n* In terms of the expected sample size:\n    * All designs show a lower ESS than the fixed sample design\n    * The OBF efficacy design has slightly higher power *and an ESS that is about 41 participants lower*. We see based on the average stopping segment that it is stopping, on average, after 80% of participants have been observed.\n    * The Pocock design has the lowest ESS, but this is because it stops for futility aggressively 28.1% of the time, even though we simulated data with a difference.\n\nWe need to balance these results with performance under the null scenario, taking into account if we are willing to tolerate reductions in power for the trade-off of more futility stopping in the null, or vice versa.\n\n### Note on Futility Stopping Under the Alternative\n\nAn interesting quirk worth noting is that futility stopping may still be beneficial in some settings. For example, our target is 80% power, meaning that 20% of the time our study will not detect the effect even though it exists. For the 20% of trials that were not significant, it still may be helpful to stop the trial early to conserve resources.\n\nFor example, let's observe the confusion matrix of the OBF design stopping for efficacy and futility with the fixed sample design:\n\n```{r}\ntable( OBF = sim_res_ef_of[2,], FS = sim_res_fs[2,])\n```\n\nFrom this table, we see that of the 202 simulated trials that didn't detect the effect, 92.1% (186/202) stopped for futility early. The trade-off, of course, is that we see of the 798 trials the did detect an effect, 7.5% (60/798) stopped for futility that would have gone on to reject the null hypothesis. The final piece of information to note is that 7.9% (16/202) of trials where the fixed sample did not reject $H_0$, the OBF approach stopped early for efficacy and detected an effect. \n\nIt can be hard to juggle all these conflicting pieces of information, but it does highlight that we are often interested in trying to minimize the trade-off that harms out overall trial performance. In this case, the OBF design stopping for both efficacy and futility does reduce power to 75.4% from 79.8% in the fixed design, but it does so with a reduced type I error rate in the null scenario with a large reduction in ESS.\n\nHowever, it is worth noting in practice we don't know if a trial stopping for futility is truly null, or we happened to observe something null-ish by chance. To address this, we may wish to evaluate other adaptive elements like sample size re-estimation to increase the probability of conducting a successful trial.\n\n\n## Alternative Scenario II Results\n\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the half of the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., `mean1` or `mean2`), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n```{r alt2-simulation, cache=T}\n#| echo: true\n#| code-fold: true\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 <- 0 # arm 1 mean\nmean2 <- 0.2 # arm 2 mean\nsd1 <- 1 # arm 1 sd\nsd2 <- 1 # arm 2 sd\nn1 <- 100 # arm 1 sample size\nn2 <- 100 # arm 2 sample size\nnsim <- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed <- 7811 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 <- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 <- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t <- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 <- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 <- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats <- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func <- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval <- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) > 0 ){\n    nstage <- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind <- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des <- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage <- length(test_statsv)\n    sig_ind <- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des <- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    <- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list <- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx <- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   <- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge <- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage <- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean <- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd <- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge <- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum <- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum <- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum <- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per <- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab <- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) <- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %>%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n```\n\nIn this scenario, we simulated a design where there is an effect, but it is only half that of what we used for the power calculation (i.e., $\\delta=0.2$ in this simulation versus $\\delta=0.4$ used in our power calculation). If this effect size is no longer clinically relevant, we would hope to stop for futility without having to carry out the entire study. If this effect is relevant, other adaptive methods like sample size re-estimation are needed to increase our power.\n\nFor this scenario, we see that designs with futility monitoring stop between 61.9-72.2% of the time. The power of the fixed sample design is low at 29.6%, with all designs including any interim monitoring showing lower power. This is partially due to futility monitoring if implemented, but also because the final testing threshold is adjusted for multiple testing making it harder to reject. Likely, many of the cases that are discordant are because the test statistic fell between `qnorm(0.975)`=1.96 and the adjusted threshold for each method.\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n\n* [Guidance on interim analysis methods in clinical trials](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/guidance-on-interim-analysis-methods-in-clinical-trials/5051FDCF5284970B3DB01FE609AAA4C2?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper focusing on interim analyses in clinical trials with included case studies"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["../header.html","../header-tracker.html"],"css":["../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"flatly","title":"Interim Monitoring for Futility/Efficacy/Safety","toc_float":true,"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}