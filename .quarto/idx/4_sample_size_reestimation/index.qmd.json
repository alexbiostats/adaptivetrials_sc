{"title":"Sample Size Re-Estimation","markdown":{"yaml":{"title":"Sample Size Re-Estimation","toc":true,"toc_float":true,"toc-location":"left","format":{"html":{"code-fold":"show","code-overflow":"wrap","code-tools":true}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n\nOftentimes our power calculations represent our best guess at a realistic treatment effect, but even using previous studies or extensive clinical/scientific background can still result in uncertainty. In this module we discuss how we can incorporate re-estimation procedures during the trial to better ensure we enroll sufficient participants to detect the observed effect.\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/4664c44f42e147b1bebe71608bda8c88\" title=\"Sample Size Re-Estimation\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/4_sample_size_reestimation.pptx).\n\n\n# Code Examples in R\n\nA selection of packages that may be helpful in implementing a re-estimation procedure include:\n\n* [`blindrecalc`](https://cran.r-project.org/web/packages/blindrecalc/index.html): a package for *blinded* sample re-estimation based on estimating nuisance parameters\n* [`rpact`](https://www.rpact.org/): a package for confirmatory adaptive clinical trial design, simulation, and analysis; includes functions for combination testing for use in unblinded designs\n* [`gsDesign`](https://keaven.github.io/gsDesign/): includes the `ssrCP()` function for unblinded re-estimation based on conditional power\n* [`esDesign`](https://cran.r-project.org/web/packages/esDesign/): looks at adaptive enrichment designs with sample size re-estimation\n\n\n## Blinded Example with Binary Outcome\n\nIn our first example we observe how the `blindrecalc` package can be used for a study with a binary outcome. We will use a chi-squared test as our motivating example, largely following the steps from their helpful [R Journal paper](https://journal.r-project.org/articles/RJ-2022-001/RJ-2022-001.pdf).\n\nFirst we set up our design as the chi-squared test using `setupChiSquare()`. In this example we are testing a one-sided hypothesis where $H_1\\colon p_1 > p_2$ with $\\alpha=0.025$, $\\beta=0.2$ (i.e., power of 80%), and we are interested in detecting a difference between two groups of 20% (i.e., $\\delta=0.2$).\n\nWe can then estimate the number needed in a fixed sample design using `n_fix()`. Here the nuisance parameter represents the average of the two groups. We can then estimate the $N$ needed overall with $N/2$ in each randomized group to detect a difference of 20%.\n\n```{r}\nlibrary(blindrecalc)\n\n# Compare basic functions\ndesign <- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn_fix(design, nuisance = c(0.2, 0.3, 0.4, 0.5))\n```\n\nWe can verify these sample sizes are analogous to the `power.prop.test()` function in base R:\n\n```{r}\n### Check that power.prop.test matches n_fix (which it does)\n## round up N for each group, multiple by 2 to match overall sample size from n_fix\n\n# nuisance 0.2\na1 <- ceiling(power.prop.test(p1=0.1,p2=0.3,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.3\na2 <- ceiling(power.prop.test(p1=0.2,p2=0.4,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.4\na3 <- ceiling(power.prop.test(p1=0.3,p2=0.5,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.5\na4 <- ceiling(power.prop.test(p1=0.4,p2=0.6,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# print sample sizes\nc(a1,a2,a3,a4)\n```\nLet's focus on the `nusiance=0.2` case. We can examine the impact on our design if we observe other nuisance parameters even though we anticipated 0.2. The `toer()` function allows us to estimate the type I error rate we may encounter if we do or do not use blinded re-estimation:\n\n```{r toer-blindrecalc, cache=T}\ndesign <- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn <- n_fix(design, nuisance = 0.2) # N = 124 total (62 per group)\np <- seq(0.2, 0.6, by = 0.1)\ntoer_fix <- toer(design, n1 = n, nuisance = p, recalculation = FALSE)\ntoer_ips <- toer(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\nt1e_tab <- rbind( \"No SSR\" = toer_fix, \"SSR at 1/2 Enrolled\" = toer_ips)\ncolnames(t1e_tab) <- p\nt1e_tab\n```\nBy comparing these type I error rates, we see that there can be an inflation to our desired $\\alpha=0.025$ due to observing different averages between our two groups (i.e., the nuisance parameters reflected by the column headers). With blinded SSR, we see better control of the type I error rate.\n\nWe can use the similar `pow()` to estimate the power under our design with varying nuisance parameters:\n\n```{r pow-blindrecalc, cache=T}\npow_fix <- pow(design, n1 = n, nuisance = p, recalculation = FALSE)\npow_ips <- pow(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\npow_tab <- rbind( \"No SSR\" = pow_fix, \"SSR at 1/2 Enrolled\" = pow_ips)\ncolnames(pow_tab) <- p\npow_tab\n```\nFor power we see that SSR maintains approximately 80% power, but without SSR the power decreases as or nuisance parameter grows.\n\nWe can also plot the distribution of sample sizes that would occur at different nuisance parameters to evaluate our potential risk of having a much larger sample size:\n\n```{r}\nn_dist(design, n1 = n/2, nuisance = p, plot = TRUE)\n```\n\nIn this figure above we see that if our nuisance parameter is actually 0.2 as assumed for the power calculation, or sample size re-estimation could range from 62 to 184 versus our planned $N=124$. However, this does assume we allow for sample size reductions and have no limit on the maximum increase. If our guess of the nuisance parameter is 0.5, we see the largest increase in our expected sample size up to 191.8, with a range of 154 to 194.\n\n\n\n## Unblinded Conditional Power Example with Binary Outcome\n\nThe `gsDesign` package includes the `ssrCP()` which allows us to implement an *unblinded* sample size re-estimation procedure. Here we continue our previous example using a binary outcome. In this case we will incorporate traditional group sequential O'Brien-Fleming boundaries for interim monitoring for efficacy. This means our sample size will also be adjusted relative to a fixed design to account for the use of the traditional GSD (this is in contrast to alpha-spending boundaries that are OBF-like, where the sample size remains fixed).\n\nFirst, we must establish the design type based on the `gsDesign()` function before extending to a conditional power design with `ssrCP()`:\n\n```{r}\nlibrary(gsDesign) # load package\n\nx <- gsDesign(\n  k = 2, # number of analyses planned, including interim and final\n  n.fix = 196, # sample size for a fixed design with no interim\n  timing = 0.5, # timing of interm analyses \n  test.type=2, # 6 options covering one- vs. two-sided and symmetric vs. asymmetric boundaries; 2 is a two-sided symmetric approach\n  alpha = 0.025, # one-sided type I error rate\n  beta = 0.2, # type II error rate (i.e., power=1-beta)\n  delta0 = 0, # null hypothesis parameter (i.e., no difference)\n  delta1 = 0.2, # alternative hypothesis parameter (i.e., difference we wish to detect)\n  sfu='OF' ) # alpha-spending for efficacy monitoring\n\n# plot stopping boundaries\nplot(x)\n```\n\nThe plot shows our stopping boundaries for our two-sided test, with the final critical value being 1.98 (versus 1.96 in a fixed design without interim monitoring), as well as an increase from $N=196$ to $N=198$.\n\nWe can then extend this design to a conditional power design based on an observed test statistic at the end of our first stage. Here we assume we observed $Z=1.6$:\n\n```{r}\n# extend design to a conditional power design\nxx <- ssrCP(x = x, # provide design used\n            z1 = 1.6, # enter observed test statistic\n            overrun = 0, # can note how many participants are enrolled but not included in the interim analysis\n            beta = 0.2, # targeted type II error for SSR (i.e., targeted power=1-beta)\n            cpadj = c(0.5,0.8), # range of conditional powers for which SSR is to be performed, otherwise N from original design used\n            maxinc = 2, # argument limiting maximum fold-increase from planned max N (e.g., 2 times)\n            z2 = z2NC) # combination function to combine stage 1 and stage 2 results; z2NC=inverse normal combination test, z2Z=sufficient stat for complete data, z2Fisher=Fisher's combination test\n\n# show immediately relevant information\nxx$dat\n```\n\nFrom the `ssrCP` documentation, we see these values represent:\n\n* z1: input z1 values, \n* z2: computed cutoffs for the standard normal test statistic based solely on stage 2 data\n* n2: stage 2 sample size (however, based on other functions I believe this may be the maximum sample size to enroll, so we need to take n2-n1)\n* CP: stage 2 conditional power \n* theta: standardize effect size used for conditional power calculation \n* delta: the natural parameter value corresponding to theta The relation between theta and delta is determined by the delta0 and delta1 values from x: delta = delta0 + theta(delta1-delta0).\n\nThe most important summary is the re-estimated sample size of $N=259.0207$ which rounds up to $N=260$, resulting in a need to enroll $N_2=260-99=161$ in stage 2 instead of the original $N_2 = 198-99=99$. Overall, this results in a total sample size of $N_1+N_2=99+161=260$, which is less than the two times inflation allowed by `maxinc=2` (i.e., up to $198\\times2 = 396$ is allowed).\n\nThe other useful summary is the `z2=1.19651`, which represents that we need to observe a test statistic at least this large for our inverse normal combination test to be significant.\n\nIf we assume we enroll the 161 additional participants and observe $Z_2=1.3$, we would have a normal combination test of \n\n$$ \\frac{Z_1 + Z_2}{\\sqrt{2}} = \\frac{1.6 +1.3}{\\sqrt{2}} = 2.05 > 1.96 = Z_{0.975} = Z_{1-\\alpha/2} $$\n\nTherefore, we would reject the null hypothesis and conclude we found an effect.\n\n\n# Simulation Study\n\nFor our simulation study, let's assume we are interested in designing a study where we wish to reduce the rate of an adverse event by exploring a new approach to a procedure:\n\n* $H_0\\colon p_1 = p_2$ versus $H_1\\colon p_1 < p_2$ (i.e., one-sided hypothesis)\n* $p_1 = p_{trt} = 0.1$ and $p_2 = p_{con} = 0.25$ for our alternative hypothesis of interest (i.e., reducing the rate of adverse events from 25% to 10%)\n* $\\alpha=0.025$\n* $\\beta=0.8$\n\nFor a fixed sample design, using `power.prop.test()`, we can identify our sample size to enroll:\n\n```{r}\npower.prop.test(p1=0.1,p2=0.25,sig.level=0.025,power=0.8, alternative='o')\n```\nGiven $n=100$ per arm, we would plan for a study enrolling a total of $N_{total}=200$.\n\nWe will evaluate five scenarios to determine the effect of sample size re-estimation:\n\n* Scenario 1: Null with $p_1 = p_{trt} = 0.25$ and $p_2 = p_{con} = 0.25$\n* Scenario 2: Null with $p_1 = p_{trt} = 0.175$ and $p_2 = p_{con} = 0.175$, where the null rate is the nuisance parameter (i.e.,$\\frac{0.1+0.25}{2}=0.175$)\n* Scenario 3: Alternative with $p_1 = 0.1$ and $p_2 = 0.25$\n* Scenario 4: Alternative with $p_1 = 0.15$ and $p_2 = 0.25$\n* Scenario 5: Alternative with $p_1 = 0.15$ and $p_2 = 0.30$ (i.e., maintaining the $\\delta$ but for different effects)\n\n\n## Blinded Re-estimation with Binary Outcome Simulation\n\nWe first implement our blinded re-estimation procedure. We will compare three strategies:\n\n1. Blinded SSR where we will allow for a smaller than planned sample size if indicated (i.e., if stage 2 needs fewer than 100 more participants, we will enroll that number)\n2. Blinded SSR where we will continue with the planned sample size if the re-estimation indicates fewer participants could be needed (i.e., if stage 2 needs fewer than 100 more participants, we will still enroll 100)\n3. A fixed sample design with no SSR\n\nThe code is hidden, but can be shown if desired. We will summarize the rejection rate and average (SD) sample size across 1,000 simulation trials.\n\n```{r blindedSSR-binary-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nsim_list <- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nblinded_res <- blinded_n2_res <- fixed_res <- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\nn <- 200 # total sample size based on fixed sample\nn1 <- 100 # sample size to enroll for stage 1\ndelta <- 0.15 # expected effect size under H1 from power calculation\nr <- 1 # randomization ratio (e.g., 1:1)\nnsim <- 1000\n\n###\n# simulate method with SSR allowing for smaller than expected N\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 <- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 <- p0 - delta*(r/(1+r))\n    pc_n1 <- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest <- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 <- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 < 0 ){ n2 <- 0 } # if sufficient sample size already, set to 0\n    \n    # simulate stage 2 data\n    trt <- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con <- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate method with SSR but enrolling at least (N-N1) in stage 2 (i.e., not allowing fewer participants)\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 <- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 <- p0 - delta*(r/(1+r))\n    pc_n1 <- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest <- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 <- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 < (n-n1) ){ n2 <- (n-n1) } # enroll at least (n-n1)\n    \n    # simulate stage 2 data\n    trt <- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con <- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_n2_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 <- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 <- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- cbind(fixed_res,blinded_res,blinded_n2_res)\n\nkbl_tab %>%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with Lower N2\"=3, \"SSR with At Least N2\"=3))\n```\n\nFrom these simulation results we can see that:\n\n* Without some form of futility testing, the blinded SSR methods lead to increased sample sizes in the null scenario with 25% versus 25%, as well as slightly higher type I error rates than the fixed sample design.\n* If the null scenario response reflects the nuisance parameter, we still see slightly higher type I error rates with SSR. However, the ESS increase is less large, likely because the effect went closer to 0. This may indicate that less prevalent outcomes are less affected by the null scenario with blinded re-estimation.\n* If we encounter the effect sizes used in the power analysis, we see that SSR that enrolls *at least* the $N_{total}=200$ of the fixed sample design increases power by allowing for larger sample sizes. On average, across all 1000 simulated trials, the average sample size was 212.7 (SD=18.5).\n* If we encounter an underpowered scenario, we see that the SSR methods also lead to an increased SSR but only slightly higher power. This suggests that either unblinded re-estimation processes or potentially futility monitoring could further improve performance.\n* Finally, if we observe the same $\\delta=0.15$ but at higher observed rates ($p_{trt}=0.15$ and $p_{con}=0.30$), our blinded SSR increases power by 5.9 to 7.3% by allowing our sample size to increase based on the higher nuisance paramter (i.e., $\\frac{0.15+0.3}{2}=0.225$).\n\n\n## Unblinded Re-estimation with Binary Outcome Simulation\n\nWe can compare the results from our blinded SSR approach with an approach using unblinded SSR approaches. In this simulation we compare:\n\n* An approach using conditional power estimated from `gsDesign::ssrCP()`, where we assume that the second stage must have at least 100 participants but could increase to 200 participants (i.e., we don't allow fewer than expected participants in stage 2). This approach uses the inverse normal combination test based on the p-values from the two-sample test of proportions.\n* An approach that unblinds the control arm to use in re-estimating the `power.prop.test()` calculation for a decrease of 15% in the treatment arm. If the control arm has a response rate less than 15%, we stop for futility and calculate the one-sided p-value to record for futility. Otherwise, we allow the re-estimation to increase the sample size from 100 up to 200. Since we do not unblind the treatment arm, in this approach we evaluate the performance if we use the overall data for our final test p-value.\n* A fixed sample design enrolling 200 total participants without interim monitoring or re-estimation.\n\nThe simulation code block is hidden, but can be shown for review.\n\n```{r unblindedSSR-binary-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nlibrary(gsDesign) # load library\n\n# list of 5 simulation scenarios to mimic our blinded SSR\nsim_list <- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nunblinded_res <- unblinded_adhoc_res <- fixed_res <- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\ndelta <- 0.15 # expected effect size under H1 from power calculation\nr <- 1 # randomization ratio (e.g., 1:1)\nnsim <- 1000\n\nx <- gsDesign(\n  k = 2, \n  n.fix = 200, timing = 0.5, test.type=2,\n  alpha = 0.025, beta = 0.2, delta0 = 0, delta1 = 0.15, sfu='OF' ) \n\nn_fix <- 200 # total sample size based on fixed sample\nn1 <- ceiling(x$n.I[1])\nn <- ceiling(x$n.I[2]) \n\n###\n# simulate method with SSR using conditional power\n\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres_gs <- data.frame( zcombined=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    res_int <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')$p.value\n    \n    # UNblinded re-estimation\n    xx <- ssrCP(x = x, z1 = qnorm(1-res_int), overrun = 0, beta = 0.2, cpadj = c(0.5,0.8), maxinc = 1.5, z2 = z2NC)\n    n_rest <- ceiling(xx$dat$n2) - n1\n\t  n2 <- n_rest\n\t  n2 <- if(n2 <= 100){ 100 }else{n2} \n    if( res_int < x$upper$prob[1,1] ){ \n      # final analysis, save results\n      zcombined <- qnorm(1-res_int) / sqrt(1)\n      simres_gs[i,] <- c(zcombined, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 <- rbinom(n=n2/2, size=1, prob=pt)\n      con2 <- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res <- prop.test(x=c(sum(trt2),sum(con2)), n=c(length(trt2),length(con2)), alternative = 'less')$p.value\n      \n      zcombined <- ( qnorm(1-res_int) + qnorm(1-res) ) / sqrt(2)\n      \n      simres_gs[i,] <- c(zcombined, n1+n2)\n    }\n  }\n  \n  unblinded_res[combo,] <- c( mean(simres_gs$zcombined >= qnorm(1-(0.025))), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n###\n# somewhat ad hoc approach to unblinded SSR\n# first stage is used to implement re-estimation; no decreases in stage 2; up to 200 (versus 100) otherwise stop for futility\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes based on fixed sample to initialize\n  n <- 200\n  n1 <- 100\n  \n  # initialize object to save results in\n  simres_gs <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    res_int <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\n    \n    # UNblinded re-estimation, where we use the estimate of the control group and still power for a 0.15 decrease in treatment\n    n_reest <- if( mean(con) <= 0.15 ){ 10000 }else{ ceiling(power.prop.test(p1=mean(con)-0.15,p2=mean(con),sig.level=0.025,power=0.8, alternative='o')$n)*2 }\n    \n    n2 <- if( (n_reest - n1) < 100 ){100}else{n_reest - n1}\n    \n\t  if( n2 > 200 ){\n      # ad hoc futility rule, save first stage result\n      p <- res_int$p.value\n      simres_gs[i,] <- c(p, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 <- rbinom(n=n2/2, size=1, prob=pt)\n      con2 <- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(length(c(trt,trt2)),length(c(con,con2))), alternative = 'less')\n      p <- res$p.value\n  \n      simres_gs[i,] <- c(p, n1+n2)\n    }\n  }\n  \n  unblinded_adhoc_res[combo,] <- c( mean(simres_gs$p < 0.025), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes for fixed sample\n  n <- 200\n  n1 <- 100\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 <- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 <- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- cbind(fixed_res,unblinded_res,unblinded_adhoc_res)\n\nkbl_tab %>%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with CP\"=3, \"SSR Unblind Control\"=3))\n```\n\nThe simulation results indicate:\n\n* That the fixed sample design is slightly conservative with type I error of 2%, which is similar to the SSR with unblinded controls at 2.2%, although the sample size increases to an average of 212.6 (SD=39.0). \n* The SSR with conditional power is overly conservative, as seen by the lower type I error rates and power compared to the fixed sample design. \n* The SSR unblinded control shows the greatest improvement when the control reference is higher than expected (i.e., 30% instead of 25%), where power increases to 72.7% versus 68.9% for a fixed sample design. However, the ESS does increase to 224.6 (SD=51.8) versus 200 for the fixed sample design.\n* Some limitations in performance may be due to the fact that our allowed increase is somewhat limited in stage 2 from 100 to 200. In practice, if it was feasible to go higher we could likely improve our power relative to a fixed sample design.\n\nWhile only a limited set of methods and scenarios with a binary outcome, these results suggest the caution should be taken in implementing sample size re-estimation since it can introduce variability in our needed sample size while also potentially resulting in limited gains in power or type I error performance relative to a fixed sample design.\n\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n\n* [Guidance on interim analysis methods in clinical trials](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/guidance-on-interim-analysis-methods-in-clinical-trials/5051FDCF5284970B3DB01FE609AAA4C2?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper focusing on interim analyses in clinical trials with included case studies\n\n* [Kieser, Meinhard. *Methods and applications of sample size calculation and Recalculation in clinical trials.* Springer, 2020.](https://link.springer.com/book/10.1007/978-3-030-49528-2): textbook covering SSR methods and approaches\n\n","srcMarkdownNoYaml":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n# Overview\n\nOftentimes our power calculations represent our best guess at a realistic treatment effect, but even using previous studies or extensive clinical/scientific background can still result in uncertainty. In this module we discuss how we can incorporate re-estimation procedures during the trial to better ensure we enroll sufficient participants to detect the observed effect.\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/4664c44f42e147b1bebe71608bda8c88\" title=\"Sample Size Re-Estimation\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/4_sample_size_reestimation.pptx).\n\n\n# Code Examples in R\n\nA selection of packages that may be helpful in implementing a re-estimation procedure include:\n\n* [`blindrecalc`](https://cran.r-project.org/web/packages/blindrecalc/index.html): a package for *blinded* sample re-estimation based on estimating nuisance parameters\n* [`rpact`](https://www.rpact.org/): a package for confirmatory adaptive clinical trial design, simulation, and analysis; includes functions for combination testing for use in unblinded designs\n* [`gsDesign`](https://keaven.github.io/gsDesign/): includes the `ssrCP()` function for unblinded re-estimation based on conditional power\n* [`esDesign`](https://cran.r-project.org/web/packages/esDesign/): looks at adaptive enrichment designs with sample size re-estimation\n\n\n## Blinded Example with Binary Outcome\n\nIn our first example we observe how the `blindrecalc` package can be used for a study with a binary outcome. We will use a chi-squared test as our motivating example, largely following the steps from their helpful [R Journal paper](https://journal.r-project.org/articles/RJ-2022-001/RJ-2022-001.pdf).\n\nFirst we set up our design as the chi-squared test using `setupChiSquare()`. In this example we are testing a one-sided hypothesis where $H_1\\colon p_1 > p_2$ with $\\alpha=0.025$, $\\beta=0.2$ (i.e., power of 80%), and we are interested in detecting a difference between two groups of 20% (i.e., $\\delta=0.2$).\n\nWe can then estimate the number needed in a fixed sample design using `n_fix()`. Here the nuisance parameter represents the average of the two groups. We can then estimate the $N$ needed overall with $N/2$ in each randomized group to detect a difference of 20%.\n\n```{r}\nlibrary(blindrecalc)\n\n# Compare basic functions\ndesign <- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn_fix(design, nuisance = c(0.2, 0.3, 0.4, 0.5))\n```\n\nWe can verify these sample sizes are analogous to the `power.prop.test()` function in base R:\n\n```{r}\n### Check that power.prop.test matches n_fix (which it does)\n## round up N for each group, multiple by 2 to match overall sample size from n_fix\n\n# nuisance 0.2\na1 <- ceiling(power.prop.test(p1=0.1,p2=0.3,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.3\na2 <- ceiling(power.prop.test(p1=0.2,p2=0.4,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.4\na3 <- ceiling(power.prop.test(p1=0.3,p2=0.5,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.5\na4 <- ceiling(power.prop.test(p1=0.4,p2=0.6,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# print sample sizes\nc(a1,a2,a3,a4)\n```\nLet's focus on the `nusiance=0.2` case. We can examine the impact on our design if we observe other nuisance parameters even though we anticipated 0.2. The `toer()` function allows us to estimate the type I error rate we may encounter if we do or do not use blinded re-estimation:\n\n```{r toer-blindrecalc, cache=T}\ndesign <- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn <- n_fix(design, nuisance = 0.2) # N = 124 total (62 per group)\np <- seq(0.2, 0.6, by = 0.1)\ntoer_fix <- toer(design, n1 = n, nuisance = p, recalculation = FALSE)\ntoer_ips <- toer(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\nt1e_tab <- rbind( \"No SSR\" = toer_fix, \"SSR at 1/2 Enrolled\" = toer_ips)\ncolnames(t1e_tab) <- p\nt1e_tab\n```\nBy comparing these type I error rates, we see that there can be an inflation to our desired $\\alpha=0.025$ due to observing different averages between our two groups (i.e., the nuisance parameters reflected by the column headers). With blinded SSR, we see better control of the type I error rate.\n\nWe can use the similar `pow()` to estimate the power under our design with varying nuisance parameters:\n\n```{r pow-blindrecalc, cache=T}\npow_fix <- pow(design, n1 = n, nuisance = p, recalculation = FALSE)\npow_ips <- pow(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\npow_tab <- rbind( \"No SSR\" = pow_fix, \"SSR at 1/2 Enrolled\" = pow_ips)\ncolnames(pow_tab) <- p\npow_tab\n```\nFor power we see that SSR maintains approximately 80% power, but without SSR the power decreases as or nuisance parameter grows.\n\nWe can also plot the distribution of sample sizes that would occur at different nuisance parameters to evaluate our potential risk of having a much larger sample size:\n\n```{r}\nn_dist(design, n1 = n/2, nuisance = p, plot = TRUE)\n```\n\nIn this figure above we see that if our nuisance parameter is actually 0.2 as assumed for the power calculation, or sample size re-estimation could range from 62 to 184 versus our planned $N=124$. However, this does assume we allow for sample size reductions and have no limit on the maximum increase. If our guess of the nuisance parameter is 0.5, we see the largest increase in our expected sample size up to 191.8, with a range of 154 to 194.\n\n\n\n## Unblinded Conditional Power Example with Binary Outcome\n\nThe `gsDesign` package includes the `ssrCP()` which allows us to implement an *unblinded* sample size re-estimation procedure. Here we continue our previous example using a binary outcome. In this case we will incorporate traditional group sequential O'Brien-Fleming boundaries for interim monitoring for efficacy. This means our sample size will also be adjusted relative to a fixed design to account for the use of the traditional GSD (this is in contrast to alpha-spending boundaries that are OBF-like, where the sample size remains fixed).\n\nFirst, we must establish the design type based on the `gsDesign()` function before extending to a conditional power design with `ssrCP()`:\n\n```{r}\nlibrary(gsDesign) # load package\n\nx <- gsDesign(\n  k = 2, # number of analyses planned, including interim and final\n  n.fix = 196, # sample size for a fixed design with no interim\n  timing = 0.5, # timing of interm analyses \n  test.type=2, # 6 options covering one- vs. two-sided and symmetric vs. asymmetric boundaries; 2 is a two-sided symmetric approach\n  alpha = 0.025, # one-sided type I error rate\n  beta = 0.2, # type II error rate (i.e., power=1-beta)\n  delta0 = 0, # null hypothesis parameter (i.e., no difference)\n  delta1 = 0.2, # alternative hypothesis parameter (i.e., difference we wish to detect)\n  sfu='OF' ) # alpha-spending for efficacy monitoring\n\n# plot stopping boundaries\nplot(x)\n```\n\nThe plot shows our stopping boundaries for our two-sided test, with the final critical value being 1.98 (versus 1.96 in a fixed design without interim monitoring), as well as an increase from $N=196$ to $N=198$.\n\nWe can then extend this design to a conditional power design based on an observed test statistic at the end of our first stage. Here we assume we observed $Z=1.6$:\n\n```{r}\n# extend design to a conditional power design\nxx <- ssrCP(x = x, # provide design used\n            z1 = 1.6, # enter observed test statistic\n            overrun = 0, # can note how many participants are enrolled but not included in the interim analysis\n            beta = 0.2, # targeted type II error for SSR (i.e., targeted power=1-beta)\n            cpadj = c(0.5,0.8), # range of conditional powers for which SSR is to be performed, otherwise N from original design used\n            maxinc = 2, # argument limiting maximum fold-increase from planned max N (e.g., 2 times)\n            z2 = z2NC) # combination function to combine stage 1 and stage 2 results; z2NC=inverse normal combination test, z2Z=sufficient stat for complete data, z2Fisher=Fisher's combination test\n\n# show immediately relevant information\nxx$dat\n```\n\nFrom the `ssrCP` documentation, we see these values represent:\n\n* z1: input z1 values, \n* z2: computed cutoffs for the standard normal test statistic based solely on stage 2 data\n* n2: stage 2 sample size (however, based on other functions I believe this may be the maximum sample size to enroll, so we need to take n2-n1)\n* CP: stage 2 conditional power \n* theta: standardize effect size used for conditional power calculation \n* delta: the natural parameter value corresponding to theta The relation between theta and delta is determined by the delta0 and delta1 values from x: delta = delta0 + theta(delta1-delta0).\n\nThe most important summary is the re-estimated sample size of $N=259.0207$ which rounds up to $N=260$, resulting in a need to enroll $N_2=260-99=161$ in stage 2 instead of the original $N_2 = 198-99=99$. Overall, this results in a total sample size of $N_1+N_2=99+161=260$, which is less than the two times inflation allowed by `maxinc=2` (i.e., up to $198\\times2 = 396$ is allowed).\n\nThe other useful summary is the `z2=1.19651`, which represents that we need to observe a test statistic at least this large for our inverse normal combination test to be significant.\n\nIf we assume we enroll the 161 additional participants and observe $Z_2=1.3$, we would have a normal combination test of \n\n$$ \\frac{Z_1 + Z_2}{\\sqrt{2}} = \\frac{1.6 +1.3}{\\sqrt{2}} = 2.05 > 1.96 = Z_{0.975} = Z_{1-\\alpha/2} $$\n\nTherefore, we would reject the null hypothesis and conclude we found an effect.\n\n\n# Simulation Study\n\nFor our simulation study, let's assume we are interested in designing a study where we wish to reduce the rate of an adverse event by exploring a new approach to a procedure:\n\n* $H_0\\colon p_1 = p_2$ versus $H_1\\colon p_1 < p_2$ (i.e., one-sided hypothesis)\n* $p_1 = p_{trt} = 0.1$ and $p_2 = p_{con} = 0.25$ for our alternative hypothesis of interest (i.e., reducing the rate of adverse events from 25% to 10%)\n* $\\alpha=0.025$\n* $\\beta=0.8$\n\nFor a fixed sample design, using `power.prop.test()`, we can identify our sample size to enroll:\n\n```{r}\npower.prop.test(p1=0.1,p2=0.25,sig.level=0.025,power=0.8, alternative='o')\n```\nGiven $n=100$ per arm, we would plan for a study enrolling a total of $N_{total}=200$.\n\nWe will evaluate five scenarios to determine the effect of sample size re-estimation:\n\n* Scenario 1: Null with $p_1 = p_{trt} = 0.25$ and $p_2 = p_{con} = 0.25$\n* Scenario 2: Null with $p_1 = p_{trt} = 0.175$ and $p_2 = p_{con} = 0.175$, where the null rate is the nuisance parameter (i.e.,$\\frac{0.1+0.25}{2}=0.175$)\n* Scenario 3: Alternative with $p_1 = 0.1$ and $p_2 = 0.25$\n* Scenario 4: Alternative with $p_1 = 0.15$ and $p_2 = 0.25$\n* Scenario 5: Alternative with $p_1 = 0.15$ and $p_2 = 0.30$ (i.e., maintaining the $\\delta$ but for different effects)\n\n\n## Blinded Re-estimation with Binary Outcome Simulation\n\nWe first implement our blinded re-estimation procedure. We will compare three strategies:\n\n1. Blinded SSR where we will allow for a smaller than planned sample size if indicated (i.e., if stage 2 needs fewer than 100 more participants, we will enroll that number)\n2. Blinded SSR where we will continue with the planned sample size if the re-estimation indicates fewer participants could be needed (i.e., if stage 2 needs fewer than 100 more participants, we will still enroll 100)\n3. A fixed sample design with no SSR\n\nThe code is hidden, but can be shown if desired. We will summarize the rejection rate and average (SD) sample size across 1,000 simulation trials.\n\n```{r blindedSSR-binary-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nsim_list <- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nblinded_res <- blinded_n2_res <- fixed_res <- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\nn <- 200 # total sample size based on fixed sample\nn1 <- 100 # sample size to enroll for stage 1\ndelta <- 0.15 # expected effect size under H1 from power calculation\nr <- 1 # randomization ratio (e.g., 1:1)\nnsim <- 1000\n\n###\n# simulate method with SSR allowing for smaller than expected N\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 <- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 <- p0 - delta*(r/(1+r))\n    pc_n1 <- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest <- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 <- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 < 0 ){ n2 <- 0 } # if sufficient sample size already, set to 0\n    \n    # simulate stage 2 data\n    trt <- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con <- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate method with SSR but enrolling at least (N-N1) in stage 2 (i.e., not allowing fewer participants)\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 <- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 <- p0 - delta*(r/(1+r))\n    pc_n1 <- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest <- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 <- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 < (n-n1) ){ n2 <- (n-n1) } # enroll at least (n-n1)\n    \n    # simulate stage 2 data\n    trt <- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con <- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_n2_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 <- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 <- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- cbind(fixed_res,blinded_res,blinded_n2_res)\n\nkbl_tab %>%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with Lower N2\"=3, \"SSR with At Least N2\"=3))\n```\n\nFrom these simulation results we can see that:\n\n* Without some form of futility testing, the blinded SSR methods lead to increased sample sizes in the null scenario with 25% versus 25%, as well as slightly higher type I error rates than the fixed sample design.\n* If the null scenario response reflects the nuisance parameter, we still see slightly higher type I error rates with SSR. However, the ESS increase is less large, likely because the effect went closer to 0. This may indicate that less prevalent outcomes are less affected by the null scenario with blinded re-estimation.\n* If we encounter the effect sizes used in the power analysis, we see that SSR that enrolls *at least* the $N_{total}=200$ of the fixed sample design increases power by allowing for larger sample sizes. On average, across all 1000 simulated trials, the average sample size was 212.7 (SD=18.5).\n* If we encounter an underpowered scenario, we see that the SSR methods also lead to an increased SSR but only slightly higher power. This suggests that either unblinded re-estimation processes or potentially futility monitoring could further improve performance.\n* Finally, if we observe the same $\\delta=0.15$ but at higher observed rates ($p_{trt}=0.15$ and $p_{con}=0.30$), our blinded SSR increases power by 5.9 to 7.3% by allowing our sample size to increase based on the higher nuisance paramter (i.e., $\\frac{0.15+0.3}{2}=0.225$).\n\n\n## Unblinded Re-estimation with Binary Outcome Simulation\n\nWe can compare the results from our blinded SSR approach with an approach using unblinded SSR approaches. In this simulation we compare:\n\n* An approach using conditional power estimated from `gsDesign::ssrCP()`, where we assume that the second stage must have at least 100 participants but could increase to 200 participants (i.e., we don't allow fewer than expected participants in stage 2). This approach uses the inverse normal combination test based on the p-values from the two-sample test of proportions.\n* An approach that unblinds the control arm to use in re-estimating the `power.prop.test()` calculation for a decrease of 15% in the treatment arm. If the control arm has a response rate less than 15%, we stop for futility and calculate the one-sided p-value to record for futility. Otherwise, we allow the re-estimation to increase the sample size from 100 up to 200. Since we do not unblind the treatment arm, in this approach we evaluate the performance if we use the overall data for our final test p-value.\n* A fixed sample design enrolling 200 total participants without interim monitoring or re-estimation.\n\nThe simulation code block is hidden, but can be shown for review.\n\n```{r unblindedSSR-binary-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nlibrary(gsDesign) # load library\n\n# list of 5 simulation scenarios to mimic our blinded SSR\nsim_list <- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nunblinded_res <- unblinded_adhoc_res <- fixed_res <- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\ndelta <- 0.15 # expected effect size under H1 from power calculation\nr <- 1 # randomization ratio (e.g., 1:1)\nnsim <- 1000\n\nx <- gsDesign(\n  k = 2, \n  n.fix = 200, timing = 0.5, test.type=2,\n  alpha = 0.025, beta = 0.2, delta0 = 0, delta1 = 0.15, sfu='OF' ) \n\nn_fix <- 200 # total sample size based on fixed sample\nn1 <- ceiling(x$n.I[1])\nn <- ceiling(x$n.I[2]) \n\n###\n# simulate method with SSR using conditional power\n\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres_gs <- data.frame( zcombined=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    res_int <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')$p.value\n    \n    # UNblinded re-estimation\n    xx <- ssrCP(x = x, z1 = qnorm(1-res_int), overrun = 0, beta = 0.2, cpadj = c(0.5,0.8), maxinc = 1.5, z2 = z2NC)\n    n_rest <- ceiling(xx$dat$n2) - n1\n\t  n2 <- n_rest\n\t  n2 <- if(n2 <= 100){ 100 }else{n2} \n    if( res_int < x$upper$prob[1,1] ){ \n      # final analysis, save results\n      zcombined <- qnorm(1-res_int) / sqrt(1)\n      simres_gs[i,] <- c(zcombined, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 <- rbinom(n=n2/2, size=1, prob=pt)\n      con2 <- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res <- prop.test(x=c(sum(trt2),sum(con2)), n=c(length(trt2),length(con2)), alternative = 'less')$p.value\n      \n      zcombined <- ( qnorm(1-res_int) + qnorm(1-res) ) / sqrt(2)\n      \n      simres_gs[i,] <- c(zcombined, n1+n2)\n    }\n  }\n  \n  unblinded_res[combo,] <- c( mean(simres_gs$zcombined >= qnorm(1-(0.025))), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n###\n# somewhat ad hoc approach to unblinded SSR\n# first stage is used to implement re-estimation; no decreases in stage 2; up to 200 (versus 100) otherwise stop for futility\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes based on fixed sample to initialize\n  n <- 200\n  n1 <- 100\n  \n  # initialize object to save results in\n  simres_gs <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    res_int <- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\n    \n    # UNblinded re-estimation, where we use the estimate of the control group and still power for a 0.15 decrease in treatment\n    n_reest <- if( mean(con) <= 0.15 ){ 10000 }else{ ceiling(power.prop.test(p1=mean(con)-0.15,p2=mean(con),sig.level=0.025,power=0.8, alternative='o')$n)*2 }\n    \n    n2 <- if( (n_reest - n1) < 100 ){100}else{n_reest - n1}\n    \n\t  if( n2 > 200 ){\n      # ad hoc futility rule, save first stage result\n      p <- res_int$p.value\n      simres_gs[i,] <- c(p, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 <- rbinom(n=n2/2, size=1, prob=pt)\n      con2 <- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(length(c(trt,trt2)),length(c(con,con2))), alternative = 'less')\n      p <- res$p.value\n  \n      simres_gs[i,] <- c(p, n1+n2)\n    }\n  }\n  \n  unblinded_adhoc_res[combo,] <- c( mean(simres_gs$p < 0.025), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes for fixed sample\n  n <- 200\n  n1 <- 100\n  \n  # initialize object to save results in\n  simres <- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt <- sim_list[[combo]][1]\n  pc <- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt <- rbinom(n=n1/2, size=1, prob=pt)\n    con <- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 <- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 <- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res <- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')\t# defined as less based on order of data entered for trt and con\n    simres[i,] <- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] <- c( mean(simres$p < 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- cbind(fixed_res,unblinded_res,unblinded_adhoc_res)\n\nkbl_tab %>%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with CP\"=3, \"SSR Unblind Control\"=3))\n```\n\nThe simulation results indicate:\n\n* That the fixed sample design is slightly conservative with type I error of 2%, which is similar to the SSR with unblinded controls at 2.2%, although the sample size increases to an average of 212.6 (SD=39.0). \n* The SSR with conditional power is overly conservative, as seen by the lower type I error rates and power compared to the fixed sample design. \n* The SSR unblinded control shows the greatest improvement when the control reference is higher than expected (i.e., 30% instead of 25%), where power increases to 72.7% versus 68.9% for a fixed sample design. However, the ESS does increase to 224.6 (SD=51.8) versus 200 for the fixed sample design.\n* Some limitations in performance may be due to the fact that our allowed increase is somewhat limited in stage 2 from 100 to 200. In practice, if it was feasible to go higher we could likely improve our power relative to a fixed sample design.\n\nWhile only a limited set of methods and scenarios with a binary outcome, these results suggest the caution should be taken in implementing sample size re-estimation since it can introduce variability in our needed sample size while also potentially resulting in limited gains in power or type I error performance relative to a fixed sample design.\n\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n\n* [Guidance on interim analysis methods in clinical trials](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/guidance-on-interim-analysis-methods-in-clinical-trials/5051FDCF5284970B3DB01FE609AAA4C2?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper focusing on interim analyses in clinical trials with included case studies\n\n* [Kieser, Meinhard. *Methods and applications of sample size calculation and Recalculation in clinical trials.* Springer, 2020.](https://link.springer.com/book/10.1007/978-3-030-49528-2): textbook covering SSR methods and approaches\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["../header.html","../header-tracker.html"],"css":["../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"flatly","title":"Sample Size Re-Estimation","toc_float":true,"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}