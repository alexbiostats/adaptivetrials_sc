{"title":"Adaptive Treatment Arm Selection","markdown":{"yaml":{"title":"Adaptive Treatment Arm Selection","toc":true,"toc_float":true,"toc-location":"left","format":{"html":{"code-fold":"show","code-overflow":"wrap","code-tools":true}}},"headingText":"Overview","containsRefs":false,"markdown":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n\nThere may be uncertainty as to which study arms to include in a prospective trial, especially if we have multiple doses to consider, multiple candidate therapies, or potentially limited resources to explore all options. In this module we introduce the adaptive concept of treatment arm dropping and adding.\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/bebd3fb80cba4dc7a3e5fb6c16608507\" title=\"Adaptive Treatment Arm Selection\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/6_treatment_arm_selection.pptx).\n\n\n# Code Examples in R\n\nWithin R there are plenty of packages to implement dose-finding algorithms, which can be thought of as a form of arm dropping:\n\n* [`CRM`](https://cran.r-project.org/web/packages/CRM/index.html): implements the continual reassessment method for phase I clinical trials\n* [`bcrm`](https://cran.r-project.org/web/packages/bcrm/index.html): implements a Bayesian version of the CRM\n* [`DoseFinding`](https://cran.r-project.org/web/packages/DoseFinding/index.html): provides functions for designing and analyzing dose-finding experiments with a focus on phase II studies\n\nFor general treatment arm selection, many approaches are custom coded by the user depending on the rules used. \n\n\n# Simulation Study\n\nFor a brief simulation study, let's compare the operating characteristics for a set of different arm dropping rules:\n\n* Keep all active arms that are not dropped for futility based on one-sided Pocock stopping boundaries\n* Drop the arm with the smallest treatment effect at each stage as long as it is not statistically significant at a more generous $\\alpha=0.1$ threshold\n* Drop the arm with the smallest treatment effect regardless of statistical significance so the study ends with two arms\n\nIn our simulation we make the following assumptions:\n\n* There are six total arms (1 shared control, 5 treatment arms )\n* Each arm will enroll 100 if it never stops (i.e., power calculation for two-sample t-test assuming $\\alpha=0.05$, $\\beta=0.8$, $\\delta=0.4$, $\\sigma=1$)\n* We will plan for 5 total stages (i.e., 20 per stage) so that the final analysis can end with a control versus winner(s) comparison\n* We assume normally distributed outcomes with $\\sigma=1$ for all arms\n* The control group mean response is 0, our 5 treatment arm responses are -0.05, 0, 0.1, 0.35, 0.4 (i.e., worse than control, same as control, three improved relative to control by differing degrees)\n* We will not make any corrections for multiple testing\n\nNow let's explore our results for the arms that remain at the end of each study, overall sample size, and the rejection rates:\n\n```{r arm-selection-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nlibrary(rpact) # load rpact for futility bounds\n\nfo_p1 <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=1, beta=0.2)\nfo_p1_crit <- fo_p1$futilityBounds #  extract futility boundaries\nfo_p1_crit_mat <- matrix( c(fo_p1_crit, -Inf), ncol=5, nrow=5) # create matrix to compare with test statistics in simulation, add -Inf for final comparison at end of trial\n\n# set means (m) per study arm\nmc <- 0\nm1 <- -0.05\nm2 <- 0\nm3 <- 0.1\nm4 <- 0.35\nm5 <- 0.4\n\n# set other parameters\nsc <- s1 <- s2 <- s3 <- s4 <- s5 <- 1 # common variance, could change for other scenarios\nseed <- 515 # seed for reproducibility\nnmax <- 100 # max per arm\nnstage <- 5 # total number of stages\nn_perstage <- ceiling( seq(0,100,length.out=6)[-1] ) # number enrolled in each stage (so you can change nmax, nstage, etc. and code still works)\nnsim <- 1000 # number of simulations\n\nstrat1_res <- strat2_res <- strat3_res <- matrix(nrow=nsim, ncol=11) # create objects to save simulation results\n\n# simulation\nset.seed(seed) # set seed for reproducibility\n\nfor( i in 1:nsim ){\n  \n  # use sapply to create matrix of data with each data set represented by a column\n  simdat <- sapply( c('c',1:5), function(x) rnorm(mean = get(paste0('m',x)), sd = get(paste0('s',x)), n=nmax) ) \n  \n  # calculate two-sample t-tests for what the observed test statistic and p-value would be at each stage\n  # write helper function, paircompare(), to extract this information\n  paircompare <- function(arm_control, arm_trt, n_perstage){\n    ### Helper function to calculate test statistic and p-value for two groups given data and sample sizes to use\n    # arm_control/arm_trt: vector with observed data up to max sample size\n    # n_perstage: sample size after each stage\n    \n    tres <- t(sapply(n_perstage, function(z) t.test(arm_trt[1:z], arm_control[1:z], alternative = 'greater')[c('p.value','statistic')] ))\n    eres <- sapply(n_perstage, function(z) mean(arm_trt[1:z]) - mean(arm_control[1:z] ) )\n    \n    return( cbind(tres, eres) )\n  }\n  \n  res <- sapply( 2:6, function(w) paircompare(arm_control = simdat[,1], arm_trt = simdat[,w], n_perstage = n_perstage)  )\n  pval <- as.matrix(res[1:5,]) # extract p-values at each stage for control vs. active arm\n  tval <- as.matrix(res[6:10,]) # extract t-values at each stage for control vs. active arm\n  diff <- as.matrix(res[11:15,]) # extract observed effect size (trt - con) at each stage (one-sided goal with trt > con)\n  \n  \n  ### Strategy 1: Pocock Boundaries\n  \n  fut_stop <- (tval < fo_p1_crit_mat) # calculate if each arm has any test statistics below the futility boundary\n  \n  arm_stop1 <- sapply( 1:5, function(a) which(fut_stop[,a] == TRUE)[1] )\n  arm_stop1[ is.na(arm_stop1) ] <- 5 # make NA 5 since they never dropped for futility\n  \n  n_strat1 <- n_perstage[arm_stop1] # record sample size for each arm\n  ntot1 <- sum(n_strat1) # sum up for total sample size\n  \n  finish1 <- arm_stop1==5 # calculate indicator if arm made it to the end\n  \n  sig1 <- rep(FALSE, 5) # create indicator if significant comparison\n  sig1[ which(arm_stop1==5) ] <- unlist(pval[,5])[ which(arm_stop1==5) ] < 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat1_res[i,] <- c(ntot1, finish1, sig1)\n  \n  \n  ### Strategy 2: Drop smallest treatment effect arm as long as not significant\n  \n  diff2 <- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop2 <- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum <- which( unlist(diff2[k,]) == min(unlist(diff2[k,])) ) # calc arm with min effect size\n    \n    if( pval[k,armnum] >= 0.1 ){\n      arm_stop2[armnum] <- k\n      diff2[,armnum] <- Inf # make all observed diffs large to ignore in next stage(s)\n    }\n  }\n\n  n_strat2 <- n_perstage[arm_stop2] # record sample size for each arm\n  ntot2 <- sum(n_strat2) # sum up for total sample size\n  \n  finish2 <- arm_stop2==5 # calculate indicator if arm made it to the end\n  \n  sig2 <- rep(FALSE, 5) # create indicator if significant comparison\n  sig2[ which(arm_stop2==5) ] <- unlist(pval[,5])[ which(arm_stop2==5) ] < 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat2_res[i,] <- c(ntot2, finish2, sig2)\n  \n\n    \n  ### Strategy 3: Drop smallest treatment effect arm regardless of significance\n  \n  diff3 <- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop3 <- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum <- which( unlist(diff3[k,]) == min(unlist(diff3[k,])) ) # calc arm with min effect size\n    \n    arm_stop3[armnum] <- k\n    diff3[,armnum] <- Inf # make all observed diffs large to ignore in next stage(s)\n  }\n\n  n_strat3 <- n_perstage[arm_stop3] # record sample size for each arm\n  ntot3 <- sum(n_strat3) # sum up for total sample size\n  \n  finish3 <- arm_stop3==5 # calculate indicator if arm made it to the end\n  \n  sig3 <- sapply( 1:5, function(u) pval[ arm_stop3[u], u] < 0.025 ) # create indicator if significant comparison, here we will check each arm regardless of stopping point\n  \n  # save results\n  strat3_res[i,] <- c(ntot3, finish3, sig3)\n}\n\n# Format results to display\nstrat1 <- colMeans(strat1_res)\ns1_sd <- sd( strat1_res[,1] )\ns1res <- c( paste0( round(strat1[1]),\" (\",round(s1_sd),\")\"), paste0( strat1[2:11]*100, \"%\") )\n\nstrat2 <- colMeans(strat2_res)\ns2_sd <- sd( strat2_res[,1] )\ns2res <- c( paste0( round(strat2[1]),\" (\",round(s2_sd),\")\"), paste0( strat2[2:11]*100, \"%\") )\n\nstrat3 <- colMeans(strat3_res)\ns3_sd <- sd( strat3_res[,1] )\ns3res <- c( paste0( round(strat3[1]),\" (\",round(s3_sd),\")\"), paste0( strat3[2:11]*100, \"%\") )\n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- rbind('Pocock Futility' = s1res, 'Min(ES) and p>0.1' = s2res, 'Min(ES)' = s3res)\n\nkbl_tab %>%\n  kbl(col.names=c('Dropping Rule','ESS (SD)', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4') ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \"Arm Made to End of Trial\"=5, \"Arm Rejected Null Hypothesis\"=5))\n```\n\nFrom this simulation we can see that each decision rule has different performance and properties:\n\n* Pocock stopping has the lowest power because of its aggressive stopping for futility, but does allow multiple arms to be dropped resulting in a lower ESS. It also has the lowest rates of harmful (ES=-0.5) or null (ES=0) arms making it to the end of the trial.\n* Dropping the smallest effect size in a stage, if $p>\\alpha=0.1$, results in the highest proportions of the best two arms making it to the end of the trial, but it also leads to a slightly higher rate of ES=0.1 making it to the end which results in a larger ESS than we may desire.\n* Always dropping the minimum effect size, regardless of significance, but testing each arm based on the available data results in lower power and completion rates for ES=0.4, but does show slightly higher power for ES=0.35. Even though the trial completion rate for ES=0.35 is only 37.8%, this conundrum may be explained by the fact that we allowed for testing across all stages regardless of when it stopped and we may have, by chance, stopped ES=0.35 at the 4th stage when it was significant but if it continued enrollment to stage 5 its p-value increased over 0.025.\n\nIn practice, the choice of dropping rules or strategies will be driven by the context of your particular study and balancing the strengthens and weaknesses across the trial operating characteristics.\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n","srcMarkdownNoYaml":"\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  message = FALSE,\n  echo = TRUE\n)\n```\n\n\n# Overview\n\nThere may be uncertainty as to which study arms to include in a prospective trial, especially if we have multiple doses to consider, multiple candidate therapies, or potentially limited resources to explore all options. In this module we introduce the adaptive concept of treatment arm dropping and adding.\n\n\n# Slide Deck\n\n<iframe class=\"speakerdeck-iframe\" style=\"border: 0px; background: rgba(0, 0, 0, 0.1) padding-box; margin: 0px; padding: 0px; border-radius: 6px; box-shadow: rgba(0, 0, 0, 0.2) 0px 5px 40px; width: 100%; height: auto; aspect-ratio: 560 / 315;\" frameborder=\"0\" src=\"https://speakerdeck.com/player/bebd3fb80cba4dc7a3e5fb6c16608507\" title=\"Adaptive Treatment Arm Selection\" allowfullscreen=\"true\" data-ratio=\"1.7777777777777777\"></iframe>\n\n&nbsp;\n\nYou can also download the [original PowerPoint file](../files/Slides/6_treatment_arm_selection.pptx).\n\n\n# Code Examples in R\n\nWithin R there are plenty of packages to implement dose-finding algorithms, which can be thought of as a form of arm dropping:\n\n* [`CRM`](https://cran.r-project.org/web/packages/CRM/index.html): implements the continual reassessment method for phase I clinical trials\n* [`bcrm`](https://cran.r-project.org/web/packages/bcrm/index.html): implements a Bayesian version of the CRM\n* [`DoseFinding`](https://cran.r-project.org/web/packages/DoseFinding/index.html): provides functions for designing and analyzing dose-finding experiments with a focus on phase II studies\n\nFor general treatment arm selection, many approaches are custom coded by the user depending on the rules used. \n\n\n# Simulation Study\n\nFor a brief simulation study, let's compare the operating characteristics for a set of different arm dropping rules:\n\n* Keep all active arms that are not dropped for futility based on one-sided Pocock stopping boundaries\n* Drop the arm with the smallest treatment effect at each stage as long as it is not statistically significant at a more generous $\\alpha=0.1$ threshold\n* Drop the arm with the smallest treatment effect regardless of statistical significance so the study ends with two arms\n\nIn our simulation we make the following assumptions:\n\n* There are six total arms (1 shared control, 5 treatment arms )\n* Each arm will enroll 100 if it never stops (i.e., power calculation for two-sample t-test assuming $\\alpha=0.05$, $\\beta=0.8$, $\\delta=0.4$, $\\sigma=1$)\n* We will plan for 5 total stages (i.e., 20 per stage) so that the final analysis can end with a control versus winner(s) comparison\n* We assume normally distributed outcomes with $\\sigma=1$ for all arms\n* The control group mean response is 0, our 5 treatment arm responses are -0.05, 0, 0.1, 0.35, 0.4 (i.e., worse than control, same as control, three improved relative to control by differing degrees)\n* We will not make any corrections for multiple testing\n\nNow let's explore our results for the arms that remain at the end of each study, overall sample size, and the rejection rates:\n\n```{r arm-selection-sim, cache=T, warning=F}\n#| echo: true\n#| code-fold: true\nlibrary(rpact) # load rpact for futility bounds\n\nfo_p1 <- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=1, beta=0.2)\nfo_p1_crit <- fo_p1$futilityBounds #  extract futility boundaries\nfo_p1_crit_mat <- matrix( c(fo_p1_crit, -Inf), ncol=5, nrow=5) # create matrix to compare with test statistics in simulation, add -Inf for final comparison at end of trial\n\n# set means (m) per study arm\nmc <- 0\nm1 <- -0.05\nm2 <- 0\nm3 <- 0.1\nm4 <- 0.35\nm5 <- 0.4\n\n# set other parameters\nsc <- s1 <- s2 <- s3 <- s4 <- s5 <- 1 # common variance, could change for other scenarios\nseed <- 515 # seed for reproducibility\nnmax <- 100 # max per arm\nnstage <- 5 # total number of stages\nn_perstage <- ceiling( seq(0,100,length.out=6)[-1] ) # number enrolled in each stage (so you can change nmax, nstage, etc. and code still works)\nnsim <- 1000 # number of simulations\n\nstrat1_res <- strat2_res <- strat3_res <- matrix(nrow=nsim, ncol=11) # create objects to save simulation results\n\n# simulation\nset.seed(seed) # set seed for reproducibility\n\nfor( i in 1:nsim ){\n  \n  # use sapply to create matrix of data with each data set represented by a column\n  simdat <- sapply( c('c',1:5), function(x) rnorm(mean = get(paste0('m',x)), sd = get(paste0('s',x)), n=nmax) ) \n  \n  # calculate two-sample t-tests for what the observed test statistic and p-value would be at each stage\n  # write helper function, paircompare(), to extract this information\n  paircompare <- function(arm_control, arm_trt, n_perstage){\n    ### Helper function to calculate test statistic and p-value for two groups given data and sample sizes to use\n    # arm_control/arm_trt: vector with observed data up to max sample size\n    # n_perstage: sample size after each stage\n    \n    tres <- t(sapply(n_perstage, function(z) t.test(arm_trt[1:z], arm_control[1:z], alternative = 'greater')[c('p.value','statistic')] ))\n    eres <- sapply(n_perstage, function(z) mean(arm_trt[1:z]) - mean(arm_control[1:z] ) )\n    \n    return( cbind(tres, eres) )\n  }\n  \n  res <- sapply( 2:6, function(w) paircompare(arm_control = simdat[,1], arm_trt = simdat[,w], n_perstage = n_perstage)  )\n  pval <- as.matrix(res[1:5,]) # extract p-values at each stage for control vs. active arm\n  tval <- as.matrix(res[6:10,]) # extract t-values at each stage for control vs. active arm\n  diff <- as.matrix(res[11:15,]) # extract observed effect size (trt - con) at each stage (one-sided goal with trt > con)\n  \n  \n  ### Strategy 1: Pocock Boundaries\n  \n  fut_stop <- (tval < fo_p1_crit_mat) # calculate if each arm has any test statistics below the futility boundary\n  \n  arm_stop1 <- sapply( 1:5, function(a) which(fut_stop[,a] == TRUE)[1] )\n  arm_stop1[ is.na(arm_stop1) ] <- 5 # make NA 5 since they never dropped for futility\n  \n  n_strat1 <- n_perstage[arm_stop1] # record sample size for each arm\n  ntot1 <- sum(n_strat1) # sum up for total sample size\n  \n  finish1 <- arm_stop1==5 # calculate indicator if arm made it to the end\n  \n  sig1 <- rep(FALSE, 5) # create indicator if significant comparison\n  sig1[ which(arm_stop1==5) ] <- unlist(pval[,5])[ which(arm_stop1==5) ] < 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat1_res[i,] <- c(ntot1, finish1, sig1)\n  \n  \n  ### Strategy 2: Drop smallest treatment effect arm as long as not significant\n  \n  diff2 <- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop2 <- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum <- which( unlist(diff2[k,]) == min(unlist(diff2[k,])) ) # calc arm with min effect size\n    \n    if( pval[k,armnum] >= 0.1 ){\n      arm_stop2[armnum] <- k\n      diff2[,armnum] <- Inf # make all observed diffs large to ignore in next stage(s)\n    }\n  }\n\n  n_strat2 <- n_perstage[arm_stop2] # record sample size for each arm\n  ntot2 <- sum(n_strat2) # sum up for total sample size\n  \n  finish2 <- arm_stop2==5 # calculate indicator if arm made it to the end\n  \n  sig2 <- rep(FALSE, 5) # create indicator if significant comparison\n  sig2[ which(arm_stop2==5) ] <- unlist(pval[,5])[ which(arm_stop2==5) ] < 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat2_res[i,] <- c(ntot2, finish2, sig2)\n  \n\n    \n  ### Strategy 3: Drop smallest treatment effect arm regardless of significance\n  \n  diff3 <- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop3 <- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum <- which( unlist(diff3[k,]) == min(unlist(diff3[k,])) ) # calc arm with min effect size\n    \n    arm_stop3[armnum] <- k\n    diff3[,armnum] <- Inf # make all observed diffs large to ignore in next stage(s)\n  }\n\n  n_strat3 <- n_perstage[arm_stop3] # record sample size for each arm\n  ntot3 <- sum(n_strat3) # sum up for total sample size\n  \n  finish3 <- arm_stop3==5 # calculate indicator if arm made it to the end\n  \n  sig3 <- sapply( 1:5, function(u) pval[ arm_stop3[u], u] < 0.025 ) # create indicator if significant comparison, here we will check each arm regardless of stopping point\n  \n  # save results\n  strat3_res[i,] <- c(ntot3, finish3, sig3)\n}\n\n# Format results to display\nstrat1 <- colMeans(strat1_res)\ns1_sd <- sd( strat1_res[,1] )\ns1res <- c( paste0( round(strat1[1]),\" (\",round(s1_sd),\")\"), paste0( strat1[2:11]*100, \"%\") )\n\nstrat2 <- colMeans(strat2_res)\ns2_sd <- sd( strat2_res[,1] )\ns2res <- c( paste0( round(strat2[1]),\" (\",round(s2_sd),\")\"), paste0( strat2[2:11]*100, \"%\") )\n\nstrat3 <- colMeans(strat3_res)\ns3_sd <- sd( strat3_res[,1] )\ns3res <- c( paste0( round(strat3[1]),\" (\",round(s3_sd),\")\"), paste0( strat3[2:11]*100, \"%\") )\n\n# Format results\nlibrary(kableExtra)\nkbl_tab <- rbind('Pocock Futility' = s1res, 'Min(ES) and p>0.1' = s2res, 'Min(ES)' = s3res)\n\nkbl_tab %>%\n  kbl(col.names=c('Dropping Rule','ESS (SD)', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4') ) %>%\n  kable_classic() %>%\n  add_header_above(c(\" \"=1, \" \"=1, \"Arm Made to End of Trial\"=5, \"Arm Rejected Null Hypothesis\"=5))\n```\n\nFrom this simulation we can see that each decision rule has different performance and properties:\n\n* Pocock stopping has the lowest power because of its aggressive stopping for futility, but does allow multiple arms to be dropped resulting in a lower ESS. It also has the lowest rates of harmful (ES=-0.5) or null (ES=0) arms making it to the end of the trial.\n* Dropping the smallest effect size in a stage, if $p>\\alpha=0.1$, results in the highest proportions of the best two arms making it to the end of the trial, but it also leads to a slightly higher rate of ES=0.1 making it to the end which results in a larger ESS than we may desire.\n* Always dropping the minimum effect size, regardless of significance, but testing each arm based on the available data results in lower power and completion rates for ES=0.4, but does show slightly higher power for ES=0.35. Even though the trial completion rate for ES=0.35 is only 37.8%, this conundrum may be explained by the fact that we allowed for testing across all stages regardless of when it stopped and we may have, by chance, stopped ES=0.35 at the 4th stage when it was significant but if it continued enrollment to stage 5 its p-value increased over 0.025.\n\nIn practice, the choice of dropping rules or strategies will be driven by the context of your particular study and balancing the strengthens and weaknesses across the trial operating characteristics.\n\n# References\n\nBelow are some references to highlight based on the slides and code:\n\n* [FDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/adaptive-design-clinical-trials-drugs-and-biologics-guidance-industry): FDA guidance document on adaptive trial elements\n\n* [Recent innovations in adaptive trial designs: A review of design opportunities in translational research](https://www.cambridge.org/core/journals/journal-of-clinical-and-translational-science/article/recent-innovations-in-adaptive-trial-designs-a-review-of-design-opportunities-in-translational-research/614EAFEA5E89CA035E82E152AF660E5D?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark): 2023 review paper examining adaptive and novel trial elements with included case studies\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["../header.html","../header-tracker.html"],"css":["../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","theme":"flatly","title":"Adaptive Treatment Arm Selection","toc_float":true,"toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}