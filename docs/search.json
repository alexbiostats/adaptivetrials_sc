[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adaptive and Bayesian Methods for Clinical Trial Design Short Course",
    "section": "",
    "text": "This page contains materials by Alex Kaizer for the short course titled “Adaptive and Bayesian Methods for Clinical Trial Design Short Course” held for the 2024 WNAR/IMS/Graybill Annual Meeting from June 9-13, 2024. It covers a variety of modules relating to adaptive clinical trial designs and considerations over a full-day short course.\nIf you are interested in having this short course presented at your university, your organization, or at another conference please reach out via email. The material is meant to be modular and can be customized for varying lengths, audiences, and interests.\n\n\nAcknowledgements\nI want to acknowledge the following collaborators, without whom this work wouldn’t be possible:\n\nMulti-Source Exchangeability Models\n\nJoseph Koopmeiners, University of Minnesota\nBrian Hobbs, The University of Texas at Austin\n\n\n\nAdaptive Clinical Trials Overview Paper\n\nHayley Belli, New York University\nZhongyang Ma, New York University\nAndrew Nicklawsky, University of Colorado Anschutz Medical Campus\nSamantha Roberts, University of Colorado Anschutz Medical Campus\nJessica Wild, University of Colorado Anschutz Medical Campus\nAdane Wogu, University of Colorado Anschutz Medical Campus\nMengli Xiao, University of Colorado Anschutz Medical Campus\nRoy Sabo, Virginia Commonwealth University\n\n\n\nInterim Analyses in Trials Overview Paper\n\nJody Ciolino, Northwestern University\nLauren Balmert Bonner, Northwestern University\n\n\n\nGuide to Bayesian Analyses in Clinical Research Overview Paper\n\nLauren Gunn-Sandell, University of Colorado Anschutz Medical Campus\nEdward Bedrick, University of Arizona\nJacob Hutchins, University of Minnesota\nAaron Berg, University of Minnesota\nNichole Carlson, University of Colorado Anschutz Medical Campus"
  },
  {
    "objectID": "9_master_protocols/index.html",
    "href": "9_master_protocols/index.html",
    "title": "Master Protocol Designs",
    "section": "",
    "text": "Overview\nTraditionally we have designed separate, standalone trials to evaluate an intervention, even if there may be commonalities to ongoing and related studies. Master protocols serve as a flexible solution to this sometimes redundant approach to trials. In this module we discuss basket trials that allow multiple indications for a common intervention, umbrella trials that attempt to target eligible disease states for personalized therapy, and platform trials that allow for the evaluation of multiple therapies and populations.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nCode Examples in R\nWhile master protocols themselves can be analyzed with general software, some special packages that incorporate other elements include:\n\nbasket: implements asymmetric multi-source exchangeability models (MEMs) for basket trial designs where we wish to evaluate if pooling subgroups is possible based on exchangeability, restricted to binary outcomes\nNCC: provides functions to implement and evaluate platform trials with non-concurrent controls\nPlatformDesign: design parameters for two-period multiarm platform designs\n\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies"
  },
  {
    "objectID": "7_adaptive_randomization/index.html",
    "href": "7_adaptive_randomization/index.html",
    "title": "Adaptive Randomization",
    "section": "",
    "text": "Overview\nRandomization serves as our causal mechanism to draw conclusions about the effect of an intervention in a clinical trial. However, there are many approaches to randomization including both static (i.e., fixed) and dynamic (i.e., changing) ratios. In this module we first do a brief review of static randomization approaches before diving into three unique adaptive randomization (AR) approaches to modify a study’s allocation ratio: baseline covariate AR, outcome/response AR, and information balance AR.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nCode Examples in R\nVarious packages exist to assist in implementing randomization approaches in R:\n\nrandomizeR: implements static randomization schema, with a Journal of Statistical Software article to follow along for more information\ncarat: implements covariate adaptive randomization designs (six different strategies included as of package version 2.2.1), with support to implement the appropriate statistical analysis after data has been collected\nRARfreq: implements response-adaptive randomization procedures\nInformation balance AR approaches generally have to be custom coded with the choice of information borrowing software.\n\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nFDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document: FDA guidance document on adaptive trial elements\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies"
  },
  {
    "objectID": "5_adaptive_enrichment/index.html",
    "href": "5_adaptive_enrichment/index.html",
    "title": "Adaptive Enrichment",
    "section": "",
    "text": "Overview\nWhile we may hope that an intervention is universally beneficial, it is possible that treatment response varies by subgroup. In this module we review design elements that allow us to refine the eligibility of a study during interim analyses to “enrich” the study sample for subgroups that may be more likely to benefit or demonstrate an effect for our chosen outcome.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nCode Examples in R\nA few R packages that are noted to handle adaptive enrichment designs include:\n\nrpact: a package for confirmatory adaptive clinical trial design, simulation, and analysis; includes some vignettes for conducting enrichment studies with the software\nesDesign: implement adaptive enrichment designs, potentially with sample size re-estimation, as proposed by Lin et al. (2021)\n\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nFDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document: FDA guidance document on adaptive trial elements\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies"
  },
  {
    "objectID": "2_intro_bayesian/index.html",
    "href": "2_intro_bayesian/index.html",
    "title": "Bayesian 101",
    "section": "",
    "text": "This module covers a crash course in Bayesian statistics. While many adaptive trial elements can be done with frequentist methods, Bayesian methods provide additional flexibility. We will introduce the basics of the Bayesian approach to statistics and cover a brief example analysis of a clinical trial using Bayesian methods."
  },
  {
    "objectID": "2_intro_bayesian/index.html#software-options",
    "href": "2_intro_bayesian/index.html#software-options",
    "title": "Bayesian 101",
    "section": "Software Options",
    "text": "Software Options\nThere are lots of statistical packages and approaches that we can use to either run Bayesian models in R, or to connect R with external software to implement the models. Some options include:\n\nbrms package: implements the Stan programming language within R, syntax is similar to the lme4 package, this is our focus for Bayesian examples\nrstan and rstanarm packages: implements the Stan programming language within R, rstanarm uses standard glm syntax, runs more quickly than brms since models are pre-compiled\nbayestestr package: can provide Bayes factors and works with rstanarm, brms, and BayesFactor\nR2jags, rjags, runjags packages: implements JAGS (just another Gibbs sampler) which allows for non-gradient sampling, JAGS is one of the “original” approaches for implementing Bayesian analyses via software (that I remember), can be a little clunkier than other options\n\nIt is worth noting that within each software distributions may use different parameterizations, so caution should be taken to ensure the desired prior values are used. For example, the normal distribution in JAGS uses the precision (i.e., \\(\\tau = \\frac{1}{\\sigma^2}\\)), whereas Stan uses the standard deviation (i.e., \\(\\sigma\\)).\nDr. Kruschke has a nice introduction to Bayesian textbook that includes some instructions for installing software for Bayesian analyses. You may also be interested in exploring the textbook for more background on Bayesian theory, methods, and implementation.\nAdditionally, Stata and SAS (e.g., PROC MCMC and PROC GENMOD) include Bayesian options. These are detailed in “A practical guide to adopting Bayesian analyses in clinical research” for step-by-step guidance on their implementation."
  },
  {
    "objectID": "2_intro_bayesian/index.html#linear-regression-code-from-a-practical-guide-to-adopting-bayesian-analyses-in-clinical-research",
    "href": "2_intro_bayesian/index.html#linear-regression-code-from-a-practical-guide-to-adopting-bayesian-analyses-in-clinical-research",
    "title": "Bayesian 101",
    "section": "Linear Regression Code from “A practical guide to adopting Bayesian analyses in clinical research”",
    "text": "Linear Regression Code from “A practical guide to adopting Bayesian analyses in clinical research”\nThis section provides the code from the published paper in R. The dataset for the paper is included in the corresponding GitHub repository hosted by Dr. Nichole Carlson, but can also be downloaded as CSV here for convenience: drugtrial.csv.\nFor simplicity, we focus on comparing priors across simple linear regression models, but the GitHub repository includes examples for multiple linear regression and logistic regression models as well. In this example, we have a continuous outcome of time to readiness for discharge (in minutes) that are compared by two randomized treatment groups (sufentanil (new treatment) versus IV fentanyl).\nFirst, let’s load our packages and read in our data:\n\n\nCode\n# CALL LIBRARIES\nlibrary(brms) #Bayesian modeling capabilities\nlibrary(bayestestR) #particular Bayesian tests\n\n# READ IN CLINICAL TRIAL DATA FROM PAPER\ntrial &lt;- read.csv('../files/drugtrial.csv')\n\n### CHECK OUT TOP ROWS OF DATA\n## trial mini-data dictionary:\n# rowid: trial ID\n# in_phase_1_to_out_of_phase_2: time to readiness for discharge after arrival in PACU (minutes)\n# sex_n: sex of participant (1=female, 0=male)\n# groupn: randomized group (1=sufentanil, 0=IV fentanyl)\n# blockn: preoperative nerve block used (1=yes, 0=no)\n# proc_length_center: procedure length (minutes)\n\nhead(trial)\n\n\n  rowid in_phase_1_to_out_of_phase_2 sex_n groupn blockn proc_length_center\n1     1                           60     1      0      0              -37.8\n2     2                           69     1      1      1              -10.8\n3     3                          102     1      1      0              -57.8\n4     4                          165     1      0      0               22.2\n5     5                          115     1      1      0              140.2\n6     6                          104     0      0      1               21.2\n\n\n\nFrequentist Simple Linear Regression\nFor comparison sake, we can first fit our frequentist simple linear regression using the glm function:\n\n\nCode\n# Syntax: &lt;name of model object&gt; &lt;- glm(&lt;outcome variable&gt; ~ &lt;predictor variable&gt;, data = &lt;datasetname&gt;, family=&lt;distribution corresponding to model type&gt;) \nlin_reg &lt;- glm(in_phase_1_to_out_of_phase_2 ~ groupn, \n               data=trial, \n               family='gaussian')\n\n# Syntax: summary(&lt;model object&gt;) - function to show model parameter estimates/results\nsummary(lin_reg)\n\n\n\nCall:\nglm(formula = in_phase_1_to_out_of_phase_2 ~ groupn, family = \"gaussian\", \n    data = trial)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   94.364      5.289  17.842   &lt;2e-16 ***\ngroupn         3.727      7.480   0.498     0.62    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 923.0682)\n\n    Null deviance: 59306  on 65  degrees of freedom\nResidual deviance: 59076  on 64  degrees of freedom\nAIC: 641.9\n\nNumber of Fisher Scoring iterations: 2\n\n\nCode\n# Syntax: confint() - print confidence intervals in console\nconfint(lin_reg)\n\n\n                2.5 %   97.5 %\n(Intercept)  83.99771 104.7296\ngroupn      -10.93236  18.3869\n\n\n\n\nbrms Bayesian Simple Linear Regression Syntax\nThe general syntax for using brm is described below:\n\n\nCode\n# Syntax: using brm function for Bayesian modeling\n#   &lt;name of model object&gt; &lt;- brm(&lt;outcome variable&gt; ~ &lt;predictor variable&gt;, \n#            data = &lt;datasetname&gt;, \n#            family=&lt;distribution corresponding to model type&gt;,\n#            prior = c(set_prior(\"&lt;distribution(mean,SD)\", class = \"&lt;name&gt;\")),\n#            seed = &lt;value - for reproducibility&gt;,\n#            init = &lt;name of initial values list&gt;,\n#            warmup = &lt;sets the # of burn-in iterations (those that will be 'thrown out')&gt;,\n#            iter = &lt;# of total iterations for each chain including burn-in&gt;\n#            chains = &lt;# of chains&gt;,\n#            cores = &lt;#&gt; to use for executing chains in parallel - for processing)\n\n\nWe also will create a set of initial values to use for each our simple linear regressions below:\n\n\nCode\n# Set initial starting values for chains by creating a list, will be used for all simple linear regressions\n# Syntax: list(&lt;model parameter&gt; = &lt;starting value&gt;); be sure to list all parameters\ninits &lt;- list(\n  Intercept = 0,\n  sigma     = 1,\n  beta      = 0 )\n\n# Syntax: &lt;new_list&gt; &lt;- list(&lt;initial values list name&gt;) - Create list of all initial values\nlist_of_inits &lt;- list(inits, inits, inits)\n\n\n\n\nbrms SLR with Pseudo Vague Prior\nIn this example, we fit a “pseudo-vague” prior where \\(\\sigma^2 = 1000\\) or, equivalently, \\(\\sigma = \\sqrt{1000} = 31.62278\\). Here we call the prior “pseudo-vague” because it turns out that while it seems like a large variance, since \\(\\beta_0 \\sim N(\\mu=0, \\sigma=31.62278)\\), there is some biasing towards a mean of 0.\n\n\nCode\nfit_lin_1 &lt;-brm(in_phase_1_to_out_of_phase_2 ~ groupn,\n                data=trial,\n                family='gaussian',\n                prior = c(set_prior(\"normal(0,31.62278)\", class = \"b\"),\n                          set_prior(\"normal(0,31.62278)\", class =\"Intercept\"),\n                          set_prior(\"inv_gamma(0.01,0.01)\", class=\"sigma\")),\n                seed= 123,\n                init=list_of_inits,\n                warmup = 1000, iter = 10000, chains = 2, cores=4,\n                save_pars = save_pars(all = TRUE))\n\n# Summarize parameters\nsummary(fit_lin_1)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: in_phase_1_to_out_of_phase_2 ~ groupn \n   Data: trial (Number of observations: 66) \n  Draws: 2 chains, each with iter = 10000; warmup = 1000; thin = 1;\n         total post-warmup draws = 18000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    93.05      5.34    82.48   103.53 1.00    18799    13379\ngroupn        3.55      7.42   -10.90    18.16 1.00    17970    13260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    30.71      2.75    25.89    36.59 1.00    19382    13421\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\n# Obtain highest density posterior interval\nbayestestR::hdi(fit_lin_1, ci=0.95) \n\n\nHighest Density Interval\n\nParameter   |          95% HDI\n------------------------------\n(Intercept) | [ 82.66, 103.66]\ngroupn      | [-10.79,  18.24]\n\n\nCode\n# Syntax: plot() - print Bayesian diagnostic plots to console, plots in one figure\nplot(fit_lin_1)\n\n\n\n\n\nCode\n# Request plots individually \nmcmc_plot(fit_lin_1, type=\"hist\") #histogram\n\n\n\n\n\nCode\nmcmc_plot(fit_lin_1, type=\"trace\") #trace plot\n\n\n\n\n\nCode\nmcmc_plot(fit_lin_1, type=\"acf\") #autocorrelation plot\n\n\n\n\n\nCode\n# Syntax: prior_summary() - print priors used in console\nprior_summary(fit_lin_1)\n\n\n                prior     class   coef group resp dpar nlpar lb ub       source\n   normal(0,31.62278)         b                                            user\n   normal(0,31.62278)         b groupn                             (vectorized)\n   normal(0,31.62278) Intercept                                            user\n inv_gamma(0.01,0.01)     sigma                               0            user\n\n\nCode\n# Extract posterior chains\npost_samp &lt;- as_draws(fit_lin_1)\n\n# Combine and extract drug group posterior estimates (can add more list items if more than 2 chains)\nxpost &lt;- c(post_samp[[1]]$b_groupn, post_samp[[2]]$b_groupn) \n\n# Calculate the posterior probability that our group predictor is less than 0\nmean(xpost &lt; 0) \n\n\n[1] 0.3167222\n\n\n\n\nbrms SLR with Vague Prior\nIn this example, we fit a “vague” prior where \\(\\sigma^2 = 10000\\) or, equivalently, \\(\\sigma = \\sqrt{100} = 100\\).\n\n\nCode\nfit_lin_2 &lt;- brm(in_phase_1_to_out_of_phase_2 ~ groupn, \n                 data=trial, \n                 family='gaussian', \n                 prior = c(set_prior(\"normal(0,100)\", class = \"b\"),\n                           set_prior(\"normal(0,100)\", class = \"Intercept\"),\n                           set_prior(\"inv_gamma(0.01,0.01)\", class=\"sigma\")),\n                 seed= 123,\n                 init=list_of_inits,\n                 warmup = 1000, iter = 10000, chains = 2, cores=4)\n\nsummary(fit_lin_2)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: in_phase_1_to_out_of_phase_2 ~ groupn \n   Data: trial (Number of observations: 66) \n  Draws: 2 chains, each with iter = 10000; warmup = 1000; thin = 1;\n         total post-warmup draws = 18000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    94.28      5.43    83.71   105.04 1.00    16878    12729\ngroupn        3.63      7.60   -11.51    18.32 1.00    17708    11472\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    30.74      2.75    25.89    36.61 1.00    17210    13095\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nbayestestR::hdi(fit_lin_2, ci=0.95) \n\n\nHighest Density Interval\n\nParameter   |          95% HDI\n------------------------------\n(Intercept) | [ 83.20, 104.45]\ngroupn      | [-11.06,  18.71]\n\n\nCode\nplot(fit_lin_2)\n\n\n\n\n\nCode\nmcmc_plot(fit_lin_2, type=\"hist\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_2, type=\"trace\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_2, type=\"acf\") \n\n\n\n\n\nCode\nprior_summary(fit_lin_2)\n\n\n                prior     class   coef group resp dpar nlpar lb ub       source\n        normal(0,100)         b                                            user\n        normal(0,100)         b groupn                             (vectorized)\n        normal(0,100) Intercept                                            user\n inv_gamma(0.01,0.01)     sigma                               0            user\n\n\nCode\n# OPTION 1 for calculating posterior probabilities:\n# Extract posterior chains\npost_samp2 &lt;- as_draws(fit_lin_2)\nxpost2 &lt;- c(post_samp2[[1]]$b_groupn, post_samp2[[2]]$b_groupn) \n\n# Calculate the posterior probability that our group predictor is less than 0\nmean(xpost2 &lt; 0) \n\n\n[1] 0.3117778\n\n\nCode\n# OPTION 2 for calculating posterior probabilities:\n# Extract posterior chains\npost_samp2 &lt;- as_draws_df(fit_lin_2)\n\n# Create an indicator for group &lt; 0\npost_samp2$indicator &lt;- post_samp2$b_groupn&lt;0\n\n# Calculate the posterior probability\nsummary(post_samp2$indicator)\n\n\n   Mode   FALSE    TRUE \nlogical   12388    5612 \n\n\n\n\nbrms SLR with Optimistic Prior\nIn this example, we fit an “optimistic” prior on our treatment group such that \\(\\beta_1 \\sim N(\\mu=-30, \\sigma=10)\\). This was selected based on the estimates used for the power analysis in the original trial where it was estimated that a clinically meaningful difference would be a 30 minute reduction in readiness to discharge.\n\n\nCode\nfit_lin_3 &lt;- brm(in_phase_1_to_out_of_phase_2 ~ groupn, \n                 data=trial, \n                 family='gaussian', \n                 prior = c(set_prior(\"normal(-30,10)\", class = \"b\", coef = \"groupn\"),\n                           set_prior(\"normal(0,100)\", class = \"Intercept\"),\n                           set_prior(\"inv_gamma(0.01,0.01)\", class=\"sigma\")),\n                 seed= 123,\n                 init=list_of_inits,\n                 warmup = 1000, iter = 10000, chains = 2, cores=4)\nsummary(fit_lin_3)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: in_phase_1_to_out_of_phase_2 ~ groupn \n   Data: trial (Number of observations: 66) \n  Draws: 2 chains, each with iter = 10000; warmup = 1000; thin = 1;\n         total post-warmup draws = 18000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   100.48      4.99    90.87   110.33 1.00    14460    12578\ngroupn       -8.79      6.24   -21.28     3.14 1.00    14788    12967\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    31.33      2.86    26.32    37.54 1.00    15824    13469\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nbayestestR::hdi(fit_lin_3, ci=0.95) #get 95% HDP Credible Intervals\n\n\nHighest Density Interval\n\nParameter   |          95% HDI\n------------------------------\n(Intercept) | [ 90.51, 109.89]\ngroupn      | [-20.89,   3.41]\n\n\nCode\nplot(fit_lin_3)\n\n\n\n\n\nCode\nmcmc_plot(fit_lin_3, type=\"hist\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_3, type=\"trace\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_3, type=\"acf\") \n\n\n\n\n\nCode\nprior_summary(fit_lin_3)\n\n\n                prior     class   coef group resp dpar nlpar lb ub  source\n               (flat)         b                                    default\n       normal(-30,10)         b groupn                                user\n        normal(0,100) Intercept                                       user\n inv_gamma(0.01,0.01)     sigma                               0       user\n\n\nCode\n# Extract posterior chains\npost_samp3 &lt;- as_draws(fit_lin_3)\n\nxpost3 &lt;- c(post_samp3[[1]]$b_groupn, post_samp3[[2]]$b_groupn) \n\n# Calculate the posterior probability that our group predictor is less than 0\nmean(xpost3 &lt; 0) \n\n\n[1] 0.9235\n\n\n\n\nbrms SLR with Skeptical Prior\nIn this example, we fit a “skeptical” prior on our treatment group such that \\(\\beta_1 \\sim N(\\mu=0, \\sigma=10)\\). This prior represents a skeptics belief that there is a meaningful treatment difference by centering the treatment effect at 0 with smaller variance than our vague prior.\n\n\nCode\nfit_lin_4 &lt;- brm(in_phase_1_to_out_of_phase_2 ~ groupn, \n                 data=trial, \n                 family='gaussian', \n                 prior = c(set_prior(\"normal(0,10)\", class = \"b\", coef = \"groupn\"),\n                           set_prior(\"normal(0,100)\", class = \"Intercept\"),\n                           set_prior(\"inv_gamma(0.01,0.01)\", class=\"sigma\")),\n                 seed= 123,\n                 init=list_of_inits,\n                 warmup = 1000, iter = 10000, chains = 2, cores=4)\n\nsummary(fit_lin_4)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: in_phase_1_to_out_of_phase_2 ~ groupn \n   Data: trial (Number of observations: 66) \n  Draws: 2 chains, each with iter = 10000; warmup = 1000; thin = 1;\n         total post-warmup draws = 18000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    94.95      4.92    85.27   104.53 1.00    18329    12418\ngroupn        2.27      5.98    -9.59    14.05 1.00    17125    13003\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    30.66      2.73    25.90    36.53 1.00    16139    13918\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nbayestestR::hdi(fit_lin_4, ci=0.95) #get 95% HDP Credible Intervals\n\n\nHighest Density Interval\n\nParameter   |         95% HDI\n-----------------------------\n(Intercept) | [85.18, 104.41]\ngroupn      | [-9.22,  14.32]\n\n\nCode\nplot(fit_lin_4)\n\n\n\n\n\nCode\nmcmc_plot(fit_lin_4, type=\"hist\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_4, type=\"trace\") \n\n\n\n\n\nCode\nmcmc_plot(fit_lin_4, type=\"acf\") \n\n\n\n\n\nCode\nprior_summary(fit_lin_4)\n\n\n                prior     class   coef group resp dpar nlpar lb ub  source\n               (flat)         b                                    default\n         normal(0,10)         b groupn                                user\n        normal(0,100) Intercept                                       user\n inv_gamma(0.01,0.01)     sigma                               0       user\n\n\nCode\n# Extract posterior chains\npost_samp4 &lt;- as_draws(fit_lin_4)\n\nxpost4 &lt;- c(post_samp4[[1]]$b_groupn, post_samp4[[2]]$b_groupn) \n\n# Calculate the posterior probability that our group predictor is less than 0\nmean(xpost4 &lt; 0) \n\n\n[1] 0.3480556"
  },
  {
    "objectID": "10_seamless_designs/index.html",
    "href": "10_seamless_designs/index.html",
    "title": "Seamless Clinical Trial Designs",
    "section": "",
    "text": "Overview\nClinical trials progress through a series of phases to evaluate safety and dose finding (Phase I), preliminary efficacy and further safety profiling (Phase II), and confirmatory efficacy and safety profiling (Phase III). Often we have periods of whitespace between these studies as we wrap one study up and plan for the next. In this module we introduce the concept of seamless designs, where we intentionally write a single study protocol to span multiple stages to reduce the time to evaluate the phases of research.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nCode Examples in R\nThe one R package that comes up for seamless designs is:\n\nasd: runs simulations for adaptive seamless designs with and without early outcomes for treatment selection and subpopulation type designs\n\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nFDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document: FDA guidance document on adaptive trial elements\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies"
  },
  {
    "objectID": "0_intro/index.html",
    "href": "0_intro/index.html",
    "title": "Introduction to Our Short Course",
    "section": "",
    "text": "Overview\nThis module serves as the start of our “Adaptive and Bayesian Methods for Clinical Trial Design” short course! We will do brief introductions and outline the general agenda for today’s modules.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nReferences\nBelow are some references to highlight that will serve as the framework for our short course:\n\nFDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document: FDA guidance document on adaptive trial elements\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies\nGuidance on interim analysis methods in clinical trials: 2023 review paper focusing on interim analyses in clinical trials with included case studies\nA practical guide to adopting Bayesian analyses in clinical research: 2024 tutorial paper exploring the Bayesian approach to statistics and how to apply the methods for clinical trials"
  },
  {
    "objectID": "1_intro_clinical_trials/index.html",
    "href": "1_intro_clinical_trials/index.html",
    "title": "Clinical Trials 101",
    "section": "",
    "text": "Overview\nThis module serves as our starting point to provide a common understanding of what a clinical trial is and to review some common terminology we may encounter during our short course.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nThe PRECIS-2 tool: designing trials that are fit for purpose: the BMJ paper introducing the PRECIS-2 framework that helps to summarize how trials may be made pragmatic on different elements\nCONSORT 2010 statement: the CONSORT guidelines provide a template of what to report (and what we should be thinking of when designing a study)"
  },
  {
    "objectID": "3_interim_monitoring/index.html",
    "href": "3_interim_monitoring/index.html",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "",
    "text": "In this module we introduce the “original” adaptive trial element of interim monitoring to stop a trial early for efficacy, futility, and/or safety. In these designs we may terminate a trial early because it is highly unlikely we would be able to detect a significant effect if we continued (i.e., futility), because we already have sufficient evidence of a significant treatment effect (i.e., efficacy), or because there are concerns relating to adverse events (i.e., safety)."
  },
  {
    "objectID": "3_interim_monitoring/index.html#rpact-and-the-getdesigngroupsequential-function",
    "href": "3_interim_monitoring/index.html#rpact-and-the-getdesigngroupsequential-function",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "rpact and the getDesignGroupSequential() Function",
    "text": "rpact and the getDesignGroupSequential() Function\nThe powerhouse function getDesignGroupSequential() allows us to specify numerous elements of various group sequential designs.\n\n\nCode\n# Syntax: using getDesignGroupSequential function to calculate interim monitoring boundaries\ndesign &lt;- getDesignGroupSequential(\n    sided = &lt;specify if alternative hypothesis is one- or two-sided&gt;,  # default is 1 for one-sided, can also specified 2 for two-sided hypothesis test\n    alpha = &lt;desired type I error rate&gt;, # default is 0.025\n    beta = &lt;desired type II error rate&gt;, # default is 0.20 (i.e., power=1-beta, so default is 80% power)\n    kMax = &lt;maximum number of stages&gt;, # default is 3, number of interim analyses is kMax-1\n    informationRates = &lt;fixed information rates prior to start of trial when interim monitoring will occur&gt;, # default is (1:kMax)/kMax, can manually specify and ignore kMax argument\n    typeOfDesign = &lt;type of boundaries for efficacy monitoring&gt;, # many choices (see documentation) including O'Brien-Fleming (\"OF\"), Pocock (\"P\"), and alpha-spending versions (\"asOF\" and \"asP\")\n    typeBetaSpending = &lt;type of boundaries for futility monitoring&gt;, # many choices (see documentation) including O'Brien-Fleming (\"bsOF\") and Pocock (\"bsP\")\n    futilityBounds = &lt;manually defined futility boundaries on the test statistic Z-scale&gt; # can define futility with desired rules or use spending in next argument\n)"
  },
  {
    "objectID": "3_interim_monitoring/index.html#rpact-and-monitoring-for-only-efficacy",
    "href": "3_interim_monitoring/index.html#rpact-and-monitoring-for-only-efficacy",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "rpact and Monitoring for Only Efficacy",
    "text": "rpact and Monitoring for Only Efficacy\nIn some settings, we may only wish to monitor our clinical trial to allow stopping for efficacy. In this setting, we would only stop early if we observed an overwhelming effect for our outcome that is being monitored.\n\nTwo-Sided Efficacy Boundaries\nLet’s start by exploring a few different boundaries and compare them graphically. We’ll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed:\n\n\nCode\neo_of &lt;- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=2, alpha=0.05) # O'Brien-Fleming\neo_hp &lt;- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=2, alpha=0.05) # Haybittle-Peto\neo_asof &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp &lt;- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=2, alpha=0.05) # Alpha-spending Pocock-like boundary\n\n\nThe package includes the ability to extract these objects and make a plot:\n\n\nCode\n# create plot of stopping boundaries\ndesignSet &lt;- getDesignSet(designs = c(eo_of, eo_hp, eo_asof, eo_asp), variedParameters = \"typeOfDesign\")\n\nplot(designSet,\n     type=1, # can plot boundaries with type=1, but also plot other characteristics (see ?plot.TrialDesignSet)\n     legendPosition = 2) # functionality doesn't seem to work to move legend placement\n\n\n\n\n\nHowever, the legend placement argument does not seem to be working, making it a little challenging to view. Instead, we can create our own figure by extracting the relevant information. First let’s look at the R output for our O’Brien-Fleming object and compare with the extracted critical values:\n\n\nCode\nprint(eo_of) # review output; without print() in Rmd it creates nicely formatted results\n\n\n## Design parameters and output of group sequential design\n\n### User defined parameters\n\n* *Maximum number of stages*: 4 \n* *Stages*: 1, 2, 3, 4 \n* *Significance level*: 0.0500 \n* *Test*: two-sided \n\n### Derived from user defined parameters\n\n* *Information rates*: 0.250, 0.500, 0.750, 1.000 \n\n### Default parameters\n\n* *Type of design*: O'Brien & Fleming \n* *Type II error rate*: 0.2000 \n* *Two-sided power*: FALSE \n* *Tolerance*: 1e-08 \n\n### Output\n\n* *Cumulative alpha spending*: 5.153e-05, 0.004221, 0.02091, 0.0500 \n* *Critical values*: 4.049, 2.863, 2.337, 2.024 \n* *Stage levels (one-sided)*: 2.576e-05, 0.0021, 0.009708, 0.02147 \n\n\nCode\neo_of_crit &lt;- eo_of$criticalValues # extracts the critical values, which here are symmetric\neo_of_crit # check values are correct\n\n\n[1] 4.048591 2.862786 2.337455 2.024295\n\n\nNow that we see we’ve extracted the correct information, we can extract for each boundary and create a plot:\n\n\nCode\n# extract critical values for all objects\neo_hp_crit &lt;- eo_hp$criticalValues\neo_asof_crit &lt;- eo_asof$criticalValues\neo_asp_crit &lt;- eo_asp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-eo_of_crit, type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asof_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-eo_hp_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\nlines(x=seq(0.25,1,by=0.25), y=-eo_asp_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2','blue'), lwd=2, legend=c('OF','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n\n\n\n\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\nO’Brien-Fleming (OF) and the alpha-spending O’Brien-Fleming-like boundary (asOF) are very similar, but we do see that the alpha-spending is slightly more conservative at earlier interim looks.\nBoth Haybittle-Peto (HP) and the alpha-spending Pocock-like boundary (asP) have the same critical value at the first three looks. However, HP is more conservative, because at the final look its value of 1.982751 is very similar to the traditional threshold of 1.96. In contrast, asP’s boundary at the end of the study is now 2.3500295.\nBoundaries that are more conservative early on tend to be most similar to designs without interim monitoring for futility for the final analysis.\n\n\n\nOne-Sided Efficacy Boundaries\nWe can also easily make the same figure we had before by modifying some arguments in getDesignGroupSequential():\n\nsided becomes 1\nalpha becomes 0.025\n\nYou may be wondering, why would we change our \\(\\alpha\\) from 0.05 to 0.025? In practice, we could choose a more liberal one-sided \\(\\alpha=0.05\\), however this would change our critical value from 1.959964 at \\(\\alpha=0.025\\) to 1.6448536 at \\(\\alpha=0.05\\). This could increase our risk of a type I error (i.e., falsely finding an effect when none exists). Depending on the context, we may wish to use the more conservative one-sided \\(\\alpha=0.025\\).\nNow, let’s examine the modified code and observe the boundaries. We’ll add a comparison with \\(\\alpha=0.05\\) for our O’Brien-Fleming boundary for comparison:\n\n\nCode\neo_of1 &lt;- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.025) # O'Brien-Fleming\neo_hp1 &lt;- getDesignGroupSequential(typeOfDesign = \"HP\", kMax = 4, sided=1, alpha=0.025) # Haybittle-Peto\neo_asof1 &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending O'Brien-Fleming-like boundary\neo_asp1 &lt;- getDesignGroupSequential(typeOfDesign = \"asP\", kMax = 4, sided=1, alpha=0.025) # Alpha-spending Pocock-like boundary\n\n# add OBF with 0.05 alpha\neo_of1_alpha05 &lt;- getDesignGroupSequential(typeOfDesign = \"OF\", kMax = 4, sided=1, alpha=0.05) # O'Brien-Fleming with 0.05 boundary\n\n# extract critical values\neo_of1_crit &lt;- eo_of1$criticalValues\neo_hp1_crit &lt;- eo_hp1$criticalValues\neo_asof1_crit &lt;- eo_asof1$criticalValues\neo_asp1_crit &lt;- eo_asp1$criticalValues\neo_of1_alpha05_crit &lt;- eo_of1_alpha05$criticalValues\n\n# compare OBF with 0.025 and 0.05\nrbind( 'Two-Sided OBF with alpha=0.05'=eo_of_crit, 'One-Sided OBF with alpha=0.025'=eo_of1_crit, 'One-Sided OBF with alpha=0.05'=eo_of1_alpha05_crit)\n\n\n                                   [,1]     [,2]     [,3]     [,4]\nTwo-Sided OBF with alpha=0.05  4.048591 2.862786 2.337455 2.024295\nOne-Sided OBF with alpha=0.025 4.048591 2.862786 2.337455 2.024296\nOne-Sided OBF with alpha=0.05  3.466200 2.450973 2.001211 1.733100\n\n\nFirst, we see the top two rows are the same for our two-sided O’Brien-Fleming boundary with \\(\\alpha=0.05\\) and our one-sided O’Brien-Fleming boundary with \\(\\alpha=0.025\\). This is because our two-sided boundary assumed that our \\(\\alpha=0.05\\) was distributed symmetrically with 0.025 on both sides.\nFrom the comparison of our one-sided O’Brien Fleming boundaries, the lower row with \\(\\alpha=0.05\\) has lower critical value thresholds, making it more likely to declare significance.\nFor visual comparison of boundaries:\n\n\nCode\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Efficacy Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_crit, type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=eo_of1_alpha05_crit, type='o', lwd=2, pch=16, col='gray65')\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asof1_crit, type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_hp1_crit, type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\nlines(x=seq(0.25,1,by=0.25), y=eo_asp1_crit, type='o', lwd=2, pch=16, col='blue', lty=4)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','gray65','green4','orangered2','blue'), lwd=2, legend=c('OF','OF 0.05','asOF','HP','ASP'), bty='n', cex=0.8, lty=c(1,1,5,3,4))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Continue Study to Next Stage\")\n\n\n\n\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\nWe can visually see that the O’Brien-Fleming with \\(\\alpha=0.025\\) (OF) has higher critical value thresholds compared to the O’Brien-Fleming boundary with \\(\\alpha=0.05\\) (OF 0.05).\nAll boundaries here are identical to our previous two-sided figure since we used \\(\\alpha=0.025\\), but there is no lower boundary.\nFor one-sided hypothesis tests, the directionality of our test is very important. We must be careful to ensure we are implementing the proper test:\n\nIf we are testing \\(H_1\\colon \\mu_{trt} &gt; \\mu_{con}\\) and our estimate is \\(\\delta = \\mu_{trt} - \\mu_{con}\\), then we would want a positive critical value to stop the study early for efficacy in our one-sided test. For example, if \\(Z=5\\) we would stop at any interim look for all presented methods.\nIf \\(Z=-5\\) for our one-sided test with \\(H_1\\colon \\mu_{trt} &gt; \\mu_{con}\\), we would continue the study because we do not have evidence of efficacy. (However, if we had incorporated interim monitoring for futility we might have stopped for futility.)"
  },
  {
    "objectID": "3_interim_monitoring/index.html#rpact-and-monitoring-for-only-futility",
    "href": "3_interim_monitoring/index.html#rpact-and-monitoring-for-only-futility",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "rpact and Monitoring for Only Futility",
    "text": "rpact and Monitoring for Only Futility\nOur previous section examined efficacy-only interim monitoring. We can also design studies where we are only monitoring for futility, indicating we would only stop early if it is highly unlikely that we would be able to declare efficacy at the conclusion of the trial if we reached full enrollment.\n\nTwo-Sided Futility Boundaries\nLet’s start by exploring a few different boundaries and compare them graphically. We’ll assume we are interested in four equally spaced stages after 25%, 50%, 75%, and 100% of the trial enrollment has been observed. Using getDesignGroupSequential(), we need to add an argument that manually defines \\(\\alpha\\)-spending via the userAlphaSpending argument:\n\n\nCode\n# Two-sided O'Brien-Fleming futility boundaries\nfo_of &lt;- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.05), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock futility boundaries\nfo_p &lt;- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.05, userAlphaSpending = c(0,0,0,0.05), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n\n\nLet’s dissect some of the output and how it may look differently from our efficacy-only boundary:\n\n\nCode\nprint(fo_of)\n\n\n## Design parameters and output of group sequential design\n\n### User defined parameters\n\n* *Type of design*: No early efficacy stop \n* *Maximum number of stages*: 4 \n* *Stages*: 1, 2, 3, 4 \n* *Significance level*: 0.0500 \n* *Test*: two-sided \n* *Type of beta spending*: O'Brien & Fleming type beta spending \n\n### Derived from user defined parameters\n\n* *Information rates*: 0.250, 0.500, 0.750, 1.000 \n\n### Default parameters\n\n* *Type II error rate*: 0.2000 \n* *Two-sided power*: FALSE \n* *Binding futility*: FALSE \n* *Beta adjustment*: TRUE \n* *Tolerance*: 1e-08 \n* *User defined alpha spending*: 0.00, 0.00, 0.00, 0.05 \n\n### Output\n\n* *Power*: 0.0000, 2.872e-08, 5.862e-08, 0.8000 \n* *futilityBoundsNonBinding*: NA, 0.587, 1.369 \n* *Cumulative alpha spending*: 0.0000, 0.0000, 0.0000, 0.0500 \n* *Cumulative beta spending*: 0.00000, 0.06281, 0.13558, 0.20000 \n* *Critical values*: Inf, Inf, Inf, 1.960 \n* *Stage levels (one-sided)*: 0.0000, 0.0000, 0.0000, 0.0250 \n\n\nFrom print(fo_of) in the lower Output: section:\n\nWe now have a futilityBoundsNonBinding line that only includes 3 values, one of which is NA:\n\nThese three values represent our first three stages, which correspond to our interim analyses since the fourth stage means the trial is complete.\nThe first interim look with this design is NA, indicating that for the chosen boundary type and location of the interim look there is no setting where the design would recommend terminating for futility. In other words, we will always enroll at least 50% of the participants in our study using these O’Brien-Fleming boundaries for futility monitoring, assuming the DSMB or external factors do not terminate the trial before completion.\nNote, the because there is a stage without a valid futility boundary, the default behavior in rpact is to reallocate our \\(\\beta\\)-spending to later stages. If you wish to leave the first stage \\(\\beta\\) “unspent”, you can add the argument betaAdjustment=F to the function.\n\nThe Critical values line includes 4 values, three of which are \\(\\infty\\):\n\nThe “fourth” stage critical value is represented by the Critical values row, where we see the final threshold is our \\(Z\\)-score based on a two-sided test with \\(\\alpha=0.05\\) (i.e., qnorm(0.975)=1.96).\nThe three Inf values indicate it is impossible to stop for efficacy since we will never observe a test statistic of \\(Z=\\infty\\).\nIn practice, we could also set the critical values to be very large (e.g., \\(Z=5\\)) since this would correspond to a very large treatment effect.\n\n\nLet’s create our plots in a similar way to our efficacy examples to see these two-sided boundaries.\n\n\nCode\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of_fbnd &lt;- fo_of$futilityBounds\nfo_p_fbnd &lt;- fo_p$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of_crit &lt;- fo_of$criticalValues\nfo_p_crit &lt;- fo_p$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_of_fbnd, fo_of_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(fo_p_fbnd, fo_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n\n\n\n\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\nUnlike with efficacy boundaries, Pocock (P) futility boundaries are not constant over all stages.\nAs noted before, the O’Brien-Fleming (OF) boundaries do not have an interim stopping rule at the first stage.\n\n\n\nOne-Sided Futility Boundaries\nWe can also modify our previous futility boundaries for a one-sided hypothesis test. Here we will again change \\(\\alpha=0.05\\) to \\(\\alpha=0.025\\) for consistency with our previous example:\n\n\nCode\n# One-sided O'Brien-Fleming futility boundaries\nfo_of1 &lt;- getDesignGroupSequential(typeOfDesign = \"asUser\",  # use asUser to note we are defining a custom design\n                                  alpha=0.025, # defines alpha level for one-sided test\n                                  userAlphaSpending = c(0, 0, 0, 0.025), # sets our alpha-thresholds for stopping at each stage (0 corresponds to being impossible to stop since p-values may be approximately 0 with asymptotics but are not exact without special tests)\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# One-sided Pocock futility boundaries\nfo_p1 &lt;- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n\n\nLet’s create our plots in a similar way to our efficacy examples to see these two-sided boundaries.\n\n\nCode\n# extract critical values for all objects FOR FUTILITY BOUNDS\nfo_of1_fbnd &lt;- fo_of1$futilityBounds\nfo_p1_fbnd &lt;- fo_p1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nfo_of1_crit &lt;- fo_of1$criticalValues\nfo_p1_crit &lt;- fo_p1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Futility Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_of1_fbnd, fo_of1_crit[4]), type='o', lwd=2, pch=16)\n\nlines(x=seq(0.25,1,by=0.25), y=c(fo_p1_fbnd, fo_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4'), lwd=2, legend=c('OF','P'), bty='n', cex=0.8, lty=c(1,5))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Continue Study to Next Stage\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n\n\n\n\n\nFrom the figure comparing stopping boundaries, there are a few trends worth highlighting:\n\nWith a one-sided stopping boundary, the O’Brien-Fleming boundaries now have a critical value at the first stage! Since it is negative we can identify that we would have to observe a large effect in the “wrong” direction from our hypothesis test to stop after only 25% of the study was enrolled.\nThe O’Brien-Fleming boundary is less aggressive than the Pocock boundary. This suggests that a Pocock design may reduce our power without making other modifications (e.g., increased \\(N_{max}\\) for the study relative to a fixed sample design).\n\nWe can also compare the stopping boundaries to see how they change between our one- and two-sided designs:\n\n\nCode\nrbind( 'OBF Two-Sided'=c(fo_of_fbnd, fo_of_crit[4]), 'OBF One-Sided'=c(fo_of1_fbnd, fo_of1_crit[4]),\n       'Pocock Two-Sided'=c(fo_p_fbnd, fo_p_crit[4]), 'Pocock One-Sided'=c(fo_p1_fbnd, fo_p1_crit[4]))\n\n\n                       [,1]      [,2]     [,3]     [,4]\nOBF Two-Sided            NA 0.5866205 1.369296 1.959964\nOBF One-Sided    -0.8286311 0.5980029 1.387095 1.959964\nPocock Two-Sided  0.3105479 0.8193076 1.453437 1.959964\nPocock One-Sided  0.1181548 0.8843132 1.470841 1.959964\n\n\nWe see in the table that while boundaries are similar between one- and two-sided cases, they do have some differences, especially at earlier stages."
  },
  {
    "objectID": "3_interim_monitoring/index.html#rpact-and-monitoring-for-both-efficacy-and-futility",
    "href": "3_interim_monitoring/index.html#rpact-and-monitoring-for-both-efficacy-and-futility",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "rpact and Monitoring for Both Efficacy and Futility",
    "text": "rpact and Monitoring for Both Efficacy and Futility\nNow that we’ve covered both efficacy-only and futility-only interim monitoring, we may be interested in seeing how we can combine them together within one design. Here we’ll incorporate our code from above to leverage stopping boundaries for both efficacy and futility.\n\nTwo-Sided Boundaries for Efficacy and Futility\nIt is possible to choose different boundaries styles for futility and efficacy. Here we will consider three designs: alpha/beta-spending O’Brien-Fleming-like boundaries for both, Pocock for both, or alpha-spending with O’Brien-Fleming-like boundaries for efficacy and Pocock for futility.\n\n\nCode\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.05, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=2,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p &lt;- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.05, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n\n# \nef_ofp &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.05,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=2, beta=0.2)\n\n\nLet’s jump right into our visualized boundary shapes:\n\n\nCode\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of_fbnd &lt;- ef_of$futilityBounds\nef_p_fbnd &lt;- ef_p$futilityBounds\nef_ofp_fbnd &lt;- ef_ofp$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of_crit &lt;- ef_of$criticalValues\nef_p_crit &lt;- ef_p$criticalValues\nef_ofp_crit &lt;- ef_ofp$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact Two-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n\n\n\n\n\nThis figure is a lot busier than our previous figure, but we have some takeaways here still:\n\nThere are now 3 outcomes at each interim stage: stop for efficacy, stop for futility, or continue to the next stage.\nBy the end of the trial, we see the boundaries have all converged.\nWhen mixing the O’Brien-Fleming for efficacy and Pocock for futility we see that the efficacy boundaries are the same as the O’Brien-Fleming for both efficacy and futilty design, but the futility boundaries have shifted since they need to arrive at the same efficacy threshold.\n\nFor ease of viewing, we can also plot each design separately in a panel figure:\n\n\nCode\n# Plot boundaries\npar(mfrow=c(2,2), mar=c(4.1,4.1,3.1,1.1)) # 2x2 panel figure\n\n# OBF\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"O'Brien-Fleming-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_fbnd, ef_of_crit[4]), type='o', lwd=2, pch=16) # futility lower\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy upper\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_of_crit), type='o', lwd=2, pch=16) # efficacy lower\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"Pocock-like Boundaries\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_fbnd, ef_p_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_p_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n# OBF+Pocock\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main=\"OBF for Eff, P for Fut\")\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_fbnd, ef_ofp_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=-c(ef_ofp_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.625, y=-4.5, \"Stop for Efficacy\", cex=0.71)\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.55, y=-1.85, \"Continue Study to Next Stage\", cex=0.71)\ntext(x=0.625, y=0, \"Stop for Futility\", cex=0.71)\n\n\n\n\n\n\n\nOne-Sided Boundaries for Efficacy and Futility\nLet’s now examine how our boundaries change with a one-sided example when monitoring for both futility and efficacy. As before, we will change \\(\\alpha\\) to 0.025:\n\n\nCode\n# Two-sided O'Brien-Fleming efficacy and futility boundaries\nef_of1 &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\",  # specify O'Brien-Fleming boundaries\n                                  alpha=0.025, # defines alpha level for two-sided test\n                                  typeBetaSpending = \"bsOF\", # O'Brien-Fleming futility boundaries\n                                  bindingFutility = FALSE, # TRUE or FALSE to determine if a study MUST stop if crossing the boundary, recommend FALSE in practice to allow DSMB to take into all evidence in making recommendations\n                                  kMax = 4, \n                                  sided=1,\n                                  beta=0.2) # desired type II error rate (i.e., power=1-beta)\n\n# Two-sided Pocock efficacy and futility boundaries\nef_p1 &lt;- getDesignGroupSequential(typeOfDesign = \"asP\", alpha=0.025, \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n\n# \nef_ofp1 &lt;- getDesignGroupSequential(typeOfDesign = \"asOF\", alpha=0.025,\n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 4, sided=1, beta=0.2)\n\n\nLet’s jump right into our visualized boundary shapes:\n\n\nCode\n# extract critical values for all objects FOR FUTILITY BOUNDS\nef_of1_fbnd &lt;- ef_of1$futilityBounds\nef_p1_fbnd &lt;- ef_p1$futilityBounds\nef_ofp1_fbnd &lt;- ef_ofp1$futilityBounds\n\n# extract critical values for all objects FOR EFFICACY\nef_of1_crit &lt;- ef_of1$criticalValues\nef_p1_crit &lt;- ef_p1$criticalValues\nef_ofp1_crit &lt;- ef_ofp1$criticalValues\n\n# Plot boundaries\nplot(x=NA, y=NA, xlim=c(0.25,1), ylim=c(-5,5), xlab='Information Rate', ylab='Critical Value', xaxt='n', main='rpact One-Sided Eff+Fut Boundary Comparison')\naxis(1, at=seq(0.25,1,by=0.25) ) # label x-axis\nabline(h=seq(-6,6,by=1), col='gray90') # add horizontal lines for easier reference\nabline(h=c(-1,1)*qnorm(1-0.025), lty=2) # add critical values at Z=+/-1.96 to reflect standard group sequential boundaries\nlegend(x=0.25,y=qnorm(1-0.025)+0.4, \"qnorm(1-0.025)=1.96\", cex=0.5, adj=0.052, box.col='white')\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_fbnd, ef_of1_crit[4]), type='o', lwd=2, pch=16) # futility \nlines(x=seq(0.25,1,by=0.25), y=c(ef_of1_crit), type='o', lwd=2, pch=16) # efficacy \n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_fbnd, ef_p1_crit[4]), type='o', lwd=2, pch=16, col='green4', lty=5)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_p1_crit), type='o', lwd=2, pch=16, col='green4', lty=5)\n\n\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_fbnd, ef_ofp1_crit[4]), type='o', lwd=2, pch=16, col='orangered2', lty=3)\nlines(x=seq(0.25,1,by=0.25), y=c(ef_ofp1_crit), type='o', lwd=2, pch=16, col='orangered2', lty=3)\n\n# Add legend\nlegend(horiz=T, xpd=T, 'top', inset=-0.1, col=c('black','green4','orangered2'), lwd=2, legend=c('OF','P','OF+P'), bty='n', cex=0.8, lty=c(1,5,3))\n\n# Add text to note stopping rules\ntext(x=0.625, y=4.5, \"Stop for Efficacy\")\ntext(x=0.625, y=-4.5, \"Stop for Futility\")\ntext(x=0.55, y=1.85, \"Continue Study to Next Stage\")\ntext(x=0.625, y=0, \"Stop for Futility\")\n\n\n\n\n\nA similar story emerges from our previous examples:\n\nPocock has the biggest penalty for efficacy monitoring as evidenced by its larger critical value at 100% information.\nO’Brien-Fleming-like boundaries for both efficacy and futility have the same efficacy boundaries when using OBF for efficacy but Pocock for futility, however the futility bondaries change compared to the Pocock-like boundaries.\nWith a one-sided test, the direction of our hypothesis matters. This figure suggests that everything on the bottom portion of the graph would result in futility stopping."
  },
  {
    "objectID": "3_interim_monitoring/index.html#null-scenario-results",
    "href": "3_interim_monitoring/index.html#null-scenario-results",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "Null Scenario Results",
    "text": "Null Scenario Results\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own.\n\n\nCode\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 &lt;- 0 # arm 1 mean\nmean2 &lt;- 0 # arm 2 mean\nsd1 &lt;- 1 # arm 1 sd\nsd2 &lt;- 1 # arm 2 sd\nn1 &lt;- 100 # arm 1 sample size\nn2 &lt;- 100 # arm 2 sample size\nnsim &lt;- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed &lt;- 5145 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 &lt;- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 &lt;- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t &lt;- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 &lt;- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 &lt;- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats &lt;- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func &lt;- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval &lt;- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) &gt; 0 ){\n    nstage &lt;- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind &lt;- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des &lt;- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage &lt;- length(test_statsv)\n    sig_ind &lt;- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des &lt;- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list &lt;- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx &lt;- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   &lt;- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge &lt;- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage &lt;- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean &lt;- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd &lt;- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge &lt;- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum &lt;- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum &lt;- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per &lt;- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab &lt;- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) &lt;- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% Stopping Type\n\n\n\nStopping Rules\nRejection Rate\nESS (SD)\nAvg Stop Segment (SD)\nFutility\nEfficacy\nNo Stop\n\n\n\n\nO'Brien-Fleming Futility Only\n3.4%\n127.4 (37.0)\n3.19 (0.92)\n89.1%\n0.0%\n10.9%\n\n\nO'Brien-Fleming Efficacy Only\n4.7%\n198.8 ( 9.1)\n4.97 (0.23)\n0.0%\n2.1%\n97.9%\n\n\nO'Brien-Fleming Efficacy+Futility\n3.9%\n125.8 (35.8)\n3.15 (0.90)\n89.1%\n2.0%\n8.9%\n\n\nPocock Efficacy+Futility\n6.0%\n106.6 (33.5)\n2.66 (0.84)\n91.3%\n5.4%\n3.3%\n\n\nFixed Sample Design\n4.2%\n200.0 ( 0.0)\n5.00 (0.00)\n0.0%\n0.0%\n100.0%\n\n\n\n\n\n\n\nRemembering that this is the null scenario where we simulated no difference between study arms, we can take away numerous nuggets of information:\n\nHaving the fixed sample design is important to describe what the expected type I error rate (i.e., rejection rate) is for our simulated data. While \\(\\alpha=0.05\\), we see that in this simulation of 1000 trials the observed type I error rate was 4.2%. In other words, \\(1000 \\times 0.042 = 42\\) trials rejected the null hypothesis that \\(\\mu_1 = \\mu_2\\), even though we simulated data where the means were equal.\nIn terms of type I error control, relative to the fixed sample design:\n\nThe OBF designs with futility have lower type I error rates\nThe OBF efficacy only and Pocock E+F designs have increased type I error rates\n\nIn terms of the expected sample size:\n\nInterim monitoring helped to reduce the average sample size needed until study termination\nPocock has the lowest ESS and average stopping point because it is more aggressive with early stopping\nThe OBF efficacy only design rarely stopped, which is good since it would only stop for efficacy\nDesigns with futility stopping terminated at a point prior to trial conclusion ~80% of the time! This represents a much more efficient design relative to a fixed sample design when there is no effect.\n\nSince we simulated the null scenario here, we shouldn’t stop for efficacy. However, we see that:\n\nThe OBF Efficacy Only design stopped 2.1% of the time for efficacy\nThe OBF Efficacy+Futility design stopped 2.0% of the time for efficacy\nThe Pocock Efficacy+Futility design stopped 5.4% of the time, helping to see the more aggressive stopping boundaries\n\n\nOf course, all of the above only considers the null scenario. For a complete view of the trade-offs, we should look at alternative scenarios as well."
  },
  {
    "objectID": "3_interim_monitoring/index.html#alternative-scenario-i-results",
    "href": "3_interim_monitoring/index.html#alternative-scenario-i-results",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "Alternative Scenario I Results",
    "text": "Alternative Scenario I Results\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., mean1 or mean2), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n\nCode\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 &lt;- 0 # arm 1 mean\nmean2 &lt;- 0.4 # arm 2 mean\nsd1 &lt;- 1 # arm 1 sd\nsd2 &lt;- 1 # arm 2 sd\nn1 &lt;- 100 # arm 1 sample size\nn2 &lt;- 100 # arm 2 sample size\nnsim &lt;- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed &lt;- 7354 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 &lt;- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 &lt;- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t &lt;- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 &lt;- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 &lt;- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats &lt;- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func &lt;- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval &lt;- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) &gt; 0 ){\n    nstage &lt;- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind &lt;- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des &lt;- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage &lt;- length(test_statsv)\n    sig_ind &lt;- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des &lt;- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list &lt;- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx &lt;- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   &lt;- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge &lt;- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage &lt;- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean &lt;- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd &lt;- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge &lt;- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum &lt;- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum &lt;- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per &lt;- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab &lt;- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) &lt;- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% Stopping Type\n\n\n\nStopping Rules\nRejection Rate\nESS (SD)\nAvg Stop Segment (SD)\nFutility\nEfficacy\nNo Stop\n\n\n\n\nO'Brien-Fleming Futility Only\n74.7%\n186.6 (32.0)\n4.67 (0.80)\n17.7%\n0.0%\n82.3%\n\n\nO'Brien-Fleming Efficacy Only\n79.9%\n158.7 (40.2)\n3.97 (1.00)\n0.0%\n60.9%\n39.1%\n\n\nO'Brien-Fleming Efficacy+Futility\n75.4%\n146.9 (38.1)\n3.67 (0.95)\n18.2%\n59.9%\n21.9%\n\n\nPocock Efficacy+Futility\n66.7%\n117.7 (48.8)\n2.94 (1.22)\n28.1%\n59.8%\n12.1%\n\n\nFixed Sample Design\n79.8%\n200.0 ( 0.0)\n5.00 (0.00)\n0.0%\n0.0%\n100.0%\n\n\n\n\n\n\n\nRemembering that this is the alternative scenario where we simulated a difference of \\(\\delta=0.4\\) between study arms which was used to estimate the sample size needed for 80% power, we can take away numerous nuggets of information:\n\nHaving the fixed sample design is important to describe what the expected power (i.e., rejection rate) is for our simulated data. While \\(\\beta=0.2\\), we see that in this simulation of 1000 trials the observed power was 79.8%, which is quite close to the target of 80%.\nIn terms of power, relative to the fixed sample design:\n\nAll designs with futility monitoring for these boundaries have reduced power (~5% less for OBF, but 13.1% less for Pocock!)\nThe OBF efficacy only design has slightly higher power at 79.9% (i.e., 1 more out of 1000 simulated trials was found to be significant compared to the fixed sample design)\n\nIn terms of the expected sample size:\n\nAll designs show a lower ESS than the fixed sample design\nThe OBF efficacy design has slightly higher power and an ESS that is about 41 participants lower. We see based on the average stopping segment that it is stopping, on average, after 80% of participants have been observed.\nThe Pocock design has the lowest ESS, but this is because it stops for futility aggressively 28.1% of the time, even though we simulated data with a difference.\n\n\nWe need to balance these results with performance under the null scenario, taking into account if we are willing to tolerate reductions in power for the trade-off of more futility stopping in the null, or vice versa.\n\nNote on Futility Stopping Under the Alternative\nAn interesting quirk worth noting is that futility stopping may still be beneficial in some settings. For example, our target is 80% power, meaning that 20% of the time our study will not detect the effect even though it exists. For the 20% of trials that were not significant, it still may be helpful to stop the trial early to conserve resources.\nFor example, let’s observe the confusion matrix of the OBF design stopping for efficacy and futility with the fixed sample design:\n\n\nCode\ntable( OBF = sim_res_ef_of[2,], FS = sim_res_fs[2,])\n\n\n   FS\nOBF   0   1\n  0 186  60\n  1  16 738\n\n\nFrom this table, we see that of the 202 simulated trials that didn’t detect the effect, 92.1% (186/202) stopped for futility early. The trade-off, of course, is that we see of the 798 trials the did detect an effect, 7.5% (60/798) stopped for futility that would have gone on to reject the null hypothesis. The final piece of information to note is that 7.9% (16/202) of trials where the fixed sample did not reject \\(H_0\\), the OBF approach stopped early for efficacy and detected an effect.\nIt can be hard to juggle all these conflicting pieces of information, but it does highlight that we are often interested in trying to minimize the trade-off that harms out overall trial performance. In this case, the OBF design stopping for both efficacy and futility does reduce power to 75.4% from 79.8% in the fixed design, but it does so with a reduced type I error rate in the null scenario with a large reduction in ESS.\nHowever, it is worth noting in practice we don’t know if a trial stopping for futility is truly null, or we happened to observe something null-ish by chance. To address this, we may wish to evaluate other adaptive elements like sample size re-estimation to increase the probability of conducting a successful trial."
  },
  {
    "objectID": "3_interim_monitoring/index.html#alternative-scenario-ii-results",
    "href": "3_interim_monitoring/index.html#alternative-scenario-ii-results",
    "title": "Interim Monitoring for Futility/Efficacy/Safety",
    "section": "Alternative Scenario II Results",
    "text": "Alternative Scenario II Results\nThe code for implementing the simulation study is hidden, but you may unhide it if you wish to modify the code and re-run on your own. This alternative scenario simulates the half of the effect size used in our power calculation to achieve 80% power. The only piece of the code we need to change is one of the two means (i.e., mean1 or mean2), otherwise the code can stay as-written from the null scenario since it was flexibly written to handle multiple scenarios.\n\n\nCode\n### Step 1: Define simulation parameters for null scenario, these will be easily modifiable for any other scenario\n# Notice some of these may seem redundant (e.g., sd1=sd2=1, but are definitely separately to give you flexibility to choose different parameters by study arm)\nmean1 &lt;- 0 # arm 1 mean\nmean2 &lt;- 0.2 # arm 2 mean\nsd1 &lt;- 1 # arm 1 sd\nsd2 &lt;- 1 # arm 2 sd\nn1 &lt;- 100 # arm 1 sample size\nn2 &lt;- 100 # arm 2 sample size\nnsim &lt;- 1000 # set number of simulations, can decrease to run more quickly or increase for more precision in estimates\nseed &lt;- 7811 # set seed for reproducibility\n\n\n### Step 2: Simulate data\n# Here we will leverage the vectorization in R to minimize the use of for-loops\n# We will simulate a matrix for each study arm, with n=100 rows and 1000 columns\nset.seed(seed)\n\narm1 &lt;- matrix( rnorm(n=n1*nsim, mean=mean1, sd=sd1), ncol=nsim )\narm2 &lt;- matrix( rnorm(n=n2*nsim, mean=mean2, sd=sd2), ncol=nsim )\n\n\n### Step 3: Calculate absolute Z-scores for each simulated study at each interim stage (and final)\n# We will use this in step 4 to determine when, if at all, we stop\n# Here we leverage sapply, but you could also accomplish the same approach with a for loop\n# We are sapply/looping through the information fractions, which we round at each stage to whole number in case our n1/n2 are not nicely broken out by the 5 total looks\n\n# We will first write a helper function to take a given information fraction and implement a GLM to estimate our critical value for a linear regression (note, while it returns a t-value, since n=100 t_100 and Z are very similar):\n\nhelper_t &lt;- function(inf_fracs=c(0.2,0.4,0.6,0.8,1.0), arm1v, arm2v){\n### Helper function to estimate Z-scores given data sets and information fractions\n# inf_fracs: vector of information fractions to use, default is c(0.2,0.4,0.6,0.8,1.0)\n# arm1v/arm2v: vectors with single study of data for arm1 and arm2 to apply inf_fracs to\n  \n  n_arm1 &lt;- round( length(arm1v)*inf_fracs ) # round to ensure whole numbers\n  n_arm2 &lt;- round( length(arm2v)*inf_fracs ) # round to ensure whole numbers\n  \n  # extract |t| value for each interim fraction\n  sapply(1:length(inf_fracs), function(i) abs( summary(glm( c(arm1v[ 1:n_arm1[i] ], arm2v[ 1:n_arm2[i] ] ) ~ c(rep(0,n_arm1[i]), rep(1,n_arm2[i])) ))$coefficients[2,3] ) )\n}\n\n# Implement helper function to estimate all test statistics\nt_stats &lt;- sapply(1:nsim, function(x) helper_t(arm1v = arm1[,x], arm2v = arm2[,x])  )\n\n\n### Step 4: Apply stopping rules to each design to determine study outcome (e.g., number of stages, if study rejected H0, character description)\n\n# We will write an additional helper functions to take a single trial's statistics and return if/when it stops and why: \n\nstop_func &lt;- function(test_statsv, boundary_mat){\n### Function to estimate rejection rate for each design and average stopping boundary\n# test_statsv: vector of single trial's test statistics\n  \n  # Determine where each test statistic falls with respect to stopping interval at each stage, where 0=would stop for futility, 1=continues, 2=would stop for efficacy\n  interval &lt;- sapply(1:length(test_statsv), function(x) findInterval( test_statsv[x], vec=boundary_mat[x,] ) )\n  \n  # Determine if early stopping (0 or 2) occurs\n  if( sum( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) ) &gt; 0 ){\n    nstage &lt;- which( interval[ 1:(length(test_statsv)-1)] %in% c(0,2) )[1] # extract first segment location where stopping occurred\n    sig_ind &lt;- if( interval[nstage]==0 ){0}else if( interval[nstage]==2 ){1}else{ NA } # return if H0 rejected at interim look, NA to troubleshoot errors\n    stop_des &lt;- if( interval[nstage]==0 ){'1_futility_stop'}else{'2_efficacy_stop'}\n  }else{\n    nstage &lt;- length(test_statsv)\n    sig_ind &lt;- if( interval[ length(test_statsv) ] == 2 ){1}else{0}\n    stop_des &lt;- '3_full_enrollment'\n  }\n  \n  # return all 3 elements\n  return( c(nstage, sig_ind, stop_des) )\n}\n\nsim_res_fo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fo_of_bounds))\nsim_res_eo_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=eo_of_bounds))\nsim_res_ef_of &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_of_bounds))\nsim_res_ef_p  &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=ef_p_bounds))\nsim_res_fs    &lt;- sapply( 1:ncol(t_stats), function(y) stop_func(test_statsv=t_stats[,y], boundary_mat=fs_bounds))\n\n# Create list of results\nsim_list &lt;- list(sim_res_fo_of, sim_res_eo_of, sim_res_ef_of, sim_res_ef_p, sim_res_fs)\n\n### Step 5: Process the data and create a table for displaying results\nlibrary(kableExtra)\n\n# Create helper function to force rounding to set number of digits, including 0's\nroundx &lt;- function(x, ndigit=2){format(round(x, ndigit), nsmall = ndigit)}\n\n# Estimate mean (SD) stopping point\nsp_mean_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][1,])) )\nsp_sd_sum   &lt;- sapply(1:length(sim_list), function(x) sd( as.numeric(sim_list[[x]][1,])) )\nsp_merge &lt;- paste0( roundx(sp_mean_sum,2), ' (', roundx(sp_sd_sum,2), ')')\n\n# Estimate corresponding sample size based on stopping point\nn_stage &lt;- round(seq(0.2,1,by=0.2)*n1) + round(seq(0.2,1,by=0.2)*n2) # estimate total N at each stage\ness_mean &lt;- sapply(1:length(sim_list), function(x) mean( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_sd &lt;- sapply(1:length(sim_list), function(x) sd( n_stage[ as.numeric(sim_list[[x]][1,]) ] ) )\ness_merge &lt;- paste0( roundx(ess_mean,1), ' (', roundx(ess_sd,1), ')')\n\n# Estimate rejection rate\nrr_sum &lt;- sapply(1:length(sim_list), function(x) mean( as.numeric(sim_list[[x]][2,])) )\nrr_sum &lt;- paste0( roundx( 100*rr_sum, 1), \"%\")\n\n# Calculate % for descriptive summary of stopping\ntab_sum &lt;- sapply(1:length(sim_list), function(x) table( factor(sim_list[[x]][3,], levels=c('1_futility_stop','2_efficacy_stop','3_full_enrollment') )) ) \ntab_sum_per &lt;- matrix( paste0(roundx(100*(tab_sum / nsim), 1), \"%\"), ncol=3, byrow=T)\n\n# Combine results with kableExtra\nkbl_tab &lt;- cbind( rr_sum, ess_merge, sp_merge, tab_sum_per)\nrownames(kbl_tab) &lt;- c(\"O'Brien-Fleming Futility Only\",\"O'Brien-Fleming Efficacy Only\",\"O'Brien-Fleming Efficacy+Futility\",\"Pocock Efficacy+Futility\",\"Fixed Sample Design\")\n\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Stopping Rules','Rejection Rate','ESS (SD)','Avg Stop Segment (SD)','Futility','Efficacy','No Stop')) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \" \"=1, \" \"=1, \" \"=1, \"% Stopping Type\"=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n% Stopping Type\n\n\n\nStopping Rules\nRejection Rate\nESS (SD)\nAvg Stop Segment (SD)\nFutility\nEfficacy\nNo Stop\n\n\n\n\nO'Brien-Fleming Futility Only\n25.5%\n153.0 (43.9)\n3.82 (1.10)\n61.9%\n0.0%\n38.1%\n\n\nO'Brien-Fleming Efficacy Only\n28.1%\n189.7 (24.3)\n4.74 (0.61)\n0.0%\n18.0%\n82.0%\n\n\nO'Brien-Fleming Efficacy+Futility\n24.9%\n142.7 (39.4)\n3.57 (0.99)\n62.2%\n17.4%\n20.4%\n\n\nPocock Efficacy+Futility\n22.8%\n118.5 (41.3)\n2.96 (1.03)\n71.0%\n19.8%\n9.2%\n\n\nFixed Sample Design\n29.6%\n200.0 ( 0.0)\n5.00 (0.00)\n0.0%\n0.0%\n100.0%\n\n\n\n\n\n\n\nIn this scenario, we simulated a design where there is an effect, but it is only half that of what we used for the power calculation (i.e., \\(\\delta=0.2\\) in this simulation versus \\(\\delta=0.4\\) used in our power calculation). If this effect size is no longer clinically relevant, we would hope to stop for futility without having to carry out the entire study. If this effect is relevant, other adaptive methods like sample size re-estimation are needed to increase our power.\nFor this scenario, we see that designs with futility monitoring stop between 61.9-72.2% of the time. The power of the fixed sample design is low at 29.6%, with all designs including any interim monitoring showing lower power. This is partially due to futility monitoring if implemented, but also because the final testing threshold is adjusted for multiple testing making it harder to reject. Likely, many of the cases that are discordant are because the test statistic fell between qnorm(0.975)=1.96 and the adjusted threshold for each method."
  },
  {
    "objectID": "4_sample_size_reestimation/index.html",
    "href": "4_sample_size_reestimation/index.html",
    "title": "Sample Size Re-Estimation",
    "section": "",
    "text": "Oftentimes our power calculations represent our best guess at a realistic treatment effect, but even using previous studies or extensive clinical/scientific background can still result in uncertainty. In this module we discuss how we can incorporate re-estimation procedures during the trial to better ensure we enroll sufficient participants to detect the observed effect."
  },
  {
    "objectID": "4_sample_size_reestimation/index.html#blinded-example-with-binary-outcome",
    "href": "4_sample_size_reestimation/index.html#blinded-example-with-binary-outcome",
    "title": "Sample Size Re-Estimation",
    "section": "Blinded Example with Binary Outcome",
    "text": "Blinded Example with Binary Outcome\nIn our first example we observe how the blindrecalc package can be used for a study with a binary outcome. We will use a chi-squared test as our motivating example, largely following the steps from their helpful R Journal paper.\nFirst we set up our design as the chi-squared test using setupChiSquare(). In this example we are testing a one-sided hypothesis where \\(H_1\\colon p_1 &gt; p_2\\) with \\(\\alpha=0.025\\), \\(\\beta=0.2\\) (i.e., power of 80%), and we are interested in detecting a difference between two groups of 20% (i.e., \\(\\delta=0.2\\)).\nWe can then estimate the number needed in a fixed sample design using n_fix(). Here the nuisance parameter represents the average of the two groups. We can then estimate the \\(N\\) needed overall with \\(N/2\\) in each randomized group to detect a difference of 20%.\n\n\nCode\nlibrary(blindrecalc)\n\n# Compare basic functions\ndesign &lt;- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn_fix(design, nuisance = c(0.2, 0.3, 0.4, 0.5))\n\n\n[1] 124 164 186 194\n\n\nWe can verify these sample sizes are analogous to the power.prop.test() function in base R:\n\n\nCode\n### Check that power.prop.test matches n_fix (which it does)\n## round up N for each group, multiple by 2 to match overall sample size from n_fix\n\n# nuisance 0.2\na1 &lt;- ceiling(power.prop.test(p1=0.1,p2=0.3,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.3\na2 &lt;- ceiling(power.prop.test(p1=0.2,p2=0.4,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.4\na3 &lt;- ceiling(power.prop.test(p1=0.3,p2=0.5,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# nuisance 0.5\na4 &lt;- ceiling(power.prop.test(p1=0.4,p2=0.6,sig.level=0.025,power=0.8, alternative='o')$n)*2\n\n# print sample sizes\nc(a1,a2,a3,a4)\n\n\n[1] 124 164 186 194\n\n\nLet’s focus on the nusiance=0.2 case. We can examine the impact on our design if we observe other nuisance parameters even though we anticipated 0.2. The toer() function allows us to estimate the type I error rate we may encounter if we do or do not use blinded re-estimation:\n\n\nCode\ndesign &lt;- setupChiSquare(alpha = 0.025, beta = 0.2, delta = 0.2, alternative = \"greater\")\nn &lt;- n_fix(design, nuisance = 0.2) # N = 124 total (62 per group)\np &lt;- seq(0.2, 0.6, by = 0.1)\ntoer_fix &lt;- toer(design, n1 = n, nuisance = p, recalculation = FALSE)\ntoer_ips &lt;- toer(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\nt1e_tab &lt;- rbind( \"No SSR\" = toer_fix, \"SSR at 1/2 Enrolled\" = toer_ips)\ncolnames(t1e_tab) &lt;- p\nt1e_tab\n\n\n                           0.2        0.3        0.4        0.5        0.6\nNo SSR              0.02366058 0.02484306 0.02701163 0.02943799 0.02701163\nSSR at 1/2 Enrolled 0.02544561 0.02536076 0.02484492 0.02565171 0.02484492\n\n\nBy comparing these type I error rates, we see that there can be an inflation to our desired \\(\\alpha=0.025\\) due to observing different averages between our two groups (i.e., the nuisance parameters reflected by the column headers). With blinded SSR, we see better control of the type I error rate.\nWe can use the similar pow() to estimate the power under our design with varying nuisance parameters:\n\n\nCode\npow_fix &lt;- pow(design, n1 = n, nuisance = p, recalculation = FALSE)\npow_ips &lt;- pow(design, n1 = n/2, nuisance = p, recalculation = TRUE)\n\npow_tab &lt;- rbind( \"No SSR\" = pow_fix, \"SSR at 1/2 Enrolled\" = pow_ips)\ncolnames(pow_tab) &lt;- p\npow_tab\n\n\n                          0.2       0.3      0.4       0.5      0.6\nNo SSR              0.8100375 0.6892655 0.641467 0.6381293 0.641467\nSSR at 1/2 Enrolled 0.7871930 0.7933507 0.794409 0.7994804 0.794409\n\n\nFor power we see that SSR maintains approximately 80% power, but without SSR the power decreases as or nuisance parameter grows.\nWe can also plot the distribution of sample sizes that would occur at different nuisance parameters to evaluate our potential risk of having a much larger sample size:\n\n\nCode\nn_dist(design, n1 = n/2, nuisance = p, plot = TRUE)\n\n\n\n\n\n         p = 0.2  p = 0.3  p = 0.4  p = 0.5  p = 0.6\nMin.     62.0000  78.0000 122.0000 154.0000 122.0000\n1st Qu. 104.0000 148.0000 178.0000 192.0000 178.0000\nMedian  122.0000 166.0000 188.0000 194.0000 188.0000\nMean    122.1816 160.6994 184.1047 191.8221 184.1047\n3rd Qu. 136.0000 174.0000 192.0000 194.0000 192.0000\nMax.    184.0000 194.0000 194.0000 194.0000 194.0000\n\n\nIn this figure above we see that if our nuisance parameter is actually 0.2 as assumed for the power calculation, or sample size re-estimation could range from 62 to 184 versus our planned \\(N=124\\). However, this does assume we allow for sample size reductions and have no limit on the maximum increase. If our guess of the nuisance parameter is 0.5, we see the largest increase in our expected sample size up to 191.8, with a range of 154 to 194."
  },
  {
    "objectID": "4_sample_size_reestimation/index.html#unblinded-conditional-power-example-with-binary-outcome",
    "href": "4_sample_size_reestimation/index.html#unblinded-conditional-power-example-with-binary-outcome",
    "title": "Sample Size Re-Estimation",
    "section": "Unblinded Conditional Power Example with Binary Outcome",
    "text": "Unblinded Conditional Power Example with Binary Outcome\nThe gsDesign package includes the ssrCP() which allows us to implement an unblinded sample size re-estimation procedure. Here we continue our previous example using a binary outcome. In this case we will incorporate traditional group sequential O’Brien-Fleming boundaries for interim monitoring for efficacy. This means our sample size will also be adjusted relative to a fixed design to account for the use of the traditional GSD (this is in contrast to alpha-spending boundaries that are OBF-like, where the sample size remains fixed).\nFirst, we must establish the design type based on the gsDesign() function before extending to a conditional power design with ssrCP():\n\n\nCode\nlibrary(gsDesign) # load package\n\nx &lt;- gsDesign(\n  k = 2, # number of analyses planned, including interim and final\n  n.fix = 196, # sample size for a fixed design with no interim\n  timing = 0.5, # timing of interm analyses \n  test.type=2, # 6 options covering one- vs. two-sided and symmetric vs. asymmetric boundaries; 2 is a two-sided symmetric approach\n  alpha = 0.025, # one-sided type I error rate\n  beta = 0.2, # type II error rate (i.e., power=1-beta)\n  delta0 = 0, # null hypothesis parameter (i.e., no difference)\n  delta1 = 0.2, # alternative hypothesis parameter (i.e., difference we wish to detect)\n  sfu='OF' ) # alpha-spending for efficacy monitoring\n\n# plot stopping boundaries\nplot(x)\n\n\n\n\n\nThe plot shows our stopping boundaries for our two-sided test, with the final critical value being 1.98 (versus 1.96 in a fixed design without interim monitoring), as well as an increase from \\(N=196\\) to \\(N=198\\).\nWe can then extend this design to a conditional power design based on an observed test statistic at the end of our first stage. Here we assume we observed \\(Z=1.6\\):\n\n\nCode\n# extend design to a conditional power design\nxx &lt;- ssrCP(x = x, # provide design used\n            z1 = 1.6, # enter observed test statistic\n            overrun = 0, # can note how many participants are enrolled but not included in the interim analysis\n            beta = 0.2, # targeted type II error for SSR (i.e., targeted power=1-beta)\n            cpadj = c(0.5,0.8), # range of conditional powers for which SSR is to be performed, otherwise N from original design used\n            maxinc = 2, # argument limiting maximum fold-increase from planned max N (e.g., 2 times)\n            z2 = z2NC) # combination function to combine stage 1 and stage 2 results; z2NC=inverse normal combination test, z2Z=sufficient stat for complete data, z2Fisher=Fisher's combination test\n\n# show immediately relevant information\nxx$dat\n\n\n   z1      z2       n2        CP     theta      delta\n1 1.6 1.19651 259.0207 0.6567062 0.1609988 0.03219977\n\n\nFrom the ssrCP documentation, we see these values represent:\n\nz1: input z1 values,\nz2: computed cutoffs for the standard normal test statistic based solely on stage 2 data\nn2: stage 2 sample size (however, based on other functions I believe this may be the maximum sample size to enroll, so we need to take n2-n1)\nCP: stage 2 conditional power\ntheta: standardize effect size used for conditional power calculation\ndelta: the natural parameter value corresponding to theta The relation between theta and delta is determined by the delta0 and delta1 values from x: delta = delta0 + theta(delta1-delta0).\n\nThe most important summary is the re-estimated sample size of \\(N=259.0207\\) which rounds up to \\(N=260\\), resulting in a need to enroll \\(N_2=260-99=161\\) in stage 2 instead of the original \\(N_2 = 198-99=99\\). Overall, this results in a total sample size of \\(N_1+N_2=99+161=260\\), which is less than the two times inflation allowed by maxinc=2 (i.e., up to \\(198\\times2 = 396\\) is allowed).\nThe other useful summary is the z2=1.19651, which represents that we need to observe a test statistic at least this large for our inverse normal combination test to be significant.\nIf we assume we enroll the 161 additional participants and observe \\(Z_2=1.3\\), we would have a normal combination test of\n\\[ \\frac{Z_1 + Z_2}{\\sqrt{2}} = \\frac{1.6 +1.3}{\\sqrt{2}} = 2.05 &gt; 1.96 = Z_{0.975} = Z_{1-\\alpha/2} \\]\nTherefore, we would reject the null hypothesis and conclude we found an effect."
  },
  {
    "objectID": "4_sample_size_reestimation/index.html#blinded-re-estimation-with-binary-outcome-simulation",
    "href": "4_sample_size_reestimation/index.html#blinded-re-estimation-with-binary-outcome-simulation",
    "title": "Sample Size Re-Estimation",
    "section": "Blinded Re-estimation with Binary Outcome Simulation",
    "text": "Blinded Re-estimation with Binary Outcome Simulation\nWe first implement our blinded re-estimation procedure. We will compare three strategies:\n\nBlinded SSR where we will allow for a smaller than planned sample size if indicated (i.e., if stage 2 needs fewer than 100 more participants, we will enroll that number)\nBlinded SSR where we will continue with the planned sample size if the re-estimation indicates fewer participants could be needed (i.e., if stage 2 needs fewer than 100 more participants, we will still enroll 100)\nA fixed sample design with no SSR\n\nThe code is hidden, but can be shown if desired. We will summarize the rejection rate and average (SD) sample size across 1,000 simulation trials.\n\n\nCode\nsim_list &lt;- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nblinded_res &lt;- blinded_n2_res &lt;- fixed_res &lt;- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\nn &lt;- 200 # total sample size based on fixed sample\nn1 &lt;- 100 # sample size to enroll for stage 1\ndelta &lt;- 0.15 # expected effect size under H1 from power calculation\nr &lt;- 1 # randomization ratio (e.g., 1:1)\nnsim &lt;- 1000\n\n###\n# simulate method with SSR allowing for smaller than expected N\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres &lt;- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 &lt;- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 &lt;- p0 - delta*(r/(1+r))\n    pc_n1 &lt;- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest &lt;- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 &lt;- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 &lt; 0 ){ n2 &lt;- 0 } # if sufficient sample size already, set to 0\n    \n    # simulate stage 2 data\n    trt &lt;- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con &lt;- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res &lt;- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')    # defined as less based on order of data entered for trt and con\n    simres[i,] &lt;- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_res[combo,] &lt;- c( mean(simres$p &lt; 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate method with SSR but enrolling at least (N-N1) in stage 2 (i.e., not allowing fewer participants)\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres &lt;- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # blinded re-estimation\n    p0 &lt;- sum(trt,con) / n1\n    \n    # assuming same delta, estimate new pt and pc\n    pt_n1 &lt;- p0 - delta*(r/(1+r))\n    pc_n1 &lt;- p0 + delta*(r/(1+r))\n    \n    # use power.prop.test based on re-estimated pt (p1) and pc (p2)\n    n_rest &lt;- 2*ceiling(power.prop.test(p1=pt_n1,p2=pc_n1,sig.level=0.025,power=0.8,alternative='o')$n)\n    n2 &lt;- n_rest - n1 # estimate sample size needed for remainder\n    if( n2 &lt; (n-n1) ){ n2 &lt;- (n-n1) } # enroll at least (n-n1)\n    \n    # simulate stage 2 data\n    trt &lt;- c(trt, rbinom(n=n2/2, size=1, prob=pt) )\n    con &lt;- c(con, rbinom(n=n2/2, size=1, prob=pc) )\n    \n    # final analysis, save results\n    res &lt;- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')    # defined as less based on order of data entered for trt and con\n    simres[i,] &lt;- c(round(res$p.value,4), n1+n2 )\n  }\n  \n  blinded_n2_res[combo,] &lt;- c( mean(simres$p &lt; 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n}\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres &lt;- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 &lt;- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 &lt;- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res &lt;- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')    # defined as less based on order of data entered for trt and con\n    simres[i,] &lt;- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] &lt;- c( mean(simres$p &lt; 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab &lt;- cbind(fixed_res,blinded_res,blinded_n2_res)\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with Lower N2\"=3, \"SSR with At Least N2\"=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Sample\n\n\nSSR with Lower N2\n\n\nSSR with At Least N2\n\n\n\nScenario\nRejection Rate\nESS\nESS SD\nRejection Rate\nESS\nESS SD\nRejection Rate\nESS\nESS SD\n\n\n\n\nNull 25 v 25\n0.020\n200\n0\n0.022\n257.3\n31.0\n0.022\n258.1\n29.2\n\n\nNull 17.5 v 17.5\n0.016\n200\n0\n0.023\n197.7\n35.1\n0.021\n212.9\n18.6\n\n\nAlt 10 v 25\n0.754\n200\n0\n0.734\n198.3\n33.9\n0.784\n212.7\n18.5\n\n\nAlt 15 v 25\n0.368\n200\n0\n0.384\n218.7\n34.2\n0.394\n225.3\n24.8\n\n\nAlt 15 v 30\n0.689\n200\n0\n0.748\n239.8\n33.0\n0.762\n242.0\n28.9\n\n\n\n\n\n\n\nFrom these simulation results we can see that:\n\nWithout some form of futility testing, the blinded SSR methods lead to increased sample sizes in the null scenario with 25% versus 25%, as well as slightly higher type I error rates than the fixed sample design.\nIf the null scenario response reflects the nuisance parameter, we still see slightly higher type I error rates with SSR. However, the ESS increase is less large, likely because the effect went closer to 0. This may indicate that less prevalent outcomes are less affected by the null scenario with blinded re-estimation.\nIf we encounter the effect sizes used in the power analysis, we see that SSR that enrolls at least the \\(N_{total}=200\\) of the fixed sample design increases power by allowing for larger sample sizes. On average, across all 1000 simulated trials, the average sample size was 212.7 (SD=18.5).\nIf we encounter an underpowered scenario, we see that the SSR methods also lead to an increased SSR but only slightly higher power. This suggests that either unblinded re-estimation processes or potentially futility monitoring could further improve performance.\nFinally, if we observe the same \\(\\delta=0.15\\) but at higher observed rates (\\(p_{trt}=0.15\\) and \\(p_{con}=0.30\\)), our blinded SSR increases power by 5.9 to 7.3% by allowing our sample size to increase based on the higher nuisance paramter (i.e., \\(\\frac{0.15+0.3}{2}=0.225\\))."
  },
  {
    "objectID": "4_sample_size_reestimation/index.html#unblinded-re-estimation-with-binary-outcome-simulation",
    "href": "4_sample_size_reestimation/index.html#unblinded-re-estimation-with-binary-outcome-simulation",
    "title": "Sample Size Re-Estimation",
    "section": "Unblinded Re-estimation with Binary Outcome Simulation",
    "text": "Unblinded Re-estimation with Binary Outcome Simulation\nWe can compare the results from our blinded SSR approach with an approach using unblinded SSR approaches. In this simulation we compare:\n\nAn approach using conditional power estimated from gsDesign::ssrCP(), where we assume that the second stage must have at least 100 participants but could increase to 200 participants (i.e., we don’t allow fewer than expected participants in stage 2). This approach uses the inverse normal combination test based on the p-values from the two-sample test of proportions.\nAn approach that unblinds the control arm to use in re-estimating the power.prop.test() calculation for a decrease of 15% in the treatment arm. If the control arm has a response rate less than 15%, we stop for futility and calculate the one-sided p-value to record for futility. Otherwise, we allow the re-estimation to increase the sample size from 100 up to 200. Since we do not unblind the treatment arm, in this approach we evaluate the performance if we use the overall data for our final test p-value.\nA fixed sample design enrolling 200 total participants without interim monitoring or re-estimation.\n\nThe simulation code block is hidden, but can be shown for review.\n\n\nCode\nlibrary(gsDesign) # load library\n\n# list of 5 simulation scenarios to mimic our blinded SSR\nsim_list &lt;- list( c(0.25, 0.25), c(0.175,0.175), c(0.1, 0.25), c(0.15, 0.25), c(0.15, 0.3))\n\n# Create objects to store results in\nunblinded_res &lt;- unblinded_adhoc_res &lt;- fixed_res &lt;- matrix(nrow=5, ncol=3, dimnames = list(c('Null 25 v 25','Null 17.5 v 17.5','Alt 10 v 25','Alt 15 v 25','Alt 15 v 30'), c('Rej_Rate','ESS','ESS_SD')))\n\n# Set simulation parameters\ndelta &lt;- 0.15 # expected effect size under H1 from power calculation\nr &lt;- 1 # randomization ratio (e.g., 1:1)\nnsim &lt;- 1000\n\nx &lt;- gsDesign(\n  k = 2, \n  n.fix = 200, timing = 0.5, test.type=2,\n  alpha = 0.025, beta = 0.2, delta0 = 0, delta1 = 0.15, sfu='OF' ) \n\nn_fix &lt;- 200 # total sample size based on fixed sample\nn1 &lt;- ceiling(x$n.I[1])\nn &lt;- ceiling(x$n.I[2]) \n\n###\n# simulate method with SSR using conditional power\n\nfor( combo in 1:length(sim_list) ){\n  \n  # initialize object to save results in\n  simres_gs &lt;- data.frame( zcombined=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    res_int &lt;- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')$p.value\n    \n    # UNblinded re-estimation\n    xx &lt;- ssrCP(x = x, z1 = qnorm(1-res_int), overrun = 0, beta = 0.2, cpadj = c(0.5,0.8), maxinc = 1.5, z2 = z2NC)\n    n_rest &lt;- ceiling(xx$dat$n2) - n1\n      n2 &lt;- n_rest\n      n2 &lt;- if(n2 &lt;= 100){ 100 }else{n2} \n    if( res_int &lt; x$upper$prob[1,1] ){ \n      # final analysis, save results\n      zcombined &lt;- qnorm(1-res_int) / sqrt(1)\n      simres_gs[i,] &lt;- c(zcombined, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 &lt;- rbinom(n=n2/2, size=1, prob=pt)\n      con2 &lt;- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res &lt;- prop.test(x=c(sum(trt2),sum(con2)), n=c(length(trt2),length(con2)), alternative = 'less')$p.value\n      \n      zcombined &lt;- ( qnorm(1-res_int) + qnorm(1-res) ) / sqrt(2)\n      \n      simres_gs[i,] &lt;- c(zcombined, n1+n2)\n    }\n  }\n  \n  unblinded_res[combo,] &lt;- c( mean(simres_gs$zcombined &gt;= qnorm(1-(0.025))), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n###\n# somewhat ad hoc approach to unblinded SSR\n# first stage is used to implement re-estimation; no decreases in stage 2; up to 200 (versus 100) otherwise stop for futility\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes based on fixed sample to initialize\n  n &lt;- 200\n  n1 &lt;- 100\n  \n  # initialize object to save results in\n  simres_gs &lt;- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    res_int &lt;- prop.test(x=c(sum(trt),sum(con)), n=c(length(trt),length(con)), alternative = 'less')\n    \n    # UNblinded re-estimation, where we use the estimate of the control group and still power for a 0.15 decrease in treatment\n    n_reest &lt;- if( mean(con) &lt;= 0.15 ){ 10000 }else{ ceiling(power.prop.test(p1=mean(con)-0.15,p2=mean(con),sig.level=0.025,power=0.8, alternative='o')$n)*2 }\n    \n    n2 &lt;- if( (n_reest - n1) &lt; 100 ){100}else{n_reest - n1}\n    \n      if( n2 &gt; 200 ){\n      # ad hoc futility rule, save first stage result\n      p &lt;- res_int$p.value\n      simres_gs[i,] &lt;- c(p, n1+0)\n    }else{\n      # simulate stage 2 data\n      trt2 &lt;- rbinom(n=n2/2, size=1, prob=pt)\n      con2 &lt;- rbinom(n=n2/2, size=1, prob=pc)\n      \n      # final analysis, save results\n      res &lt;- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(length(c(trt,trt2)),length(c(con,con2))), alternative = 'less')\n      p &lt;- res$p.value\n  \n      simres_gs[i,] &lt;- c(p, n1+n2)\n    }\n  }\n  \n  unblinded_adhoc_res[combo,] &lt;- c( mean(simres_gs$p &lt; 0.025), round(mean(simres_gs$n),1), round(sd(simres_gs$n),1))\n\n}\n\n\n\n###\n# simulate fixed sample design for comparison\nfor( combo in 1:length(sim_list) ){\n  \n  # set sample sizes for fixed sample\n  n &lt;- 200\n  n1 &lt;- 100\n  \n  # initialize object to save results in\n  simres &lt;- data.frame( p=rep(NA,nsim), n=rep(NA,nsim) )\n  \n  pt &lt;- sim_list[[combo]][1]\n  pc &lt;- sim_list[[combo]][2]\n  \n  # loop through nsim simulations\n  for( i in 1:nsim ){\n    set.seed(i) # set seed for reproducibility\n    \n    # simulate stage 1\n    trt &lt;- rbinom(n=n1/2, size=1, prob=pt)\n    con &lt;- rbinom(n=n1/2, size=1, prob=pc)\n    \n    # simulate stage 2 after stage 1 to keep same random sequence\n    trt2 &lt;- rbinom(n=(n-n1)/2, size=1, prob=pt)\n    con2 &lt;- rbinom(n=(n-n1)/2, size=1, prob=pc)\n\n    # final analysis\n    res &lt;- prop.test(x=c(sum(c(trt,trt2)),sum(c(con,con2))), n=c(n/2,n/2), alternative = 'less')    # defined as less based on order of data entered for trt and con\n    simres[i,] &lt;- c(round(res$p.value,4), n )\n  }\n\n  fixed_res[combo,] &lt;- c( mean(simres$p &lt; 0.025), round(mean(simres$n),1), round(sd(simres$n),1))\n  \n} \n\n# Format results\nlibrary(kableExtra)\nkbl_tab &lt;- cbind(fixed_res,unblinded_res,unblinded_adhoc_res)\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Scenario',rep(c('Rejection Rate','ESS','ESS SD'), 3)) ) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \"Fixed Sample\"=3, \"SSR with CP\"=3, \"SSR Unblind Control\"=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixed Sample\n\n\nSSR with CP\n\n\nSSR Unblind Control\n\n\n\nScenario\nRejection Rate\nESS\nESS SD\nRejection Rate\nESS\nESS SD\nRejection Rate\nESS\nESS SD\n\n\n\n\nNull 25 v 25\n0.020\n200\n0\n0.010\n203.9\n11.7\n0.022\n212.6\n39.0\n\n\nNull 17.5 v 17.5\n0.016\n200\n0\n0.015\n203.7\n13.4\n0.022\n170.3\n49.1\n\n\nAlt 10 v 25\n0.754\n200\n0\n0.716\n198.7\n46.0\n0.740\n212.6\n39.0\n\n\nAlt 15 v 25\n0.368\n200\n0\n0.335\n206.4\n30.9\n0.378\n212.6\n39.0\n\n\nAlt 15 v 30\n0.689\n200\n0\n0.656\n202.0\n44.9\n0.727\n224.6\n51.8\n\n\n\n\n\n\n\nThe simulation results indicate:\n\nThat the fixed sample design is slightly conservative with type I error of 2%, which is similar to the SSR with unblinded controls at 2.2%, although the sample size increases to an average of 212.6 (SD=39.0).\nThe SSR with conditional power is overly conservative, as seen by the lower type I error rates and power compared to the fixed sample design.\nThe SSR unblinded control shows the greatest improvement when the control reference is higher than expected (i.e., 30% instead of 25%), where power increases to 72.7% versus 68.9% for a fixed sample design. However, the ESS does increase to 224.6 (SD=51.8) versus 200 for the fixed sample design.\nSome limitations in performance may be due to the fact that our allowed increase is somewhat limited in stage 2 from 100 to 200. In practice, if it was feasible to go higher we could likely improve our power relative to a fixed sample design.\n\nWhile only a limited set of methods and scenarios with a binary outcome, these results suggest the caution should be taken in implementing sample size re-estimation since it can introduce variability in our needed sample size while also potentially resulting in limited gains in power or type I error performance relative to a fixed sample design."
  },
  {
    "objectID": "6_treatment_arm_selection/index.html",
    "href": "6_treatment_arm_selection/index.html",
    "title": "Adaptive Treatment Arm Selection",
    "section": "",
    "text": "Overview\nThere may be uncertainty as to which study arms to include in a prospective trial, especially if we have multiple doses to consider, multiple candidate therapies, or potentially limited resources to explore all options. In this module we introduce the adaptive concept of treatment arm dropping and adding.\n\n\nSlide Deck\n\n\n \nYou can also download the original PowerPoint file.\n\n\nCode Examples in R\nWithin R there are plenty of packages to implement dose-finding algorithms, which can be thought of as a form of arm dropping:\n\nCRM: implements the continual reassessment method for phase I clinical trials\nbcrm: implements a Bayesian version of the CRM\nDoseFinding: provides functions for designing and analyzing dose-finding experiments with a focus on phase II studies\n\nFor general treatment arm selection, many approaches are custom coded by the user depending on the rules used.\n\n\nSimulation Study\nFor a brief simulation study, let’s compare the operating characteristics for a set of different arm dropping rules:\n\nKeep all active arms that are not dropped for futility based on one-sided Pocock stopping boundaries\nDrop the arm with the smallest treatment effect at each stage as long as it is not statistically significant at a more generous \\(\\alpha=0.1\\) threshold\nDrop the arm with the smallest treatment effect regardless of statistical significance so the study ends with two arms\n\nIn our simulation we make the following assumptions:\n\nThere are six total arms (1 shared control, 5 treatment arms )\nEach arm will enroll 100 if it never stops (i.e., power calculation for two-sample t-test assuming \\(\\alpha=0.05\\), \\(\\beta=0.8\\), \\(\\delta=0.4\\), \\(\\sigma=1\\))\nWe will plan for 5 total stages (i.e., 20 per stage) so that the final analysis can end with a control versus winner(s) comparison\nWe assume normally distributed outcomes with \\(\\sigma=1\\) for all arms\nThe control group mean response is 0, our 5 treatment arm responses are -0.05, 0, 0.1, 0.35, 0.4 (i.e., worse than control, same as control, three improved relative to control by differing degrees)\nWe will not make any corrections for multiple testing\n\nNow let’s explore our results for the arms that remain at the end of each study, overall sample size, and the rejection rates:\n\n\nCode\nlibrary(rpact) # load rpact for futility bounds\n\nfo_p1 &lt;- getDesignGroupSequential(typeOfDesign = \"asUser\", alpha=0.025, userAlphaSpending = c(0,0,0,0,0.025), \n                                 typeBetaSpending = \"bsP\", # Pocock futility boundaries\n                                 bindingFutility = FALSE, kMax = 5, sided=1, beta=0.2)\nfo_p1_crit &lt;- fo_p1$futilityBounds #  extract futility boundaries\nfo_p1_crit_mat &lt;- matrix( c(fo_p1_crit, -Inf), ncol=5, nrow=5) # create matrix to compare with test statistics in simulation, add -Inf for final comparison at end of trial\n\n# set means (m) per study arm\nmc &lt;- 0\nm1 &lt;- -0.05\nm2 &lt;- 0\nm3 &lt;- 0.1\nm4 &lt;- 0.35\nm5 &lt;- 0.4\n\n# set other parameters\nsc &lt;- s1 &lt;- s2 &lt;- s3 &lt;- s4 &lt;- s5 &lt;- 1 # common variance, could change for other scenarios\nseed &lt;- 515 # seed for reproducibility\nnmax &lt;- 100 # max per arm\nnstage &lt;- 5 # total number of stages\nn_perstage &lt;- ceiling( seq(0,100,length.out=6)[-1] ) # number enrolled in each stage (so you can change nmax, nstage, etc. and code still works)\nnsim &lt;- 1000 # number of simulations\n\nstrat1_res &lt;- strat2_res &lt;- strat3_res &lt;- matrix(nrow=nsim, ncol=11) # create objects to save simulation results\n\n# simulation\nset.seed(seed) # set seed for reproducibility\n\nfor( i in 1:nsim ){\n  \n  # use sapply to create matrix of data with each data set represented by a column\n  simdat &lt;- sapply( c('c',1:5), function(x) rnorm(mean = get(paste0('m',x)), sd = get(paste0('s',x)), n=nmax) ) \n  \n  # calculate two-sample t-tests for what the observed test statistic and p-value would be at each stage\n  # write helper function, paircompare(), to extract this information\n  paircompare &lt;- function(arm_control, arm_trt, n_perstage){\n    ### Helper function to calculate test statistic and p-value for two groups given data and sample sizes to use\n    # arm_control/arm_trt: vector with observed data up to max sample size\n    # n_perstage: sample size after each stage\n    \n    tres &lt;- t(sapply(n_perstage, function(z) t.test(arm_trt[1:z], arm_control[1:z], alternative = 'greater')[c('p.value','statistic')] ))\n    eres &lt;- sapply(n_perstage, function(z) mean(arm_trt[1:z]) - mean(arm_control[1:z] ) )\n    \n    return( cbind(tres, eres) )\n  }\n  \n  res &lt;- sapply( 2:6, function(w) paircompare(arm_control = simdat[,1], arm_trt = simdat[,w], n_perstage = n_perstage)  )\n  pval &lt;- as.matrix(res[1:5,]) # extract p-values at each stage for control vs. active arm\n  tval &lt;- as.matrix(res[6:10,]) # extract t-values at each stage for control vs. active arm\n  diff &lt;- as.matrix(res[11:15,]) # extract observed effect size (trt - con) at each stage (one-sided goal with trt &gt; con)\n  \n  \n  ### Strategy 1: Pocock Boundaries\n  \n  fut_stop &lt;- (tval &lt; fo_p1_crit_mat) # calculate if each arm has any test statistics below the futility boundary\n  \n  arm_stop1 &lt;- sapply( 1:5, function(a) which(fut_stop[,a] == TRUE)[1] )\n  arm_stop1[ is.na(arm_stop1) ] &lt;- 5 # make NA 5 since they never dropped for futility\n  \n  n_strat1 &lt;- n_perstage[arm_stop1] # record sample size for each arm\n  ntot1 &lt;- sum(n_strat1) # sum up for total sample size\n  \n  finish1 &lt;- arm_stop1==5 # calculate indicator if arm made it to the end\n  \n  sig1 &lt;- rep(FALSE, 5) # create indicator if significant comparison\n  sig1[ which(arm_stop1==5) ] &lt;- unlist(pval[,5])[ which(arm_stop1==5) ] &lt; 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat1_res[i,] &lt;- c(ntot1, finish1, sig1)\n  \n  \n  ### Strategy 2: Drop smallest treatment effect arm as long as not significant\n  \n  diff2 &lt;- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop2 &lt;- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum &lt;- which( unlist(diff2[k,]) == min(unlist(diff2[k,])) ) # calc arm with min effect size\n    \n    if( pval[k,armnum] &gt;= 0.1 ){\n      arm_stop2[armnum] &lt;- k\n      diff2[,armnum] &lt;- Inf # make all observed diffs large to ignore in next stage(s)\n    }\n  }\n\n  n_strat2 &lt;- n_perstage[arm_stop2] # record sample size for each arm\n  ntot2 &lt;- sum(n_strat2) # sum up for total sample size\n  \n  finish2 &lt;- arm_stop2==5 # calculate indicator if arm made it to the end\n  \n  sig2 &lt;- rep(FALSE, 5) # create indicator if significant comparison\n  sig2[ which(arm_stop2==5) ] &lt;- unlist(pval[,5])[ which(arm_stop2==5) ] &lt; 0.025 # estimate if arm is significant at alpha=0.025\n  \n  # save results\n  strat2_res[i,] &lt;- c(ntot2, finish2, sig2)\n  \n\n    \n  ### Strategy 3: Drop smallest treatment effect arm regardless of significance\n  \n  diff3 &lt;- diff # create copy of object to manipulate for decision rule\n  \n  arm_stop3 &lt;- rep(5,5) # create object to save when arm stops, assume 5 for all to start\n  \n  for( k in 1:4 ){\n    armnum &lt;- which( unlist(diff3[k,]) == min(unlist(diff3[k,])) ) # calc arm with min effect size\n    \n    arm_stop3[armnum] &lt;- k\n    diff3[,armnum] &lt;- Inf # make all observed diffs large to ignore in next stage(s)\n  }\n\n  n_strat3 &lt;- n_perstage[arm_stop3] # record sample size for each arm\n  ntot3 &lt;- sum(n_strat3) # sum up for total sample size\n  \n  finish3 &lt;- arm_stop3==5 # calculate indicator if arm made it to the end\n  \n  sig3 &lt;- sapply( 1:5, function(u) pval[ arm_stop3[u], u] &lt; 0.025 ) # create indicator if significant comparison, here we will check each arm regardless of stopping point\n  \n  # save results\n  strat3_res[i,] &lt;- c(ntot3, finish3, sig3)\n}\n\n# Format results to display\nstrat1 &lt;- colMeans(strat1_res)\ns1_sd &lt;- sd( strat1_res[,1] )\ns1res &lt;- c( paste0( round(strat1[1]),\" (\",round(s1_sd),\")\"), paste0( strat1[2:11]*100, \"%\") )\n\nstrat2 &lt;- colMeans(strat2_res)\ns2_sd &lt;- sd( strat2_res[,1] )\ns2res &lt;- c( paste0( round(strat2[1]),\" (\",round(s2_sd),\")\"), paste0( strat2[2:11]*100, \"%\") )\n\nstrat3 &lt;- colMeans(strat3_res)\ns3_sd &lt;- sd( strat3_res[,1] )\ns3res &lt;- c( paste0( round(strat3[1]),\" (\",round(s3_sd),\")\"), paste0( strat3[2:11]*100, \"%\") )\n\n# Format results\nlibrary(kableExtra)\nkbl_tab &lt;- rbind('Pocock Futility' = s1res, 'Min(ES) and p&gt;0.1' = s2res, 'Min(ES)' = s3res)\n\nkbl_tab %&gt;%\n  kbl(col.names=c('Dropping Rule','ESS (SD)', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4', 'ES=-0.5', 'ES=0', 'ES=0.1', 'ES=0.35','ES=0.4') ) %&gt;%\n  kable_classic() %&gt;%\n  add_header_above(c(\" \"=1, \" \"=1, \"Arm Made to End of Trial\"=5, \"Arm Rejected Null Hypothesis\"=5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArm Made to End of Trial\n\n\nArm Rejected Null Hypothesis\n\n\n\nDropping Rule\nESS (SD)\nES=-0.5\nES=0\nES=0.1\nES=0.35\nES=0.4\nES=-0.5\nES=0\nES=0.1\nES=0.35\nES=0.4\n\n\n\n\nPocock Futility\n292 (83)\n2.8%\n4.8%\n15.1%\n64.8%\n73.4%\n1.3%\n3.9%\n13.2%\n51.4%\n68.9%\n\n\nMin(ES) and p&gt;0.1\n327 (28)\n3.3%\n5.7%\n18%\n80.7%\n89.2%\n1.6%\n4.2%\n14.8%\n58.6%\n76.9%\n\n\nMin(ES)\n300 (0)\n0.1%\n0%\n1.4%\n37.8%\n60.7%\n1.6%\n2.7%\n7.2%\n61%\n71.4%\n\n\n\n\n\n\n\nFrom this simulation we can see that each decision rule has different performance and properties:\n\nPocock stopping has the lowest power because of its aggressive stopping for futility, but does allow multiple arms to be dropped resulting in a lower ESS. It also has the lowest rates of harmful (ES=-0.5) or null (ES=0) arms making it to the end of the trial.\nDropping the smallest effect size in a stage, if \\(p&gt;\\alpha=0.1\\), results in the highest proportions of the best two arms making it to the end of the trial, but it also leads to a slightly higher rate of ES=0.1 making it to the end which results in a larger ESS than we may desire.\nAlways dropping the minimum effect size, regardless of significance, but testing each arm based on the available data results in lower power and completion rates for ES=0.4, but does show slightly higher power for ES=0.35. Even though the trial completion rate for ES=0.35 is only 37.8%, this conundrum may be explained by the fact that we allowed for testing across all stages regardless of when it stopped and we may have, by chance, stopped ES=0.35 at the 4th stage when it was significant but if it continued enrollment to stage 5 its p-value increased over 0.025.\n\nIn practice, the choice of dropping rules or strategies will be driven by the context of your particular study and balancing the strengthens and weaknesses across the trial operating characteristics.\n\n\nReferences\nBelow are some references to highlight based on the slides and code:\n\nFDA Adaptive Design Clinical Trials for Drugs and Biologics Guidance for Industry Guidance Document: FDA guidance document on adaptive trial elements\nRecent innovations in adaptive trial designs: A review of design opportunities in translational research: 2023 review paper examining adaptive and novel trial elements with included case studies"
  },
  {
    "objectID": "8_bayesian_information_sharing/index.html",
    "href": "8_bayesian_information_sharing/index.html",
    "title": "Bayesian Information Sharing",
    "section": "",
    "text": "Clinical trials do not often occur out of thin air, oftentimes they are an evolution across multiple phases of research across many different populations. In many settings, we may have access to historic, supplemental, or external data that we could incorporate directly into our trial analysis beyond simply using them for a power calculation to motivate the target sample size. In this module we introduce Bayesian approaches to information sharing and discuss some of the strengths and challenges of implementing these approaches that may allow you to enroll fewer prospective participants and increase study power, albeit at a risk of increased bias in some scenarios."
  },
  {
    "objectID": "8_bayesian_information_sharing/index.html#example-with-maps-and-rbest",
    "href": "8_bayesian_information_sharing/index.html#example-with-maps-and-rbest",
    "title": "Bayesian Information Sharing",
    "section": "Example with MAPs and RBesT",
    "text": "Example with MAPs and RBesT\nThe following example builds from a homework assignment I used in an advanced clinical trials course. It focuses on RBesT and implementing MAPs for a hypothetical study.\nFor the MAP-related questions, we will assume we have three supplemental sources of data. Each source has \\(n=50\\) and \\(\\sigma=20\\):\n\n\nCode\nlibrary(RBesT)\n\ndat &lt;- data.frame(\n  study = c('Study 1','Study 2','Study 3'),\n  n = c(50,50,50),\n  y = c(5,7,15)\n)\n\nstudy_sigma &lt;- 20 # set common SD\ndat$y.se &lt;- study_sigma / sqrt(dat$n)\n\n\n\nExplore different specifications of the half-normal prior for \\(\\tau\\)\nOne of the great things about RBesT package is that they’ve provided a wide range of possibilities to specify a prior on our between-study variance, denoted \\(\\sigma^2_{\\eta}\\) in our van Rosmalen reading (2018) or \\(\\tau\\) in the RBesT package. This includes the default half-normal prior, but also truncated normal, uniform, gamma, inverse gamma, log-normal, truncated Cauchy, exponential, and fixed options!\nFor the purpose of this homework, we first want to fit the general MAP prior with 3 different half-normal specifications (\\(\\tau \\sim HN(0,\\sigma_{\\tau})\\)):\n1. Prior \\(\\sigma_{\\tau}\\) of study_sigma / 2 = 10 (the prior used in the RBesT vignette for their example, stored in object map_mcmc_tauHN)\n2. Prior \\(\\sigma_{\\tau}\\) of study_sigma x 10 = 200 (a larger prior value on the between-study variability, stored in object map_mcmc_tauHNv2)\n3. Prior \\(\\sigma_{\\tau}\\) of study_sigma / 20 = 1 (a smaller prior value on the between-study variability, stored in object map_mcmc_tauHNv3)\n\n\nCode\n### Derivation of MAP prior\n## Half-normal tau prior with study_sigma/2\nset.seed(1234) #set to ensure reproducibility of the MCMC estimate\nmap_mcmc_tauHN &lt;- gMAP(cbind(y, y.se) ~ 1 | study, \n                 weights=n,\n                 data=dat,\n                 family=gaussian,\n                 beta.prior=cbind(0, study_sigma),\n                 tau.dist=\"HalfNormal\",\n                 tau.prior=cbind(0,study_sigma/2))\nprint(map_mcmc_tauHN)\n\n\nGeneralized Meta Analytic Predictive Prior Analysis\n\nCall:  gMAP(formula = cbind(y, y.se) ~ 1 | study, family = gaussian, \n    data = dat, weights = n, tau.dist = \"HalfNormal\", tau.prior = cbind(0, \n        study_sigma/2), beta.prior = cbind(0, study_sigma))\n\nExchangeability tau strata: 1 \nPrediction tau stratum    : 1 \nMaximal Rhat              : 1 \nEstimated reference scale : 20 \n\nBetween-trial heterogeneity of tau prediction stratum\n  mean     sd   2.5%    50%  97.5% \n 6.380  4.160  0.684  5.550 17.100 \n\nMAP Prior MCMC sample\n  mean     sd   2.5%    50%  97.5% \n  8.53   9.22 -11.70   8.79  27.20 \n\n\nCode\n## Half-normal tau prior with study_sigma*10\nset.seed(1234) #set to ensure reproducibility of the MCMC estimate\nmap_mcmc_tauHNv2 &lt;- gMAP(cbind(y, y.se) ~ 1 | study, \n                       weights=n,\n                       data=dat,\n                       family=gaussian,\n                       beta.prior=cbind(0, study_sigma),\n                       tau.dist=\"HalfNormal\",\n                       tau.prior=cbind(0,study_sigma*10))\nprint(map_mcmc_tauHNv2)\n\n\nGeneralized Meta Analytic Predictive Prior Analysis\n\nCall:  gMAP(formula = cbind(y, y.se) ~ 1 | study, family = gaussian, \n    data = dat, weights = n, tau.dist = \"HalfNormal\", tau.prior = cbind(0, \n        study_sigma * 10), beta.prior = cbind(0, study_sigma))\n\nExchangeability tau strata: 1 \nPrediction tau stratum    : 1 \nMaximal Rhat              : 1 \nEstimated reference scale : 20 \n\nBetween-trial heterogeneity of tau prediction stratum\n mean    sd  2.5%   50% 97.5% \n12.90 14.70  1.03  8.35 55.00 \n\nMAP Prior MCMC sample\n  mean     sd   2.5%    50%  97.5% \n  7.63  20.60 -35.10   8.33  44.30 \n\n\nCode\n## Half-normal tau prior with study_sigma/20\nset.seed(1234) #set to ensure reproducibility of the MCMC estimate\nmap_mcmc_tauHNv3 &lt;- gMAP(cbind(y, y.se) ~ 1 | study, \n                       weights=n,\n                       data=dat,\n                       family=gaussian,\n                       beta.prior=cbind(0, study_sigma),\n                       tau.dist=\"HalfNormal\",\n                       tau.prior=cbind(0,study_sigma/20))\nprint(map_mcmc_tauHNv3)\n\n\nGeneralized Meta Analytic Predictive Prior Analysis\n\nCall:  gMAP(formula = cbind(y, y.se) ~ 1 | study, family = gaussian, \n    data = dat, weights = n, tau.dist = \"HalfNormal\", tau.prior = cbind(0, \n        study_sigma/20), beta.prior = cbind(0, study_sigma))\n\nExchangeability tau strata: 1 \nPrediction tau stratum    : 1 \nMaximal Rhat              : 1 \nEstimated reference scale : 20 \n\nBetween-trial heterogeneity of tau prediction stratum\n mean    sd  2.5%   50% 97.5% \n0.948 0.672 0.040 0.834 2.460 \n\nMAP Prior MCMC sample\n mean    sd  2.5%   50% 97.5% \n 8.91  2.11  4.75  8.88 13.10 \n\n\n\n\nForest Plots to View Our Data\nOne helpful way to understand the data and the impact of the MAP prior is to graph it. The RBesT package include forest plots that are easy to create from our gMAP objects saved above. Note: the default is the lighter blue dashed line is the original data, the dark blue solid line is the MAP estimate.\n\n\nCode\n## Plot forest plots\nprint( plot(map_mcmc_tauHN)$forest_model )\n\n\n\n\n\nCode\nprint( plot(map_mcmc_tauHNv2)$forest_model )\n\n\n\n\n\nCode\nprint( plot(map_mcmc_tauHNv3)$forest_model )\n\n\n\n\n\n\n\nConverting the estimated MAP from the MCMC into a parametric approximation for use in later steps\nThe gMAP objects that were estimated given our different priors above were estimated using the MCMC chains fit with rstan. However, to apply these estimates in the process of estimating our trial performance we need to approximate the MAP in some (parametric) way. Within RBesT we can achieve this by using the automixfit function, which takes our MCMC output for the object and uses an expectation-maximization (EM) algorithm to estimate a parametric mixture of different components.\nWe can also note that RBesT includes a function to visualize the mixtures:\n\n\nCode\n## Approximation of MAP Prior using a mixture distribution\nmap &lt;- automixfit(map_mcmc_tauHN)\nmapv2 &lt;- automixfit(map_mcmc_tauHNv2)\nmapv3 &lt;- automixfit(map_mcmc_tauHNv3)\n\nprint(map)\n\n\nEM for Normal Mixture Model\nLog-Likelihood = -14041.76\n\nUnivariate normal mixture\nReference scale: 20\nMixture Components:\n  comp1      comp2      comp3     \nw  0.4884107  0.4625307  0.0490586\nm  8.9697909  8.3180233  6.0787268\ns  3.7118243 10.3381879 24.0664794\n\n\nCode\nprint(mapv2)\n\n\nEM for Normal Mixture Model\nLog-Likelihood = -16253.76\n\nUnivariate normal mixture\nReference scale: 20\nMixture Components:\n  comp1       comp2       comp3       comp4      \nw  0.38367072  0.33720288  0.23881644  0.04030997\nm  8.51390751  8.48768978  6.97075811 -4.13223612\ns  4.28683959 10.91393632 25.08149588 73.81527800\n\n\nCode\nprint(mapv3)\n\n\nEM for Normal Mixture Model\nLog-Likelihood = -8626.03\n\nUnivariate normal mixture\nReference scale: 20\nMixture Components:\n  comp1     comp2    \nw 0.5375988 0.4624012\nm 8.8567284 8.9741893\ns 1.5686685 2.6002141\n\n\nCode\n# Check accuracy of mixture fits\nplot(map)$mix\n\n\n\n\n\nCode\nplot(mapv2)$mix\n\n\n\n\n\nCode\nplot(mapv3)$mix\n\n\n\n\n\n\n\nThe effective sample size of each prior\nAs our final stop in comparing the performance of different prior specifications on \\(\\tau\\) we can examine the resulting effective sample size that would be imparted by the differing choices of our prior. Interestingly, there are multiple methods one can use in calculating the contribution of a given prior. The RBesT package has the functionlity to estimate three: elir, moment, or morita. The elir method is the expected local information ratio proposed by Neuenschwander et al. in a paper under review and is the default method. The method approaches utilizes the mean and SD of the mixture which are then approximated by conjugate distributions with the same mean and SD. Finally, morita is a method proposed by Morita that utilizes the mode instead of the mean.\n\n\nCode\n## Effective Sample Size (ESS)\nround(ess(map)) #default elir method\n\n\n[1] 10\n\n\nCode\nround(ess(mapv2)) #default elir method\n\n\n[1] 5\n\n\nCode\nround(ess(mapv3)) #default elir method\n\n\n[1] 95"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About This Website",
    "section": "",
    "text": "This is an R Markdown website, created by Alex Kaizer for a short course on adaptive and Bayesian clinical trial designs for the 2024 WNAR/IMS/Graybill Conference hosted at Colorado State University.\nIn the short course, we’ll actively use several packages which are listed below. Code to install the packages follows, and version information about each of these packages and their dependencies is at the bottom of this page.\nFor step-by-step instructions on installing brms and Stan, the following learnB4SS provides nice steps on their GitHub website.\nInstalling packages from CRAN\n\n# main packages\ninstall.packages(c(\"brms\",\"bayestestR\",\"rpact\",\"carat\",\"kableExtra\",\"blindrecalc\",\"gsDesign\"))\n\nVersions of key packages:\n\ndevtools::session_info(c(\"brms\",\"bayestestR\",\"rpact\",\"carat\",\"kableExtra\",\"blindrecalc\",\"gsDesign\"))\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.0 (2024-04-24 ucrt)\n os       Windows 10 x64 (build 19045)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United States.utf8\n ctype    English_United States.utf8\n tz       America/Denver\n date     2024-08-13\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package        * version    date (UTC) lib source\n   abind            1.4-5      2016-07-21 [1] CRAN (R 4.4.0)\n   backports        1.4.1      2021-12-13 [1] CRAN (R 4.4.0)\n   base64enc        0.1-3      2015-07-28 [1] CRAN (R 4.4.0)\n   bayesplot        1.11.1     2024-02-15 [1] CRAN (R 4.4.0)\n   bayestestR       0.13.2     2024-02-12 [1] CRAN (R 4.4.0)\n   BH               1.84.0-0   2024-01-10 [1] CRAN (R 4.4.0)\n   bigD             0.2.0      2022-09-05 [1] CRAN (R 4.4.0)\n   bitops           1.0-7      2021-04-24 [1] CRAN (R 4.4.0)\n   blindrecalc      1.0.1      2023-10-04 [1] CRAN (R 4.4.0)\n   bridgesampling   1.1-2      2021-04-16 [1] CRAN (R 4.4.0)\n   brms             2.21.0     2024-03-20 [1] CRAN (R 4.4.0)\n   Brobdingnag      1.2-9      2022-10-19 [1] CRAN (R 4.4.0)\n   bslib            0.7.0      2024-03-29 [1] CRAN (R 4.4.0)\n   cachem           1.0.8      2023-05-01 [1] CRAN (R 4.4.0)\n   callr            3.7.6      2024-03-25 [1] CRAN (R 4.4.0)\n   carat            2.2.1      2023-09-05 [1] CRAN (R 4.4.0)\n   checkmate        2.3.1      2023-12-04 [1] CRAN (R 4.4.0)\n   cli              3.6.2      2023-12-11 [1] CRAN (R 4.4.0)\n   coda             0.19-4.1   2024-01-31 [1] CRAN (R 4.4.0)\n   codetools        0.2-20     2024-03-31 [2] CRAN (R 4.4.0)\n   colorspace       2.1-0      2023-01-23 [1] CRAN (R 4.4.0)\n   commonmark       1.9.1      2024-01-30 [1] CRAN (R 4.4.0)\n   cpp11            0.4.7      2023-12-02 [1] CRAN (R 4.4.0)\n   curl             5.2.1      2024-03-01 [1] CRAN (R 4.4.0)\n   datawizard       0.11.0     2024-06-05 [1] CRAN (R 4.4.0)\n   desc             1.4.3      2023-12-10 [1] CRAN (R 4.4.0)\n   digest           0.6.35     2024-03-11 [1] CRAN (R 4.4.0)\n   distributional   0.4.0      2024-02-07 [1] CRAN (R 4.4.0)\n   dplyr            1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n   evaluate         0.23       2023-11-01 [1] CRAN (R 4.4.0)\n   fansi            1.0.6      2023-12-08 [1] CRAN (R 4.4.0)\n   farver           2.1.1      2022-07-06 [1] CRAN (R 4.4.0)\n   fastmap          1.1.1      2023-02-24 [1] CRAN (R 4.4.0)\n   fontawesome      0.5.2      2023-08-19 [1] CRAN (R 4.4.0)\n   fs               1.6.4      2024-04-25 [1] CRAN (R 4.4.0)\n   future           1.33.2     2024-03-26 [1] CRAN (R 4.4.0)\n   future.apply     1.11.2     2024-03-28 [1] CRAN (R 4.4.0)\n   generics         0.1.3      2022-07-05 [1] CRAN (R 4.4.0)\n   ggplot2          3.5.1      2024-04-23 [1] CRAN (R 4.4.0)\n   ggridges         0.5.6      2024-01-23 [1] CRAN (R 4.4.0)\n   globals          0.16.3     2024-03-08 [1] CRAN (R 4.4.0)\n   glue             1.7.0      2024-01-09 [1] CRAN (R 4.4.0)\n   gridExtra        2.3        2017-09-09 [1] CRAN (R 4.4.0)\n   gsDesign         3.6.2      2024-04-09 [1] CRAN (R 4.4.0)\n   gt               0.10.1     2024-01-17 [1] CRAN (R 4.4.0)\n   gtable           0.3.5      2024-04-22 [1] CRAN (R 4.4.0)\n   highr            0.10       2022-12-22 [1] CRAN (R 4.4.0)\n   htmltools        0.5.8.1    2024-04-04 [1] CRAN (R 4.4.0)\n   htmlwidgets      1.6.4      2023-12-06 [1] CRAN (R 4.4.0)\n   inline           0.3.19     2021-05-31 [1] CRAN (R 4.4.0)\n   insight          0.20.0     2024-06-04 [1] CRAN (R 4.4.0)\n   isoband          0.2.7      2022-12-20 [1] CRAN (R 4.4.0)\n   jquerylib        0.1.4      2021-04-26 [1] CRAN (R 4.4.1)\n   jsonlite         1.8.8      2023-12-04 [1] CRAN (R 4.4.0)\n   juicyjuice       0.1.0      2022-11-10 [1] CRAN (R 4.4.0)\n   kableExtra       1.4.0      2024-01-24 [1] CRAN (R 4.4.0)\n   knitr            1.46       2024-04-06 [1] CRAN (R 4.4.0)\n   labeling         0.4.3      2023-08-29 [1] CRAN (R 4.4.0)\n   lattice          0.22-6     2024-03-20 [2] CRAN (R 4.4.0)\n   lifecycle        1.0.4      2023-11-07 [1] CRAN (R 4.4.0)\n   listenv          0.9.1      2024-01-29 [1] CRAN (R 4.4.0)\n   loo              2.7.0      2024-02-24 [1] CRAN (R 4.4.0)\n   magrittr         2.0.3      2022-03-30 [1] CRAN (R 4.4.0)\n   markdown         1.12       2023-12-06 [1] CRAN (R 4.4.0)\n   MASS             7.3-60.2   2024-04-24 [2] local\n   Matrix           1.7-0      2024-03-22 [2] CRAN (R 4.4.0)\n   matrixStats      1.3.0      2024-04-11 [1] CRAN (R 4.4.0)\n   memoise          2.0.1      2021-11-26 [1] CRAN (R 4.4.0)\n   mgcv             1.9-1      2023-12-21 [2] CRAN (R 4.4.0)\n   mime             0.12       2021-09-28 [1] CRAN (R 4.4.0)\n   munsell          0.5.1      2024-04-01 [1] CRAN (R 4.4.0)\n   mvtnorm          1.2-4      2023-11-27 [1] CRAN (R 4.4.0)\n   nleqslv          3.3.5      2023-11-26 [1] CRAN (R 4.4.0)\n   nlme             3.1-164    2023-11-27 [2] CRAN (R 4.4.0)\n   numDeriv         2016.8-1.1 2019-06-06 [1] CRAN (R 4.4.0)\n   parallelly       1.37.1     2024-02-29 [1] CRAN (R 4.4.0)\n   pillar           1.9.0      2023-03-22 [1] CRAN (R 4.4.0)\n   pkgbuild         1.4.4      2024-03-17 [1] CRAN (R 4.4.0)\n   pkgconfig        2.0.3      2019-09-22 [1] CRAN (R 4.4.0)\n   plyr             1.8.9      2023-10-02 [1] CRAN (R 4.4.0)\n   posterior        1.5.0      2023-10-31 [1] CRAN (R 4.4.0)\n   processx         3.8.4      2024-03-16 [1] CRAN (R 4.4.0)\n   ps               1.7.6      2024-01-18 [1] CRAN (R 4.4.0)\n   purrr            1.0.2      2023-08-10 [1] CRAN (R 4.4.0)\n   QuickJSR         1.1.3      2024-01-31 [1] CRAN (R 4.4.0)\n   r2rtf            1.1.1      2023-10-25 [1] CRAN (R 4.4.0)\n   R6               2.5.1      2021-08-19 [1] CRAN (R 4.4.0)\n   rappdirs         0.3.3      2021-01-31 [1] CRAN (R 4.4.0)\n   RColorBrewer     1.1-3      2022-04-03 [1] CRAN (R 4.4.0)\n   Rcpp             1.0.12     2024-01-09 [1] CRAN (R 4.4.0)\n   RcppArmadillo    0.12.8.4.0 2024-05-31 [1] CRAN (R 4.4.0)\n   RcppEigen        0.3.4.0.0  2024-02-28 [1] CRAN (R 4.4.0)\n D RcppParallel     5.1.7      2023-02-27 [1] CRAN (R 4.4.0)\n   reactable        0.4.4      2023-03-12 [1] CRAN (R 4.4.0)\n   reactR           0.5.0      2023-10-11 [1] CRAN (R 4.4.0)\n   reshape2         1.4.4      2020-04-09 [1] CRAN (R 4.4.0)\n   rlang            1.1.3      2024-01-10 [1] CRAN (R 4.4.0)\n   rmarkdown        2.27       2024-05-17 [1] CRAN (R 4.4.0)\n   rpact            3.5.1      2024-02-27 [1] CRAN (R 4.4.0)\n   rstan            2.32.6     2024-03-05 [1] CRAN (R 4.4.0)\n   rstantools       2.4.0      2024-01-31 [1] CRAN (R 4.4.0)\n   rstudioapi       0.16.0     2024-03-24 [1] CRAN (R 4.4.0)\n   sass             0.4.9      2024-03-15 [1] CRAN (R 4.4.0)\n   scales           1.3.0      2023-11-28 [1] CRAN (R 4.4.0)\n   StanHeaders      2.32.7     2024-04-25 [1] CRAN (R 4.4.0)\n   stringi          1.8.3      2023-12-11 [1] CRAN (R 4.4.0)\n   stringr          1.5.1      2023-11-14 [1] CRAN (R 4.4.0)\n   svglite          2.1.3      2023-12-08 [1] CRAN (R 4.4.0)\n   systemfonts      1.1.0      2024-05-15 [1] CRAN (R 4.4.0)\n   tensorA          0.36.2.1   2023-12-13 [1] CRAN (R 4.4.0)\n   tibble           3.2.1      2023-03-20 [1] CRAN (R 4.4.0)\n   tidyr            1.3.1      2024-01-24 [1] CRAN (R 4.4.0)\n   tidyselect       1.2.1      2024-03-11 [1] CRAN (R 4.4.0)\n   tinytex          0.51       2024-05-06 [1] CRAN (R 4.4.0)\n   utf8             1.2.4      2023-10-22 [1] CRAN (R 4.4.0)\n   V8               4.4.2      2024-02-15 [1] CRAN (R 4.4.0)\n   vctrs            0.6.5      2023-12-01 [1] CRAN (R 4.4.0)\n   viridisLite      0.4.2      2023-05-02 [1] CRAN (R 4.4.0)\n   withr            3.0.0      2024-01-16 [1] CRAN (R 4.4.0)\n   xfun             0.44       2024-05-15 [1] CRAN (R 4.4.0)\n   xml2             1.3.6      2023-12-04 [1] CRAN (R 4.4.0)\n   xtable           1.8-4      2019-04-21 [1] CRAN (R 4.4.0)\n   yaml             2.3.8      2023-12-11 [1] CRAN (R 4.4.0)\n\n [1] C:/Users/kaizera/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.0/library\n\n D ── DLL MD5 mismatch, broken installation.\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]